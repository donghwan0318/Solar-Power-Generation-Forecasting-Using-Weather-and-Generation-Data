{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 시계열 data split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "# timeseriessplit 시각화\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.pyplot as plt\n",
    "# 모델\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeseriessplit 시각화용 함수\n",
    "def plot_cv_indices(cv, X, n_splits, lw=10):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits))\n",
    "    ax.set(yticks=np.arange(n_splits) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+0.1, -.1], xlim=[0, len(X)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    \n",
    "    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n",
    "          ['Testing set', 'Training set'], loc=(1.02, .8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blocked TimeSeriesSplit 용 class 정의\n",
    "class BlockingTimeSeriesSplit():\n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def get_n_splits(self, groups):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        k_fold_size = n_samples // self.n_splits\n",
    "        indices = np.arange(n_samples)\n",
    "    \n",
    "        margin = 0\n",
    "        for i in range(self.n_splits):\n",
    "            start = i * k_fold_size\n",
    "            stop = start + k_fold_size\n",
    "            mid = int(0.8 * (stop - start)) + start\n",
    "            yield indices[start: mid], indices[mid + margin: stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training 데이터 셋 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. gens : 실제 발전량 ( y로 쓸 예정 ) \n",
    "## 2. p_gens : 모델별 발전량 ( x변수로 추가 할 예정 )\n",
    "## 3. wf : 기상 예측량 ( x변수로 추가 할 예정 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gens = pd.read_csv(\"../data/OIBC2023_data/gens.csv\")\n",
    "p_gens = pd.read_csv(\"../data/OIBC2023_data/pred.csv\")\n",
    "wf = pd.read_csv(\"../data/OIBC2023_data/weather_forecast.csv\")\n",
    "add_gens = pd.read_csv('../data/OIBC2023_data/add_gens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gens = pd.concat([gens, add_gens], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-06-19 01:00:00+09:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-06-19 02:00:00+09:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-06-19 03:00:00+09:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-06-19 04:00:00+09:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-06-19 05:00:00+09:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>2023-11-12 20:00:00+09:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>2023-11-12 21:00:00+09:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>2023-11-12 22:00:00+09:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>2023-11-12 23:00:00+09:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>2023-11-13 00:00:00+09:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12264 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          time  amount\n",
       "0    2022-06-19 01:00:00+09:00     0.0\n",
       "1    2022-06-19 02:00:00+09:00     0.0\n",
       "2    2022-06-19 03:00:00+09:00     0.0\n",
       "3    2022-06-19 04:00:00+09:00     0.0\n",
       "4    2022-06-19 05:00:00+09:00     0.0\n",
       "..                         ...     ...\n",
       "643  2023-11-12 20:00:00+09:00     0.0\n",
       "644  2023-11-12 21:00:00+09:00     0.0\n",
       "645  2023-11-12 22:00:00+09:00     0.0\n",
       "646  2023-11-12 23:00:00+09:00     0.0\n",
       "647  2023-11-13 00:00:00+09:00     0.0\n",
       "\n",
       "[12264 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>time</th>\n",
       "      <th>model_id</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19 01:00:00+09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19 01:00:00+09:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19 01:00:00+09:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19 01:00:00+09:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19 01:00:00+09:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116035</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-10-16 00:00:00+09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116036</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-10-16 00:00:00+09:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116037</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-10-16 00:00:00+09:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116038</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-10-16 00:00:00+09:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116039</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-10-16 00:00:00+09:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116040 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        round                       time  model_id  amount\n",
       "0           1  2022-06-19 01:00:00+09:00         0     0.0\n",
       "1           1  2022-06-19 01:00:00+09:00         1     0.0\n",
       "2           1  2022-06-19 01:00:00+09:00         2     0.0\n",
       "3           1  2022-06-19 01:00:00+09:00         3     0.0\n",
       "4           1  2022-06-19 01:00:00+09:00         4     0.0\n",
       "...       ...                        ...       ...     ...\n",
       "116035      2  2023-10-16 00:00:00+09:00         0     0.0\n",
       "116036      2  2023-10-16 00:00:00+09:00         1     0.0\n",
       "116037      2  2023-10-16 00:00:00+09:00         2     0.0\n",
       "116038      2  2023-10-16 00:00:00+09:00         3     0.0\n",
       "116039      2  2023-10-16 00:00:00+09:00         4     0.0\n",
       "\n",
       "[116040 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_gens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 날씨 예측을 10시에 한번 17시에 한번 진행 함으로 2개로 나눠야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날씨 예측량을 round1,2로 분리함\n",
    "wf_round1 = wf[wf[\"round\"]==1]\n",
    "wf_round2 = wf[wf[\"round\"]==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_29096\\3743235756.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wf_round1.drop([\"round\"],axis=1,inplace=True)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_29096\\3743235756.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wf_round2.drop([\"round\"],axis=1,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "wf_round1.drop([\"round\"],axis=1,inplace=True)\n",
    "wf_round2.drop([\"round\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p_gens를 x변수로 만들기 위해서 모델별로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 발전량 예측량을 모델별로 분리해서 변수로 만들라고 분리하는거\n",
    "# round1에 대해 진행할 경우에 이거 실행\n",
    "p_gens10 = p_gens.loc[(p_gens[\"model_id\"]==0) & (p_gens[\"round\"]==1)].reset_index()\n",
    "p_gens11 = p_gens.loc[(p_gens[\"model_id\"]==1) & (p_gens[\"round\"]==1)].reset_index()\n",
    "p_gens12 = p_gens.loc[(p_gens[\"model_id\"]==2) & (p_gens[\"round\"]==1)].reset_index()\n",
    "p_gens13 = p_gens.loc[(p_gens[\"model_id\"]==3) & (p_gens[\"round\"]==1)].reset_index()\n",
    "p_gens14 = p_gens.loc[(p_gens[\"model_id\"]==4) & (p_gens[\"round\"]==1)].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 발전량 예측량을 모델별로 분리해서 변수로 만들라고 분리하는거\n",
    "# round2에 대해 진행할 경우에 이거 실행\n",
    "p_gens20 = p_gens.loc[(p_gens[\"model_id\"]==0) & (p_gens[\"round\"]==2)].reset_index()\n",
    "p_gens21 = p_gens.loc[(p_gens[\"model_id\"]==1) & (p_gens[\"round\"]==2)].reset_index()\n",
    "p_gens22 = p_gens.loc[(p_gens[\"model_id\"]==2) & (p_gens[\"round\"]==2)].reset_index()\n",
    "p_gens23 = p_gens.loc[(p_gens[\"model_id\"]==3) & (p_gens[\"round\"]==2)].reset_index()\n",
    "p_gens24 = p_gens.loc[(p_gens[\"model_id\"]==4) & (p_gens[\"round\"]==2)].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p_gens 변수로 만들어진 데이터 셋, wf로 만든 데이터셋을 통합해야함. 시간을 기준으로 통합 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_gens(모델 예측량) 데이터 프레임 생성\n",
    "x_df1 = pd.DataFrame({\"m0\": p_gens10[\"amount\"],\"m1\": p_gens11[\"amount\"],\n",
    "              \"m2\": p_gens12[\"amount\"],\"m3\": p_gens13[\"amount\"],\n",
    "              \"time\" : p_gens10[\"time\"]})\n",
    "x_df2 = pd.DataFrame({\"m0\": p_gens20[\"amount\"],\"m1\": p_gens21[\"amount\"],\n",
    "              \"m2\": p_gens22[\"amount\"],\"m3\": p_gens23[\"amount\"],\n",
    "              \"time\" : p_gens20[\"time\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10월 발전량 데이터 추가 round 1\n",
    "for i in range(21,32):\n",
    "    exec(f\"a = pd.read_csv('../data/gen_fcst_10_2023-10-{i}.csv')\")\n",
    "    a.columns = [\"time\",\"m0\",\"m1\",\"m2\",\"m3\",\"m4\"]\n",
    "    a = a[[\"m0\",\"m1\",\"m2\",\"m3\",\"time\"]]\n",
    "    x_df1 = pd.concat([x_df1,a],axis=0)\n",
    "    \n",
    "\n",
    "# 10월 발전량 데이터 추가 round 2\n",
    "for i in range(21,32):\n",
    "    exec(f\"a = pd.read_csv('../data/gen_fcst_17_2023-10-{i}.csv')\")\n",
    "    a.columns = [\"time\",\"m0\",\"m1\",\"m2\",\"m3\",\"m4\"]\n",
    "    a = a[[\"m0\",\"m1\",\"m2\",\"m3\",\"time\"]]\n",
    "    x_df2 = pd.concat([x_df2,a],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11월 발전량 데이터 추가 round 1\n",
    "for i in range(1,10):\n",
    "    exec(f\"a = pd.read_csv('../data/gen_fcst_10_2023-11-0{i}.csv')\")\n",
    "    a.columns = [\"time\",\"m0\",\"m1\",\"m2\",\"m3\",\"m4\"]\n",
    "    a = a[[\"m0\",\"m1\",\"m2\",\"m3\",\"time\"]]\n",
    "    x_df1 = pd.concat([x_df1,a],axis=0)\n",
    "    \n",
    "for i in range(10,13):\n",
    "    exec(f\"a = pd.read_csv('../data/gen_fcst_10_2023-11-{i}.csv')\")\n",
    "    a.columns = [\"time\",\"m0\",\"m1\",\"m2\",\"m3\",\"m4\"]\n",
    "    a = a[[\"m0\",\"m1\",\"m2\",\"m3\",\"time\"]]\n",
    "    x_df1 = pd.concat([x_df1,a],axis=0)\n",
    "\n",
    "# 11월 발전량 데이터 추가 round 2\n",
    "for i in range(1,10):\n",
    "    exec(f\"a = pd.read_csv('../data/gen_fcst_17_2023-11-0{i}.csv')\")\n",
    "    a.columns = [\"time\",\"m0\",\"m1\",\"m2\",\"m3\",\"m4\"]\n",
    "    a = a[[\"m0\",\"m1\",\"m2\",\"m3\",\"time\"]]\n",
    "    x_df2 = pd.concat([x_df2,a],axis=0)\n",
    "\n",
    "for i in range(10,13):\n",
    "    exec(f\"a = pd.read_csv('../data/gen_fcst_17_2023-11-{i}.csv')\")\n",
    "    a.columns = [\"time\",\"m0\",\"m1\",\"m2\",\"m3\",\"m4\"]\n",
    "    a = a[[\"m0\",\"m1\",\"m2\",\"m3\",\"time\"]]\n",
    "    x_df2 = pd.concat([x_df2,a],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10월 날씨 데이터 추가 round 1\n",
    "for i in range(21,32):\n",
    "    exec(f\"a = pd.read_csv('../data/weather_forecasts/wf10_2023-10-{i}.csv')\")\n",
    "    wf_round1 = pd.concat([wf_round1,a],axis=0)\n",
    "    \n",
    "\n",
    "# 10월 날씨 데이터 추가 round 2\n",
    "for i in range(21,32):\n",
    "    exec(f\"a = pd.read_csv('../data/weather_forecasts/wf17_2023-10-{i}.csv')\")\n",
    "    wf_round2 = pd.concat([wf_round2,a],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11월 날씨 데이터 추가 round 1\n",
    "for i in range(1,10):\n",
    "    exec(f\"a = pd.read_csv('../data/weather_forecasts/wf10_2023-11-0{i}.csv')\")\n",
    "    wf_round1 = pd.concat([wf_round1,a],axis=0)\n",
    "\n",
    "for i in range(10,13):\n",
    "    exec(f\"a = pd.read_csv('../data/weather_forecasts/wf10_2023-11-{i}.csv')\")\n",
    "    wf_round1 = pd.concat([wf_round1,a],axis=0)\n",
    "\n",
    "# 11월 날씨 데이터 추가 round 2\n",
    "for i in range(1,10):\n",
    "    exec(f\"a = pd.read_csv('../data/weather_forecasts/wf17_2023-11-0{i}.csv')\")\n",
    "    wf_round2 = pd.concat([wf_round2,a],axis=0)\n",
    "    \n",
    "for i in range(10,13):\n",
    "    exec(f\"a = pd.read_csv('../data/weather_forecasts/wf17_2023-11-{i}.csv')\")\n",
    "    wf_round2 = pd.concat([wf_round2,a],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time을 기준으로 두 데이터 병합 ( p_gens, wf )\n",
    "# 1차 모델링의 Y값으로 쓰일 발전량 gens도 같이 병합\n",
    "# round2에 대해 수행해야하면 wf_round2/ 아니면 wf_round1\n",
    "xy_df1 = pd.merge(x_df1,wf_round1,on=\"time\")\n",
    "xy_df1 = pd.merge(xy_df1,gens,on=\"time\")\n",
    "\n",
    "# 월과 날짜 변수를 추가 할 것임, 또한 시간대를 기준으로 데이터프레임을 3개 생성해야함으로 hour 변수도 추가\n",
    "xy_df1[\"time\"]=pd.to_datetime(xy_df1[\"time\"])\n",
    "xy_df1[\"month\"]=xy_df1[\"time\"].dt.month\n",
    "xy_df1[\"day\"]=xy_df1[\"time\"].dt.day\n",
    "xy_df1[\"hour\"]=xy_df1[\"time\"].dt.hour\n",
    "\n",
    "# round2에 대해 수행해야하면 wf_round2/ 아니면 wf_round1\n",
    "xy_df2 = pd.merge(x_df2,wf_round2,on=\"time\")\n",
    "xy_df2 = pd.merge(xy_df2,gens,on=\"time\")\n",
    "\n",
    "# 월2 날짜 변수를 추가 할 것임, 또한 시간대를 기준으로 데이터프레임을 3개 생성해야함으로 hour 변수도 추가\n",
    "xy_df2[\"time\"]=pd.to_datetime(xy_df2[\"time\"])\n",
    "xy_df2[\"month\"]=xy_df2[\"time\"].dt.month\n",
    "xy_df2[\"day\"]=xy_df2[\"time\"].dt.day\n",
    "xy_df2[\"hour\"]=xy_df2[\"time\"].dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간에 따라 전체 통합된 데이터셋을 3개로 분리\n",
    "# 7-10, 11-14 , 15-19\n",
    "xy_df1_1 = xy_df1[(xy_df1[\"hour\"]<=10) & (xy_df1[\"hour\"]>=7)]\n",
    "xy_df1_2 = xy_df1[(xy_df1[\"hour\"]<=14) & (xy_df1[\"hour\"]>=11)]\n",
    "xy_df1_3 = xy_df1[(xy_df1[\"hour\"]<=19) & (xy_df1[\"hour\"]>=15)]\n",
    "\n",
    "xy_df2_1 = xy_df2[(xy_df2[\"hour\"]<=10) & (xy_df2[\"hour\"]>=7)]\n",
    "xy_df2_2 = xy_df2[(xy_df2[\"hour\"]<=14) & (xy_df2[\"hour\"]>=11)]\n",
    "xy_df2_3 = xy_df2[(xy_df2[\"hour\"]<=19) & (xy_df2[\"hour\"]>=15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1차 모델링에서 사용할 X데이터셋 생성\n",
    "# 데이터프레임 이름 읽는 법.\n",
    "# 변수_몇차모델링_시간분리\n",
    "# 시간 분리는 (1. 7-10 / 2. 11-14 / 3. 15-19)\n",
    "# ex) x_df1_1 (x데이터셋, 1차 모델링, 7-10)\n",
    "x_df11_1 = xy_df1_1[[\"m0\",\"m1\",\"m2\",\"m3\",\"uv_idx\",\"elevation\"]]\n",
    "x_df11_2 = xy_df1_2[[\"m0\",\"m1\",\"m2\",\"m3\",\"uv_idx\",\"elevation\"]]\n",
    "x_df11_3 = xy_df1_3[[\"m0\",\"m1\",\"m2\",\"m3\",\"uv_idx\",\"elevation\"]]\n",
    "x_df12_1 = xy_df1_1.drop([\"round\",\"amount\",\"m0\",\"m1\",\"m2\",\"m3\",\"uv_idx\",\"elevation\",\"time\",\"hour\"],axis=1)\n",
    "x_df12_2 = xy_df1_2.drop([\"round\",\"amount\",\"m0\",\"m1\",\"m2\",\"m3\",\"uv_idx\",\"elevation\",\"time\",\"hour\"],axis=1)\n",
    "x_df12_3 = xy_df1_3.drop([\"round\",\"amount\",\"m0\",\"m1\",\"m2\",\"m3\",\"uv_idx\",\"elevation\",\"time\",\"hour\"],axis=1)\n",
    "\n",
    "x_df21_1 = xy_df2_1[[\"m0\",\"m1\",\"m2\",\"m3\",\"uv_idx\",\"elevation\"]]\n",
    "x_df21_2 = xy_df2_2[[\"m0\",\"m1\",\"m2\",\"m3\",\"uv_idx\",\"elevation\"]]\n",
    "x_df21_3 = xy_df2_3[[\"m0\",\"m1\",\"m2\",\"m3\",\"uv_idx\",\"elevation\"]]\n",
    "x_df22_1 = xy_df2_1.drop([\"round\",\"amount\",\"m0\",\"m1\",\"m2\",\"m3\",\"uv_idx\",\"elevation\",\"time\",\"hour\"],axis=1)\n",
    "x_df22_2 = xy_df2_2.drop([\"round\",\"amount\",\"m0\",\"m1\",\"m2\",\"m3\",\"uv_idx\",\"elevation\",\"time\",\"hour\"],axis=1)\n",
    "x_df22_3 = xy_df2_3.drop([\"round\",\"amount\",\"m0\",\"m1\",\"m2\",\"m3\",\"uv_idx\",\"elevation\",\"time\",\"hour\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1차 모델링에서 사용할 Y데이터셋 생성\n",
    "# 표기 법은 X와 동일\n",
    "# 다만 2차 모델링에서 사용할 Y는 1차 모델링에서 구한 Y_hat을 실제 Y에서 뺀 e임으로 추후에 구해야함\n",
    "# e = Y - Y_hat\n",
    "y_df11_1 = xy_df1_1[\"amount\"]\n",
    "y_df11_2 = xy_df1_2[\"amount\"]\n",
    "y_df11_3 = xy_df1_3[\"amount\"]\n",
    "\n",
    "# e = Y - Y_hat\n",
    "y_df21_1 = xy_df2_1[\"amount\"]\n",
    "y_df21_2 = xy_df2_2[\"amount\"]\n",
    "y_df21_3 = xy_df2_3[\"amount\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round 1 해당하는 데이터\n",
    "x_df11_1.to_csv('../data/MLP2LGBM_V2/X_r1_M1_T1.csv', index = False)\n",
    "x_df11_2.to_csv('../data/MLP2LGBM_V2/X_r1_M1_T2.csv', index = False)\n",
    "x_df11_3.to_csv('../data/MLP2LGBM_V2/X_r1_M1_T3.csv', index = False)\n",
    "x_df12_1.to_csv('../data/MLP2LGBM_V2/X_r1_M2_T1.csv', index = False)\n",
    "x_df12_2.to_csv('../data/MLP2LGBM_V2/X_r1_M2_T2.csv', index = False)\n",
    "x_df12_3.to_csv('../data/MLP2LGBM_V2/X_r1_M2_T3.csv', index = False)\n",
    "y_df11_1.to_csv('../data/MLP2LGBM_V2/Y_r1_M1_T1.csv', index = False)\n",
    "y_df11_2.to_csv('../data/MLP2LGBM_V2/Y_r1_M1_T2.csv', index = False)\n",
    "y_df11_3.to_csv('../data/MLP2LGBM_V2/Y_r1_M1_T3.csv', index = False)\n",
    "\n",
    "# round 2 해당하는 데이터\n",
    "x_df21_1.to_csv('../data/MLP2LGBM_V2/X_r2_M1_T1.csv', index = False)\n",
    "x_df21_2.to_csv('../data/MLP2LGBM_V2/X_r2_M1_T2.csv', index = False)\n",
    "x_df21_3.to_csv('../data/MLP2LGBM_V2/X_r2_M1_T3.csv', index = False)\n",
    "x_df22_1.to_csv('../data/MLP2LGBM_V2/X_r2_M2_T1.csv', index = False)\n",
    "x_df22_2.to_csv('../data/MLP2LGBM_V2/X_r2_M2_T2.csv', index = False)\n",
    "x_df22_3.to_csv('../data/MLP2LGBM_V2/X_r2_M2_T3.csv', index = False)\n",
    "y_df21_1.to_csv('../data/MLP2LGBM_V2/Y_r2_M1_T1.csv', index = False)\n",
    "y_df21_2.to_csv('../data/MLP2LGBM_V2/Y_r2_M1_T2.csv', index = False)\n",
    "y_df21_3.to_csv('../data/MLP2LGBM_V2/Y_r2_M1_T3.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링1 선형관계 linear or mlp (모델은 바뀔수 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch 패키지 import 및 CPU/GPU 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# display images\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# utils\n",
    "import os\n",
    "from torchsummary import summary\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from torch_utils_MLP import timeseriesKFoldCV, train_val, seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기 및 스케일링 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r1_M1_T1 = pd.read_csv('../data/MLP2LGBM_V2/X_r1_M1_T1.csv')\n",
    "X_r1_M1_T2 = pd.read_csv('../data/MLP2LGBM_V2/X_r1_M1_T2.csv')\n",
    "X_r1_M1_T3 = pd.read_csv('../data/MLP2LGBM_V2/X_r1_M1_T3.csv')\n",
    "Y_r1_M1_T1 = pd.read_csv('../data/MLP2LGBM_V2/Y_r1_M1_T1.csv')\n",
    "Y_r1_M1_T2 = pd.read_csv('../data/MLP2LGBM_V2/Y_r1_M1_T2.csv')\n",
    "Y_r1_M1_T3 = pd.read_csv('../data/MLP2LGBM_V2/Y_r1_M1_T3.csv')\n",
    "\n",
    "X_r2_M1_T1 = pd.read_csv('../data/MLP2LGBM_V2/X_r2_M1_T1.csv')\n",
    "X_r2_M1_T2 = pd.read_csv('../data/MLP2LGBM_V2/X_r2_M1_T2.csv')\n",
    "X_r2_M1_T3 = pd.read_csv('../data/MLP2LGBM_V2/X_r2_M1_T3.csv')\n",
    "Y_r2_M1_T1 = pd.read_csv('../data/MLP2LGBM_V2/Y_r2_M1_T1.csv')\n",
    "Y_r2_M1_T2 = pd.read_csv('../data/MLP2LGBM_V2/Y_r2_M1_T2.csv')\n",
    "Y_r2_M1_T3 = pd.read_csv('../data/MLP2LGBM_V2/Y_r2_M1_T3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "# round: 1 / Modeling: 1 / TimeFold : 1, 2, 3\n",
    "mms_X_111 = MinMaxScaler()\n",
    "mms_X_112 = MinMaxScaler()\n",
    "mms_X_113 = MinMaxScaler()\n",
    "mms_X_111.fit(X_r1_M1_T1)\n",
    "mms_X_112.fit(X_r1_M1_T2)\n",
    "mms_X_113.fit(X_r1_M1_T3)\n",
    "X_r1_M1_T1_scaled = mms_X_111.transform(X_r1_M1_T1)\n",
    "X_r1_M1_T2_scaled = mms_X_112.transform(X_r1_M1_T2)\n",
    "X_r1_M1_T3_scaled = mms_X_113.transform(X_r1_M1_T3)\n",
    "\n",
    "\n",
    "Y_r1_M1_T1 = np.array(Y_r1_M1_T1).reshape(-1, 1)\n",
    "Y_r1_M1_T2 = np.array(Y_r1_M1_T2).reshape(-1, 1)\n",
    "Y_r1_M1_T3 = np.array(Y_r1_M1_T3).reshape(-1, 1)\n",
    "mms_Y_111 = MinMaxScaler()\n",
    "mms_Y_112 = MinMaxScaler()\n",
    "mms_Y_113 = MinMaxScaler()\n",
    "mms_Y_111.fit(Y_r1_M1_T1)\n",
    "mms_Y_112.fit(Y_r1_M1_T2)\n",
    "mms_Y_113.fit(Y_r1_M1_T3)\n",
    "Y_r1_M1_T1_scaled = mms_Y_111.transform(Y_r1_M1_T1)\n",
    "Y_r1_M1_T2_scaled = mms_Y_112.transform(Y_r1_M1_T2)\n",
    "Y_r1_M1_T3_scaled = mms_Y_113.transform(Y_r1_M1_T3)\n",
    "\n",
    "# train_Y = np.array(train_Y).reshape(-1, 1) # 원래 Y는 (-1,)차원, sklearn의 scaler들은 2차원 이상의 인풋을 기대함\n",
    "# test_Y = np.array(test_Y).reshape(-1, 1) # 차원을 맞춰줘야 에러가 발생하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "# round: 1 / Modeling: 1 / TimeFold : 1, 2, 3\n",
    "mms_X_211 = MinMaxScaler()\n",
    "mms_X_212 = MinMaxScaler()\n",
    "mms_X_213 = MinMaxScaler()\n",
    "mms_X_211.fit(X_r2_M1_T1)\n",
    "mms_X_212.fit(X_r2_M1_T2)\n",
    "mms_X_213.fit(X_r2_M1_T3)\n",
    "X_r2_M1_T1_scaled = mms_X_111.transform(X_r2_M1_T1)\n",
    "X_r2_M1_T2_scaled = mms_X_112.transform(X_r2_M1_T2)\n",
    "X_r2_M1_T3_scaled = mms_X_113.transform(X_r2_M1_T3)\n",
    "\n",
    "\n",
    "Y_r2_M1_T1 = np.array(Y_r2_M1_T1).reshape(-1, 1)\n",
    "Y_r2_M1_T2 = np.array(Y_r2_M1_T2).reshape(-1, 1)\n",
    "Y_r2_M1_T3 = np.array(Y_r2_M1_T3).reshape(-1, 1)\n",
    "mms_Y_211 = MinMaxScaler()\n",
    "mms_Y_212 = MinMaxScaler()\n",
    "mms_Y_213 = MinMaxScaler()\n",
    "mms_Y_211.fit(Y_r2_M1_T1)\n",
    "mms_Y_212.fit(Y_r2_M1_T2)\n",
    "mms_Y_213.fit(Y_r2_M1_T3)\n",
    "Y_r2_M1_T1_scaled = mms_Y_211.transform(Y_r2_M1_T1)\n",
    "Y_r2_M1_T2_scaled = mms_Y_212.transform(Y_r2_M1_T2)\n",
    "Y_r2_M1_T3_scaled = mms_Y_213.transform(Y_r2_M1_T3)\n",
    "\n",
    "# train_Y = np.array(train_Y).reshape(-1, 1) # 원래 Y는 (-1,)차원, sklearn의 scaler들은 2차원 이상의 인풋을 기대함\n",
    "# test_Y = np.array(test_Y).reshape(-1, 1) # 차원을 맞춰줘야 에러가 발생하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 객체 선언 및 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlpDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X, dtype = torch.float32)#.type(torch.DoubleTensor)#.type(torch.float32)\n",
    "        self.Y = torch.tensor(Y, dtype = torch.float32)#.type(torch.DoubleTensor)#.type(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# 시계열 k-Fold를 위한 인덱스 할당\n",
    "n_split = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_split)\n",
    "\n",
    "tscv_train_idx = []\n",
    "tscv_valid_idx = []\n",
    "for idx in tscv.split(X_r1_M1_T1_scaled):\n",
    "    tscv_train_idx.append(idx[0])\n",
    "    tscv_valid_idx.append(idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326 322\n",
      "648 322\n",
      "970 322\n",
      "1292 322\n",
      "1614 322\n"
     ]
    }
   ],
   "source": [
    "for t, v in zip(tscv_train_idx, tscv_valid_idx):\n",
    "    print(len(t), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "seed_everything(seed)\n",
    "num_batch = 128\n",
    "\n",
    "train_ds111 = mlpDataset(X_r1_M1_T1_scaled, Y_r1_M1_T1_scaled)\n",
    "train_dl111 = DataLoader(train_ds111, batch_size = num_batch, shuffle = True)\n",
    "\n",
    "train_ds112 = mlpDataset(X_r1_M1_T2_scaled, Y_r1_M1_T2_scaled)\n",
    "train_dl112 = DataLoader(train_ds112, batch_size = num_batch, shuffle = True)\n",
    "\n",
    "train_ds113 = mlpDataset(X_r1_M1_T3_scaled, Y_r1_M1_T3_scaled)\n",
    "train_dl113 = DataLoader(train_ds113, batch_size = num_batch, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "seed_everything(seed)\n",
    "num_batch = 128\n",
    "\n",
    "train_ds211 = mlpDataset(X_r2_M1_T1_scaled, Y_r2_M1_T1_scaled)\n",
    "train_dl211 = DataLoader(train_ds211, batch_size = num_batch, shuffle = True)\n",
    "train_ds212 = mlpDataset(X_r2_M1_T2_scaled, Y_r2_M1_T2_scaled)\n",
    "train_dl212 = DataLoader(train_ds212, batch_size = num_batch, shuffle = True)\n",
    "train_ds213 = mlpDataset(X_r2_M1_T3_scaled, Y_r2_M1_T3_scaled)\n",
    "train_dl213 = DataLoader(train_ds213, batch_size = num_batch, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조 정의\n",
    "class MLP_regressor(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 32).type(torch.float32)#.type(torch.float32)\n",
    "        self.fc2 = nn.Linear(32, 64).type(torch.float32)#.type(torch.float32) \n",
    "        self.fc3 = nn.Linear(64, 128).type(torch.float32)#.type(torch.float32) \n",
    "        self.fc4 = nn.Linear(128, 64).type(torch.float32)#.type(torch.float32)\n",
    "        self.fc5 = nn.Linear(64, 32).type(torch.float32)#.type(torch.float32)\n",
    "        self.fc6 = nn.Linear(32, 16).type(torch.float32)#.type(torch.float32)\n",
    "        self.fc7 = nn.Linear(16, 8).type(torch.float32)#.type(torch.float32)\n",
    "        self.fc8 = nn.Linear(8, 1).type(torch.float32)#.type(torch.float32)\n",
    "        \n",
    "        # type()\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        # self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.dropout(torch.sigmoid(self.fc2(x)))\n",
    "        x = self.dropout(torch.sigmoid(self.fc3(x)))\n",
    "        x = self.dropout(torch.sigmoid(self.fc4(x)))\n",
    "        x = torch.sigmoid(self.fc5(x))\n",
    "        x = torch.sigmoid(self.fc6(x))\n",
    "        x = torch.sigmoid(self.fc7(x))\n",
    "        x = torch.sigmoid(self.fc8(x))\n",
    "      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold 평가 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss(reduction = 'sum').to(DEVICE)\n",
    "\n",
    "def hyperParamTuning(classModel, objDataset, objDataLoader, X, Y, expName):\n",
    "    # definc the training parameters\n",
    "    model_params = {\n",
    "        'num_epochs': 5000,\n",
    "        'loss_func': loss_func,\n",
    "\n",
    "        'X': X,\n",
    "        'Y': Y,\n",
    "        'tscv_train_idx': tscv_train_idx,\n",
    "        'tscv_valid_idx': tscv_valid_idx,\n",
    "        'batch_size': 128,\n",
    "\n",
    "        'sanity_check': False, # True인 경우 데이터 조금만(batch 1개) 사용해서 학습 코드가 잘 돌아가는지 확인 / False면 데이터 전체 사용\n",
    "        # 'lr_scheduler': lr_scheduler,\n",
    "        'path2weights': f'../model/testMLP/{expName}.pt', # 모델 파라미터를 저장할 경로\n",
    "        'DEVICE': DEVICE, # cpu / cuda\n",
    "        'early_stopping': True, # 학습 조기 종료 적용 여부 -> True: 적용 / False: 미적용\n",
    "        'patience_limit': 500\n",
    "        # 'num_folds': n_split\n",
    "    }\n",
    "    \n",
    "    seed = 42\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    model = classModel(num_features = 6)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for LR in [0.01, 0.001, 0.0001]:\n",
    "            model_params['learning_rate'] = LR\n",
    "            \n",
    "            avg_loss, model_params = timeseriesKFoldCV(MLP_regressor, objDataset, objDataLoader, model_params)\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                model_params['avg_loss'] = avg_loss\n",
    "                with open(model_params['path2weights'][:-2]+'pickle','wb') as fw:\n",
    "                    pickle.dump(model_params, fw)\n",
    "            else:\n",
    "                continue\n",
    "    return 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on fold-0\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Epoch 0/4999, current lr=0.01\n",
      "train loss: 0.021026, val loss: 0.089696, mse: 0.09, time: 0.0004 min\n",
      "----------\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Epoch 100/4999, current lr=0.01\n",
      "train loss: 0.011411, val loss: 0.029755, mse: 0.03, time: 0.0266 min\n",
      "----------\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Epoch 200/4999, current lr=0.01\n",
      "train loss: 0.010627, val loss: 0.026718, mse: 0.03, time: 0.0515 min\n",
      "----------\n",
      "Epoch 300/4999, current lr=0.01\n",
      "train loss: 0.010742, val loss: 0.031592, mse: 0.03, time: 0.0765 min\n",
      "----------\n",
      "Epoch 400/4999, current lr=0.01\n",
      "train loss: 0.010725, val loss: 0.026341, mse: 0.03, time: 0.1006 min\n",
      "----------\n",
      "EARLY STOPPED AT EPOCH--499\n",
      "Train on fold-1\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Epoch 0/4999, current lr=0.01\n",
      "train loss: 0.024107, val loss: 0.076080, mse: 0.08, time: 0.0004 min\n",
      "----------\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Epoch 100/4999, current lr=0.01\n",
      "train loss: 0.011821, val loss: 0.014247, mse: 0.01, time: 0.0423 min\n",
      "----------\n",
      "Epoch 200/4999, current lr=0.01\n",
      "train loss: 0.011330, val loss: 0.015545, mse: 0.02, time: 0.0813 min\n",
      "----------\n",
      "Epoch 300/4999, current lr=0.01\n",
      "train loss: 0.011078, val loss: 0.016286, mse: 0.02, time: 0.1202 min\n",
      "----------\n",
      "Epoch 400/4999, current lr=0.005\n",
      "train loss: 0.011257, val loss: 0.017517, mse: 0.02, time: 0.1603 min\n",
      "----------\n",
      "EARLY STOPPED AT EPOCH--499\n",
      "Train on fold-2\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Epoch 0/4999, current lr=0.01\n",
      "train loss: 0.023278, val loss: 0.085165, mse: 0.09, time: 0.0006 min\n",
      "----------\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Epoch 100/4999, current lr=0.01\n",
      "train loss: 0.011066, val loss: 0.017451, mse: 0.02, time: 0.0513 min\n",
      "----------\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Epoch 200/4999, current lr=0.01\n",
      "train loss: 0.010975, val loss: 0.016872, mse: 0.02, time: 0.1036 min\n",
      "----------\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Epoch 300/4999, current lr=0.01\n",
      "train loss: 0.010744, val loss: 0.016284, mse: 0.02, time: 0.1575 min\n",
      "----------\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Epoch 400/4999, current lr=0.01\n",
      "train loss: 0.010826, val loss: 0.016528, mse: 0.02, time: 0.2125 min\n",
      "----------\n",
      "EARLY STOPPED AT EPOCH--499\n",
      "Train on fold-3\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Epoch 0/4999, current lr=0.01\n",
      "train loss: 0.023875, val loss: 0.067364, mse: 0.07, time: 0.0008 min\n",
      "----------\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Epoch 100/4999, current lr=0.01\n",
      "train loss: 0.011257, val loss: 0.019612, mse: 0.02, time: 0.0730 min\n",
      "----------\n",
      "Epoch 200/4999, current lr=0.01\n",
      "train loss: 0.010841, val loss: 0.020150, mse: 0.02, time: 0.1427 min\n",
      "----------\n",
      "Epoch 300/4999, current lr=0.005\n",
      "train loss: 0.010725, val loss: 0.020035, mse: 0.02, time: 0.2130 min\n",
      "----------\n",
      "Epoch 400/4999, current lr=0.005\n",
      "train loss: 0.010882, val loss: 0.020104, mse: 0.02, time: 0.2811 min\n",
      "----------\n",
      "EARLY STOPPED AT EPOCH--499\n",
      "Train on fold-4\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Epoch 0/4999, current lr=0.01\n",
      "train loss: 0.023688, val loss: 0.076616, mse: 0.08, time: 0.0008 min\n",
      "----------\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "Epoch 100/4999, current lr=0.01\n",
      "train loss: 0.011478, val loss: 0.011683, mse: 0.01, time: 0.0882 min\n",
      "----------\n",
      "Epoch 200/4999, current lr=0.01\n",
      "train loss: 0.011299, val loss: 0.011781, mse: 0.01, time: 0.1631 min\n",
      "----------\n",
      "Epoch 300/4999, current lr=0.01\n",
      "train loss: 0.011291, val loss: 0.013407, mse: 0.01, time: 0.2394 min\n",
      "----------\n",
      "Epoch 400/4999, current lr=0.005\n",
      "train loss: 0.011148, val loss: 0.012779, mse: 0.01, time: 0.3175 min\n",
      "----------\n",
      "EARLY STOPPED AT EPOCH--499\n",
      "Averaged loss for 5-fold: 0.016277210571751095\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'tuple' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36620\\3472466530.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m hyperParamTuning(classModel = MLP_regressor,\n\u001b[0m\u001b[0;32m      2\u001b[0m                  \u001b[0mobjDataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlpDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjDataLoader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                  \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_r1_M1_T1_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_r1_M1_T1_scaled\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                  expName = 'MLP_r1_M1_T1')\n\u001b[0;32m      5\u001b[0m hyperParamTuning(classModel = MLP_regressor,\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36620\\1637036612.py\u001b[0m in \u001b[0;36mhyperParamTuning\u001b[1;34m(classModel, objDataset, objDataLoader, X, Y, expName)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeseriesKFoldCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMLP_regressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjDataLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 \u001b[0mbest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mmodel_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'avg_loss'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'tuple' and 'float'"
     ]
    }
   ],
   "source": [
    "hyperParamTuning(classModel = MLP_regressor,\n",
    "                 objDataset = mlpDataset, objDataLoader = DataLoader,\n",
    "                 X = X_r1_M1_T1_scaled, Y = Y_r1_M1_T1_scaled,\n",
    "                 expName = 'MLP_r1_M1_T1')\n",
    "hyperParamTuning(classModel = MLP_regressor,\n",
    "                 objDataset = mlpDataset, objDataLoader = DataLoader,\n",
    "                 X = X_r1_M1_T2_scaled, Y = Y_r1_M1_T2_scaled,\n",
    "                 expName = 'MLP_r1_M1_T2')\n",
    "hyperParamTuning(classModel = MLP_regressor,\n",
    "                 objDataset = mlpDataset, objDataLoader = DataLoader,\n",
    "                 X = X_r1_M1_T3_scaled, Y = Y_r1_M1_T3_scaled,\n",
    "                 expName = 'MLP_r1_M1_T3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 3.90625e-05\n",
      "    maximize: False\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "with open('../model/testMLP/MLP_r1_M1_T1.pickle', 'rb') as fr:\n",
    "    MLP_r1_M1_T1_params = pickle.load(fr)\n",
    "print(MLP_r1_M1_T1_params['optimizer'])\n",
    "print(MLP_r1_M1_T1_params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 7.8125e-05\n",
      "    maximize: False\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "with open('../model/testMLP/MLP_r1_M1_T2.pickle', 'rb') as fr:\n",
    "    MLP_r1_M1_T2_params = pickle.load(fr)\n",
    "print(MLP_r1_M1_T2_params['optimizer'])\n",
    "print(MLP_r1_M1_T2_params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 3.90625e-05\n",
      "    maximize: False\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "with open('../model/testMLP/MLP_r1_M1_T3.pickle', 'rb') as fr:\n",
    "    MLP_r1_M1_T3_params = pickle.load(fr)\n",
    "print(MLP_r1_M1_T3_params['optimizer'])\n",
    "print(MLP_r1_M1_T3_params['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### round1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "seed_everything(seed)\n",
    "m1_mlp11 = MLP_regressor(num_features = X_r1_M1_T1.shape[1]).to(DEVICE) # num_features = 6\n",
    "m1_mlp12 = MLP_regressor(num_features = X_r1_M1_T1.shape[1]).to(DEVICE) # num_features = 6\n",
    "m1_mlp13 = MLP_regressor(num_features = X_r1_M1_T1.shape[1]).to(DEVICE) # num_features = 6\n",
    "\n",
    "loss_func = nn.MSELoss(reduction = 'sum').to(DEVICE) # utils 내에서 데이터 개수로 나누기 때문에 -> 기본값 평균이 아닌 sum으로 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(m1_mlp11.parameters(), lr = 0.01, momentum = 0.0) # Adam에 모델의 학습 파라미터를 넣어줌\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor = 0.5, patience = 250)\n",
    "\n",
    "m1_mlp11_params = {\n",
    "    'num_epochs': 5000,\n",
    "    'loss_func': loss_func,\n",
    "    'optimizer': opt,\n",
    "\n",
    "    'train_dl': train_dl111,\n",
    "    'val_dl': None,\n",
    "    'batch_size': 32,\n",
    "\n",
    "    'sanity_check': False, # True인 경우 데이터 조금만(batch 1개) 사용해서 학습 코드가 잘 돌아가는지 확인 / False면 데이터 전체 사용\n",
    "    'lr_scheduler': lr_scheduler,\n",
    "    'path2weights': '../model/MLP2LGBM_V2/round1/m1_mlp1.pt', # 모델 파라미터를 저장할 경로\n",
    "    'DEVICE': DEVICE, # cpu / cuda\n",
    "    'early_stopping': True, # 학습 조기 종료 적용 여부 -> True: 적용 / False: 미적용\n",
    "    'patience_limit': 500,\n",
    "    'num_folds': n_split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(m1_mlp12.parameters(), lr = 0.01, momentum = 0.0) # Adam에 모델의 학습 파라미터를 넣어줌\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor = 0.5, patience = 250)\n",
    "\n",
    "m1_mlp12_params = {\n",
    "    'num_epochs': 5000,\n",
    "    'loss_func': loss_func,\n",
    "    'optimizer': opt,\n",
    "\n",
    "    'train_dl': train_dl112,\n",
    "    'val_dl': None,\n",
    "    'batch_size': 32,\n",
    "\n",
    "    'sanity_check': False, # True인 경우 데이터 조금만(batch 1개) 사용해서 학습 코드가 잘 돌아가는지 확인 / False면 데이터 전체 사용\n",
    "    'lr_scheduler': lr_scheduler,\n",
    "    'path2weights': '../model/MLP2LGBM_V2/round1/m1_mlp12.pt', # 모델 파라미터를 저장할 경로\n",
    "    'DEVICE': DEVICE, # cpu / cuda\n",
    "    'early_stopping': True, # 학습 조기 종료 적용 여부 -> True: 적용 / False: 미적용\n",
    "    'patience_limit': 500,\n",
    "    'num_folds': n_split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(m1_mlp13.parameters(), lr = 0.01, momentum = 0.0) # Adam에 모델의 학습 파라미터를 넣어줌\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor = 0.5, patience = 250)\n",
    "\n",
    "m1_mlp13_params = {\n",
    "    'num_epochs': 5000,\n",
    "    'loss_func': loss_func,\n",
    "    'optimizer': opt,\n",
    "\n",
    "    'train_dl': train_dl113,\n",
    "    'val_dl': None,\n",
    "    'batch_size': 32,\n",
    "\n",
    "    'sanity_check': False, # True인 경우 데이터 조금만(batch 1개) 사용해서 학습 코드가 잘 돌아가는지 확인 / False면 데이터 전체 사용\n",
    "    'lr_scheduler': lr_scheduler,\n",
    "    'path2weights': '../model/MLP2LGBM_V2/round1/m1_mlp13.pt', # 모델 파라미터를 저장할 경로\n",
    "    'DEVICE': DEVICE, # cpu / cuda\n",
    "    'early_stopping': True, # 학습 조기 종료 적용 여부 -> True: 적용 / False: 미적용\n",
    "    'patience_limit': 500,\n",
    "    'num_folds': n_split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4999, current lr=0.01\n",
      "train loss: 0.046792, val loss: 0.000000, mse: 0.00, time: 0.0020 min\n",
      "----------\n",
      "Epoch 100/4999, current lr=0.01\n",
      "train loss: 0.046902, val loss: 0.000000, mse: 0.00, time: 0.2137 min\n",
      "----------\n",
      "Epoch 200/4999, current lr=0.01\n",
      "train loss: 0.046812, val loss: 0.000000, mse: 0.00, time: 0.4208 min\n",
      "----------\n",
      "Epoch 300/4999, current lr=0.01\n",
      "train loss: 0.046803, val loss: 0.000000, mse: 0.00, time: 0.6230 min\n",
      "----------\n",
      "Epoch 400/4999, current lr=0.01\n",
      "train loss: 0.046797, val loss: 0.000000, mse: 0.00, time: 0.8434 min\n",
      "----------\n",
      "Epoch 500/4999, current lr=0.01\n",
      "train loss: 0.046856, val loss: 0.000000, mse: 0.00, time: 1.0540 min\n",
      "----------\n",
      "Epoch 600/4999, current lr=0.01\n",
      "train loss: 0.046710, val loss: 0.000000, mse: 0.00, time: 1.2556 min\n",
      "----------\n",
      "Epoch 700/4999, current lr=0.01\n",
      "train loss: 0.046731, val loss: 0.000000, mse: 0.00, time: 1.4770 min\n",
      "----------\n",
      "Epoch 800/4999, current lr=0.01\n",
      "train loss: 0.046779, val loss: 0.000000, mse: 0.00, time: 1.6713 min\n",
      "----------\n",
      "Epoch 900/4999, current lr=0.01\n",
      "train loss: 0.046748, val loss: 0.000000, mse: 0.00, time: 1.8974 min\n",
      "----------\n",
      "Epoch 1000/4999, current lr=0.01\n",
      "train loss: 0.046842, val loss: 0.000000, mse: 0.00, time: 2.1034 min\n",
      "----------\n",
      "Epoch 1100/4999, current lr=0.01\n",
      "train loss: 0.046829, val loss: 0.000000, mse: 0.00, time: 2.3192 min\n",
      "----------\n",
      "Epoch 1200/4999, current lr=0.01\n",
      "train loss: 0.046883, val loss: 0.000000, mse: 0.00, time: 2.5357 min\n",
      "----------\n",
      "Epoch 1300/4999, current lr=0.01\n",
      "train loss: 0.046843, val loss: 0.000000, mse: 0.00, time: 2.7420 min\n",
      "----------\n",
      "Epoch 1400/4999, current lr=0.01\n",
      "train loss: 0.046768, val loss: 0.000000, mse: 0.00, time: 2.9479 min\n",
      "----------\n",
      "Epoch 1500/4999, current lr=0.01\n",
      "train loss: 0.046795, val loss: 0.000000, mse: 0.00, time: 3.1538 min\n",
      "----------\n",
      "Epoch 1600/4999, current lr=0.01\n",
      "train loss: 0.046877, val loss: 0.000000, mse: 0.00, time: 3.3453 min\n",
      "----------\n",
      "Epoch 1700/4999, current lr=0.01\n",
      "train loss: 0.046847, val loss: 0.000000, mse: 0.00, time: 3.5515 min\n",
      "----------\n",
      "Epoch 1800/4999, current lr=0.01\n",
      "train loss: 0.046835, val loss: 0.000000, mse: 0.00, time: 3.7527 min\n",
      "----------\n",
      "Epoch 1900/4999, current lr=0.01\n",
      "train loss: 0.046926, val loss: 0.000000, mse: 0.00, time: 3.9837 min\n",
      "----------\n",
      "Epoch 2000/4999, current lr=0.01\n",
      "train loss: 0.046890, val loss: 0.000000, mse: 0.00, time: 4.1966 min\n",
      "----------\n",
      "Epoch 2100/4999, current lr=0.01\n",
      "train loss: 0.046771, val loss: 0.000000, mse: 0.00, time: 4.3998 min\n",
      "----------\n",
      "Epoch 2200/4999, current lr=0.01\n",
      "train loss: 0.046682, val loss: 0.000000, mse: 0.00, time: 4.6196 min\n",
      "----------\n",
      "Epoch 2300/4999, current lr=0.01\n",
      "train loss: 0.046725, val loss: 0.000000, mse: 0.00, time: 4.8247 min\n",
      "----------\n",
      "Epoch 2400/4999, current lr=0.01\n",
      "train loss: 0.046876, val loss: 0.000000, mse: 0.00, time: 5.0440 min\n",
      "----------\n",
      "Epoch 2500/4999, current lr=0.01\n",
      "train loss: 0.046791, val loss: 0.000000, mse: 0.00, time: 5.2519 min\n",
      "----------\n",
      "Epoch 2600/4999, current lr=0.01\n",
      "train loss: 0.046718, val loss: 0.000000, mse: 0.00, time: 5.4536 min\n",
      "----------\n",
      "Epoch 2700/4999, current lr=0.01\n",
      "train loss: 0.046879, val loss: 0.000000, mse: 0.00, time: 5.6615 min\n",
      "----------\n",
      "Epoch 2800/4999, current lr=0.01\n",
      "train loss: 0.046937, val loss: 0.000000, mse: 0.00, time: 5.8570 min\n",
      "----------\n",
      "Epoch 2900/4999, current lr=0.01\n",
      "train loss: 0.046790, val loss: 0.000000, mse: 0.00, time: 6.0665 min\n",
      "----------\n",
      "Epoch 3000/4999, current lr=0.01\n",
      "train loss: 0.046726, val loss: 0.000000, mse: 0.00, time: 6.2882 min\n",
      "----------\n",
      "Epoch 3100/4999, current lr=0.01\n",
      "train loss: 0.046840, val loss: 0.000000, mse: 0.00, time: 6.4818 min\n",
      "----------\n",
      "Epoch 3200/4999, current lr=0.01\n",
      "train loss: 0.046705, val loss: 0.000000, mse: 0.00, time: 6.6995 min\n",
      "----------\n",
      "Epoch 3300/4999, current lr=0.01\n",
      "train loss: 0.046761, val loss: 0.000000, mse: 0.00, time: 6.9108 min\n",
      "----------\n",
      "Epoch 3400/4999, current lr=0.01\n",
      "train loss: 0.046889, val loss: 0.000000, mse: 0.00, time: 7.1296 min\n",
      "----------\n",
      "Epoch 3500/4999, current lr=0.01\n",
      "train loss: 0.046871, val loss: 0.000000, mse: 0.00, time: 7.3466 min\n",
      "----------\n",
      "Epoch 3600/4999, current lr=0.01\n",
      "train loss: 0.046822, val loss: 0.000000, mse: 0.00, time: 7.5474 min\n",
      "----------\n",
      "Epoch 3700/4999, current lr=0.01\n",
      "train loss: 0.046828, val loss: 0.000000, mse: 0.00, time: 7.7586 min\n",
      "----------\n",
      "Epoch 3800/4999, current lr=0.01\n",
      "train loss: 0.046856, val loss: 0.000000, mse: 0.00, time: 7.9572 min\n",
      "----------\n",
      "Epoch 3900/4999, current lr=0.01\n",
      "train loss: 0.046792, val loss: 0.000000, mse: 0.00, time: 8.1649 min\n",
      "----------\n",
      "Epoch 4000/4999, current lr=0.01\n",
      "train loss: 0.046913, val loss: 0.000000, mse: 0.00, time: 8.3868 min\n",
      "----------\n",
      "Epoch 4100/4999, current lr=0.01\n",
      "train loss: 0.046878, val loss: 0.000000, mse: 0.00, time: 8.6079 min\n",
      "----------\n",
      "Epoch 4200/4999, current lr=0.01\n",
      "train loss: 0.046845, val loss: 0.000000, mse: 0.00, time: 8.8207 min\n",
      "----------\n",
      "Epoch 4300/4999, current lr=0.01\n",
      "train loss: 0.046839, val loss: 0.000000, mse: 0.00, time: 9.0162 min\n",
      "----------\n",
      "Epoch 4400/4999, current lr=0.01\n",
      "train loss: 0.046684, val loss: 0.000000, mse: 0.00, time: 9.2217 min\n",
      "----------\n",
      "Epoch 4500/4999, current lr=0.01\n",
      "train loss: 0.046783, val loss: 0.000000, mse: 0.00, time: 9.4506 min\n",
      "----------\n",
      "Epoch 4600/4999, current lr=0.01\n",
      "train loss: 0.046754, val loss: 0.000000, mse: 0.00, time: 9.6564 min\n",
      "----------\n",
      "Epoch 4700/4999, current lr=0.01\n",
      "train loss: 0.046789, val loss: 0.000000, mse: 0.00, time: 9.8836 min\n",
      "----------\n",
      "Epoch 4800/4999, current lr=0.01\n",
      "train loss: 0.046816, val loss: 0.000000, mse: 0.00, time: 10.0839 min\n",
      "----------\n",
      "Epoch 4900/4999, current lr=0.01\n",
      "train loss: 0.046701, val loss: 0.000000, mse: 0.00, time: 10.2970 min\n",
      "----------\n",
      "Epoch 0/4999, current lr=0.01\n",
      "train loss: 0.043887, val loss: 0.000000, mse: 0.00, time: 0.0020 min\n",
      "----------\n",
      "Epoch 100/4999, current lr=0.01\n",
      "train loss: 0.042761, val loss: 0.000000, mse: 0.00, time: 0.2023 min\n",
      "----------\n",
      "Epoch 200/4999, current lr=0.01\n",
      "train loss: 0.042776, val loss: 0.000000, mse: 0.00, time: 0.4085 min\n",
      "----------\n",
      "Epoch 300/4999, current lr=0.01\n",
      "train loss: 0.042826, val loss: 0.000000, mse: 0.00, time: 0.6128 min\n",
      "----------\n",
      "Epoch 400/4999, current lr=0.01\n",
      "train loss: 0.042735, val loss: 0.000000, mse: 0.00, time: 0.8243 min\n",
      "----------\n",
      "Epoch 500/4999, current lr=0.01\n",
      "train loss: 0.042786, val loss: 0.000000, mse: 0.00, time: 1.0515 min\n",
      "----------\n",
      "Epoch 600/4999, current lr=0.01\n",
      "train loss: 0.042811, val loss: 0.000000, mse: 0.00, time: 1.2502 min\n",
      "----------\n",
      "Epoch 700/4999, current lr=0.01\n",
      "train loss: 0.042770, val loss: 0.000000, mse: 0.00, time: 1.4534 min\n",
      "----------\n",
      "Epoch 800/4999, current lr=0.01\n",
      "train loss: 0.042834, val loss: 0.000000, mse: 0.00, time: 1.6539 min\n",
      "----------\n",
      "Epoch 900/4999, current lr=0.01\n",
      "train loss: 0.042796, val loss: 0.000000, mse: 0.00, time: 1.8743 min\n",
      "----------\n",
      "Epoch 1000/4999, current lr=0.01\n",
      "train loss: 0.042755, val loss: 0.000000, mse: 0.00, time: 2.1128 min\n",
      "----------\n",
      "Epoch 1100/4999, current lr=0.01\n",
      "train loss: 0.042753, val loss: 0.000000, mse: 0.00, time: 2.3059 min\n",
      "----------\n",
      "Epoch 1200/4999, current lr=0.01\n",
      "train loss: 0.042758, val loss: 0.000000, mse: 0.00, time: 2.5005 min\n",
      "----------\n",
      "Epoch 1300/4999, current lr=0.01\n",
      "train loss: 0.042773, val loss: 0.000000, mse: 0.00, time: 2.7302 min\n",
      "----------\n",
      "Epoch 1400/4999, current lr=0.01\n",
      "train loss: 0.042758, val loss: 0.000000, mse: 0.00, time: 2.9452 min\n",
      "----------\n",
      "Epoch 1500/4999, current lr=0.01\n",
      "train loss: 0.042703, val loss: 0.000000, mse: 0.00, time: 3.1687 min\n",
      "----------\n",
      "Epoch 1600/4999, current lr=0.01\n",
      "train loss: 0.042685, val loss: 0.000000, mse: 0.00, time: 3.3629 min\n",
      "----------\n",
      "Epoch 1700/4999, current lr=0.01\n",
      "train loss: 0.042751, val loss: 0.000000, mse: 0.00, time: 3.5624 min\n",
      "----------\n",
      "Epoch 1800/4999, current lr=0.01\n",
      "train loss: 0.042773, val loss: 0.000000, mse: 0.00, time: 3.7808 min\n",
      "----------\n",
      "Epoch 1900/4999, current lr=0.01\n",
      "train loss: 0.042657, val loss: 0.000000, mse: 0.00, time: 3.9840 min\n",
      "----------\n",
      "Epoch 2000/4999, current lr=0.01\n",
      "train loss: 0.042705, val loss: 0.000000, mse: 0.00, time: 4.1958 min\n",
      "----------\n",
      "Epoch 2100/4999, current lr=0.01\n",
      "train loss: 0.042678, val loss: 0.000000, mse: 0.00, time: 4.4083 min\n",
      "----------\n",
      "Epoch 2200/4999, current lr=0.01\n",
      "train loss: 0.042784, val loss: 0.000000, mse: 0.00, time: 4.6063 min\n",
      "----------\n",
      "Epoch 2300/4999, current lr=0.01\n",
      "train loss: 0.042781, val loss: 0.000000, mse: 0.00, time: 4.8125 min\n",
      "----------\n",
      "Epoch 2400/4999, current lr=0.01\n",
      "train loss: 0.042825, val loss: 0.000000, mse: 0.00, time: 5.0102 min\n",
      "----------\n",
      "Epoch 2500/4999, current lr=0.01\n",
      "train loss: 0.042785, val loss: 0.000000, mse: 0.00, time: 5.2366 min\n",
      "----------\n",
      "Epoch 2600/4999, current lr=0.01\n",
      "train loss: 0.042747, val loss: 0.000000, mse: 0.00, time: 5.4383 min\n",
      "----------\n",
      "Epoch 2700/4999, current lr=0.01\n",
      "train loss: 0.042781, val loss: 0.000000, mse: 0.00, time: 5.6295 min\n",
      "----------\n",
      "Epoch 2800/4999, current lr=0.01\n",
      "train loss: 0.042766, val loss: 0.000000, mse: 0.00, time: 5.8421 min\n",
      "----------\n",
      "Epoch 2900/4999, current lr=0.01\n",
      "train loss: 0.042731, val loss: 0.000000, mse: 0.00, time: 6.0409 min\n",
      "----------\n",
      "Epoch 3000/4999, current lr=0.01\n",
      "train loss: 0.042763, val loss: 0.000000, mse: 0.00, time: 6.2666 min\n",
      "----------\n",
      "Epoch 3100/4999, current lr=0.01\n",
      "train loss: 0.042774, val loss: 0.000000, mse: 0.00, time: 6.4703 min\n",
      "----------\n",
      "Epoch 3200/4999, current lr=0.01\n",
      "train loss: 0.042794, val loss: 0.000000, mse: 0.00, time: 6.6687 min\n",
      "----------\n",
      "Epoch 3300/4999, current lr=0.01\n",
      "train loss: 0.042802, val loss: 0.000000, mse: 0.00, time: 6.8760 min\n",
      "----------\n",
      "Epoch 3400/4999, current lr=0.01\n",
      "train loss: 0.042829, val loss: 0.000000, mse: 0.00, time: 7.0730 min\n",
      "----------\n",
      "Epoch 3500/4999, current lr=0.01\n",
      "train loss: 0.042706, val loss: 0.000000, mse: 0.00, time: 7.2874 min\n",
      "----------\n",
      "Epoch 3600/4999, current lr=0.01\n",
      "train loss: 0.042756, val loss: 0.000000, mse: 0.00, time: 7.5053 min\n",
      "----------\n",
      "Epoch 3700/4999, current lr=0.01\n",
      "train loss: 0.042792, val loss: 0.000000, mse: 0.00, time: 7.6989 min\n",
      "----------\n",
      "Epoch 3800/4999, current lr=0.01\n",
      "train loss: 0.042780, val loss: 0.000000, mse: 0.00, time: 7.9001 min\n",
      "----------\n",
      "Epoch 3900/4999, current lr=0.01\n",
      "train loss: 0.042776, val loss: 0.000000, mse: 0.00, time: 8.1113 min\n",
      "----------\n",
      "Epoch 4000/4999, current lr=0.01\n",
      "train loss: 0.042732, val loss: 0.000000, mse: 0.00, time: 8.3367 min\n",
      "----------\n",
      "Epoch 4100/4999, current lr=0.01\n",
      "train loss: 0.042794, val loss: 0.000000, mse: 0.00, time: 8.5679 min\n",
      "----------\n",
      "Epoch 4200/4999, current lr=0.01\n",
      "train loss: 0.042817, val loss: 0.000000, mse: 0.00, time: 8.7655 min\n",
      "----------\n",
      "Epoch 4300/4999, current lr=0.01\n",
      "train loss: 0.042739, val loss: 0.000000, mse: 0.00, time: 8.9618 min\n",
      "----------\n",
      "Epoch 4400/4999, current lr=0.01\n",
      "train loss: 0.042725, val loss: 0.000000, mse: 0.00, time: 9.1698 min\n",
      "----------\n",
      "Epoch 4500/4999, current lr=0.01\n",
      "train loss: 0.042765, val loss: 0.000000, mse: 0.00, time: 9.3797 min\n",
      "----------\n",
      "Epoch 4600/4999, current lr=0.01\n",
      "train loss: 0.042786, val loss: 0.000000, mse: 0.00, time: 9.6000 min\n",
      "----------\n",
      "Epoch 4700/4999, current lr=0.01\n",
      "train loss: 0.042764, val loss: 0.000000, mse: 0.00, time: 9.7911 min\n",
      "----------\n",
      "Epoch 4800/4999, current lr=0.01\n",
      "train loss: 0.042797, val loss: 0.000000, mse: 0.00, time: 9.9971 min\n",
      "----------\n",
      "Epoch 4900/4999, current lr=0.01\n",
      "train loss: 0.042838, val loss: 0.000000, mse: 0.00, time: 10.1975 min\n",
      "----------\n",
      "Epoch 0/4999, current lr=0.01\n",
      "train loss: 0.043640, val loss: 0.000000, mse: 0.00, time: 0.0027 min\n",
      "----------\n",
      "Epoch 100/4999, current lr=0.01\n",
      "train loss: 0.042646, val loss: 0.000000, mse: 0.00, time: 0.2768 min\n",
      "----------\n",
      "Epoch 200/4999, current lr=0.01\n",
      "train loss: 0.042592, val loss: 0.000000, mse: 0.00, time: 0.5177 min\n",
      "----------\n",
      "Epoch 300/4999, current lr=0.01\n",
      "train loss: 0.042678, val loss: 0.000000, mse: 0.00, time: 0.7679 min\n",
      "----------\n",
      "Epoch 400/4999, current lr=0.01\n",
      "train loss: 0.042614, val loss: 0.000000, mse: 0.00, time: 1.0187 min\n",
      "----------\n",
      "Epoch 500/4999, current lr=0.01\n",
      "train loss: 0.042614, val loss: 0.000000, mse: 0.00, time: 1.2987 min\n",
      "----------\n",
      "Epoch 600/4999, current lr=0.01\n",
      "train loss: 0.042678, val loss: 0.000000, mse: 0.00, time: 1.5461 min\n",
      "----------\n",
      "Epoch 700/4999, current lr=0.01\n",
      "train loss: 0.042639, val loss: 0.000000, mse: 0.00, time: 1.7863 min\n",
      "----------\n",
      "Epoch 800/4999, current lr=0.01\n",
      "train loss: 0.042540, val loss: 0.000000, mse: 0.00, time: 2.0422 min\n",
      "----------\n",
      "Epoch 900/4999, current lr=0.01\n",
      "train loss: 0.042593, val loss: 0.000000, mse: 0.00, time: 2.3148 min\n",
      "----------\n",
      "Epoch 1000/4999, current lr=0.01\n",
      "train loss: 0.042581, val loss: 0.000000, mse: 0.00, time: 2.5664 min\n",
      "----------\n",
      "Epoch 1100/4999, current lr=0.01\n",
      "train loss: 0.042637, val loss: 0.000000, mse: 0.00, time: 2.8232 min\n",
      "----------\n",
      "Epoch 1200/4999, current lr=0.01\n",
      "train loss: 0.042660, val loss: 0.000000, mse: 0.00, time: 3.0876 min\n",
      "----------\n",
      "Epoch 1300/4999, current lr=0.01\n",
      "train loss: 0.042556, val loss: 0.000000, mse: 0.00, time: 3.3612 min\n",
      "----------\n",
      "Epoch 1400/4999, current lr=0.01\n",
      "train loss: 0.042544, val loss: 0.000000, mse: 0.00, time: 3.6263 min\n",
      "----------\n",
      "Epoch 1500/4999, current lr=0.01\n",
      "train loss: 0.042586, val loss: 0.000000, mse: 0.00, time: 3.8697 min\n",
      "----------\n",
      "Epoch 1600/4999, current lr=0.01\n",
      "train loss: 0.042602, val loss: 0.000000, mse: 0.00, time: 4.1265 min\n",
      "----------\n",
      "Epoch 1700/4999, current lr=0.01\n",
      "train loss: 0.042598, val loss: 0.000000, mse: 0.00, time: 4.3977 min\n",
      "----------\n",
      "Epoch 1800/4999, current lr=0.01\n",
      "train loss: 0.042563, val loss: 0.000000, mse: 0.00, time: 4.6668 min\n",
      "----------\n",
      "Epoch 1900/4999, current lr=0.01\n",
      "train loss: 0.042628, val loss: 0.000000, mse: 0.00, time: 4.9087 min\n",
      "----------\n",
      "Epoch 2000/4999, current lr=0.01\n",
      "train loss: 0.042593, val loss: 0.000000, mse: 0.00, time: 5.1698 min\n",
      "----------\n",
      "Epoch 2100/4999, current lr=0.01\n",
      "train loss: 0.042635, val loss: 0.000000, mse: 0.00, time: 5.4328 min\n",
      "----------\n",
      "Epoch 2200/4999, current lr=0.01\n",
      "train loss: 0.042631, val loss: 0.000000, mse: 0.00, time: 5.6950 min\n",
      "----------\n",
      "Epoch 2300/4999, current lr=0.01\n",
      "train loss: 0.042650, val loss: 0.000000, mse: 0.00, time: 5.9521 min\n",
      "----------\n",
      "Epoch 2400/4999, current lr=0.01\n",
      "train loss: 0.042582, val loss: 0.000000, mse: 0.00, time: 6.2125 min\n",
      "----------\n",
      "Epoch 2500/4999, current lr=0.01\n",
      "train loss: 0.042623, val loss: 0.000000, mse: 0.00, time: 6.4766 min\n",
      "----------\n",
      "Epoch 2600/4999, current lr=0.01\n",
      "train loss: 0.042638, val loss: 0.000000, mse: 0.00, time: 6.7523 min\n",
      "----------\n",
      "Epoch 2700/4999, current lr=0.01\n",
      "train loss: 0.042635, val loss: 0.000000, mse: 0.00, time: 6.9928 min\n",
      "----------\n",
      "Epoch 2800/4999, current lr=0.01\n",
      "train loss: 0.042626, val loss: 0.000000, mse: 0.00, time: 7.2475 min\n",
      "----------\n",
      "Epoch 2900/4999, current lr=0.01\n",
      "train loss: 0.042620, val loss: 0.000000, mse: 0.00, time: 7.4947 min\n",
      "----------\n",
      "Epoch 3000/4999, current lr=0.01\n",
      "train loss: 0.042596, val loss: 0.000000, mse: 0.00, time: 7.7735 min\n",
      "----------\n",
      "Epoch 3100/4999, current lr=0.01\n",
      "train loss: 0.042587, val loss: 0.000000, mse: 0.00, time: 8.0189 min\n",
      "----------\n",
      "Epoch 3200/4999, current lr=0.01\n",
      "train loss: 0.042587, val loss: 0.000000, mse: 0.00, time: 8.2628 min\n",
      "----------\n",
      "Epoch 3300/4999, current lr=0.01\n",
      "train loss: 0.042579, val loss: 0.000000, mse: 0.00, time: 8.5159 min\n",
      "----------\n",
      "Epoch 3400/4999, current lr=0.01\n",
      "train loss: 0.042621, val loss: 0.000000, mse: 0.00, time: 8.7929 min\n",
      "----------\n",
      "Epoch 3500/4999, current lr=0.01\n",
      "train loss: 0.042587, val loss: 0.000000, mse: 0.00, time: 9.0412 min\n",
      "----------\n",
      "Epoch 3600/4999, current lr=0.01\n",
      "train loss: 0.042550, val loss: 0.000000, mse: 0.00, time: 9.2877 min\n",
      "----------\n",
      "Epoch 3700/4999, current lr=0.01\n",
      "train loss: 0.042553, val loss: 0.000000, mse: 0.00, time: 9.5451 min\n",
      "----------\n",
      "Epoch 3800/4999, current lr=0.01\n",
      "train loss: 0.042621, val loss: 0.000000, mse: 0.00, time: 9.8079 min\n",
      "----------\n",
      "Epoch 3900/4999, current lr=0.01\n",
      "train loss: 0.042570, val loss: 0.000000, mse: 0.00, time: 10.0733 min\n",
      "----------\n",
      "Epoch 4000/4999, current lr=0.01\n",
      "train loss: 0.042595, val loss: 0.000000, mse: 0.00, time: 10.3140 min\n",
      "----------\n",
      "Epoch 4100/4999, current lr=0.01\n",
      "train loss: 0.042615, val loss: 0.000000, mse: 0.00, time: 10.5674 min\n",
      "----------\n",
      "Epoch 4200/4999, current lr=0.01\n",
      "train loss: 0.042470, val loss: 0.000000, mse: 0.00, time: 10.8312 min\n",
      "----------\n",
      "Epoch 4300/4999, current lr=0.01\n",
      "train loss: 0.042597, val loss: 0.000000, mse: 0.00, time: 11.1010 min\n",
      "----------\n",
      "Epoch 4400/4999, current lr=0.01\n",
      "train loss: 0.042640, val loss: 0.000000, mse: 0.00, time: 11.3414 min\n",
      "----------\n",
      "Epoch 4500/4999, current lr=0.01\n",
      "train loss: 0.042548, val loss: 0.000000, mse: 0.00, time: 11.5838 min\n",
      "----------\n",
      "Epoch 4600/4999, current lr=0.01\n",
      "train loss: 0.042650, val loss: 0.000000, mse: 0.00, time: 11.8365 min\n",
      "----------\n",
      "Epoch 4700/4999, current lr=0.01\n",
      "train loss: 0.042613, val loss: 0.000000, mse: 0.00, time: 12.0971 min\n",
      "----------\n",
      "Epoch 4800/4999, current lr=0.01\n",
      "train loss: 0.042670, val loss: 0.000000, mse: 0.00, time: 12.3419 min\n",
      "----------\n",
      "Epoch 4900/4999, current lr=0.01\n",
      "train loss: 0.042536, val loss: 0.000000, mse: 0.00, time: 12.5953 min\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MLP_regressor(\n",
       "   (fc1): Linear(in_features=6, out_features=32, bias=True)\n",
       "   (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
       "   (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
       "   (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
       "   (fc5): Linear(in_features=64, out_features=32, bias=True)\n",
       "   (fc6): Linear(in_features=32, out_features=16, bias=True)\n",
       "   (fc7): Linear(in_features=16, out_features=8, bias=True)\n",
       "   (fc8): Linear(in_features=8, out_features=1, bias=True)\n",
       "   (dropout): Dropout(p=0.4, inplace=False)\n",
       " ),\n",
       " {'train': [0.0436403494482198,\n",
       "   0.04263676601993151,\n",
       "   0.04267143222911299,\n",
       "   0.04263084789445577,\n",
       "   0.04257795086576919,\n",
       "   0.04256741640981564,\n",
       "   0.04265158604984441,\n",
       "   0.042678090264974544,\n",
       "   0.04264369638990765,\n",
       "   0.042558369533089564,\n",
       "   0.04259567659748487,\n",
       "   0.04258428100219443,\n",
       "   0.04261304260778033,\n",
       "   0.04261780442777744,\n",
       "   0.042571498958532475,\n",
       "   0.042567377297346255,\n",
       "   0.04259922233494845,\n",
       "   0.04250781723290436,\n",
       "   0.042615331450769726,\n",
       "   0.04258488869864093,\n",
       "   0.0426102659672745,\n",
       "   0.04267541613460572,\n",
       "   0.042628189157848516,\n",
       "   0.04252635292277848,\n",
       "   0.042545116169393556,\n",
       "   0.04255050987251534,\n",
       "   0.04261014978747722,\n",
       "   0.04257845388463706,\n",
       "   0.04264501229790617,\n",
       "   0.04258311425358796,\n",
       "   0.04266100668710125,\n",
       "   0.0425748867436874,\n",
       "   0.04261814929729651,\n",
       "   0.04258362197678937,\n",
       "   0.04260224044815568,\n",
       "   0.04257853728187971,\n",
       "   0.04258970881789184,\n",
       "   0.042622499081714095,\n",
       "   0.04261648364303526,\n",
       "   0.042640697759045054,\n",
       "   0.04258893637125157,\n",
       "   0.04260138694412452,\n",
       "   0.04261296327449073,\n",
       "   0.042637976536080854,\n",
       "   0.042653791963561505,\n",
       "   0.04259940119814282,\n",
       "   0.042601385146133174,\n",
       "   0.04260392420548053,\n",
       "   0.042621587771029514,\n",
       "   0.042611411681845165,\n",
       "   0.04266585949038671,\n",
       "   0.04261141641080872,\n",
       "   0.042596043436980445,\n",
       "   0.04264827730241886,\n",
       "   0.042590824779400155,\n",
       "   0.04263729802833116,\n",
       "   0.04264323583319168,\n",
       "   0.04261914393133368,\n",
       "   0.04255559863137805,\n",
       "   0.04258917841044339,\n",
       "   0.04257643535610073,\n",
       "   0.04265574992688234,\n",
       "   0.042524822782879036,\n",
       "   0.042601749054656544,\n",
       "   0.04257564022521342,\n",
       "   0.04261888221275708,\n",
       "   0.04266429740535326,\n",
       "   0.04257094919188949,\n",
       "   0.04261804444730775,\n",
       "   0.042665195169527666,\n",
       "   0.04265108517378815,\n",
       "   0.04262318251546749,\n",
       "   0.04262517473914407,\n",
       "   0.042609546475174016,\n",
       "   0.04255831748986047,\n",
       "   0.042576247626099704,\n",
       "   0.04254009410862095,\n",
       "   0.042619202304477535,\n",
       "   0.04256169830472016,\n",
       "   0.042571309134979876,\n",
       "   0.04262773882259022,\n",
       "   0.04263559698073332,\n",
       "   0.04264816947219786,\n",
       "   0.042661324069519675,\n",
       "   0.0426091421734203,\n",
       "   0.04263129717062328,\n",
       "   0.04264471971791638,\n",
       "   0.04258092878278622,\n",
       "   0.042661934105817936,\n",
       "   0.042620987192658354,\n",
       "   0.04259122568221132,\n",
       "   0.04258900939925643,\n",
       "   0.0425653257153251,\n",
       "   0.042584522770456046,\n",
       "   0.042635183935323034,\n",
       "   0.042672678409529124,\n",
       "   0.04258586478627418,\n",
       "   0.04260319407321205,\n",
       "   0.04258649442806717,\n",
       "   0.04251655852499087,\n",
       "   0.04264628783730436,\n",
       "   0.04261232018470764,\n",
       "   0.04260333712435951,\n",
       "   0.04259993197504154,\n",
       "   0.04265864565352763,\n",
       "   0.042528590215139155,\n",
       "   0.04257319623773748,\n",
       "   0.042617980827969956,\n",
       "   0.042610792483180025,\n",
       "   0.042584526415698785,\n",
       "   0.042650146129702735,\n",
       "   0.04261780312238646,\n",
       "   0.04263202814031238,\n",
       "   0.04254761175675826,\n",
       "   0.0426491057577212,\n",
       "   0.0425781006409117,\n",
       "   0.042537931962446734,\n",
       "   0.04258739770936572,\n",
       "   0.04264417704471872,\n",
       "   0.04266638305069001,\n",
       "   0.04256726197959963,\n",
       "   0.04263949783380366,\n",
       "   0.04260651774642881,\n",
       "   0.04261779100441736,\n",
       "   0.042594619526350797,\n",
       "   0.04255914407328141,\n",
       "   0.04259989448815338,\n",
       "   0.04256417928648389,\n",
       "   0.04259113999437695,\n",
       "   0.042630830283992546,\n",
       "   0.04254741873130326,\n",
       "   0.04260139790448275,\n",
       "   0.04261594326042932,\n",
       "   0.042709248223580604,\n",
       "   0.04262026484347572,\n",
       "   0.042574734825733276,\n",
       "   0.04255400403472018,\n",
       "   0.042639629210322355,\n",
       "   0.04259439672320342,\n",
       "   0.042617634160459536,\n",
       "   0.04260567392199493,\n",
       "   0.04264581681283052,\n",
       "   0.04257725787556861,\n",
       "   0.04265738725662231,\n",
       "   0.04262082209764433,\n",
       "   0.042519893183195885,\n",
       "   0.042616742701569864,\n",
       "   0.042618201143485454,\n",
       "   0.04259207364448831,\n",
       "   0.04253105885726361,\n",
       "   0.042597610645057744,\n",
       "   0.04261661073393073,\n",
       "   0.04259530098970271,\n",
       "   0.04259626835830941,\n",
       "   0.042617163185245735,\n",
       "   0.04261324679063371,\n",
       "   0.04254737358447934,\n",
       "   0.04257565588990519,\n",
       "   0.04261395167713323,\n",
       "   0.04260780872884861,\n",
       "   0.04266757004517169,\n",
       "   0.04263530959767744,\n",
       "   0.042557567259496894,\n",
       "   0.04266878706364592,\n",
       "   0.042582130924729275,\n",
       "   0.04261295376730359,\n",
       "   0.042654131562256614,\n",
       "   0.042546850393626316,\n",
       "   0.04255730982654351,\n",
       "   0.042561261688382174,\n",
       "   0.04262345319937083,\n",
       "   0.042617280424133805,\n",
       "   0.042596796475166135,\n",
       "   0.0426707032790854,\n",
       "   0.04266563225383601,\n",
       "   0.04267830725543755,\n",
       "   0.042622655925671916,\n",
       "   0.0426089865609634,\n",
       "   0.04263730640253745,\n",
       "   0.04266911429807174,\n",
       "   0.042650813898764364,\n",
       "   0.04262811300183131,\n",
       "   0.04262130733363884,\n",
       "   0.042601092196693106,\n",
       "   0.042679940299554305,\n",
       "   0.042625346311852955,\n",
       "   0.04263618795339726,\n",
       "   0.042625822286960505,\n",
       "   0.04255633785212336,\n",
       "   0.04258100355952239,\n",
       "   0.04263535117314867,\n",
       "   0.04272033966277256,\n",
       "   0.04261726628650318,\n",
       "   0.04262520998470054,\n",
       "   0.042641980983009024,\n",
       "   0.04257570859814478,\n",
       "   0.042609433743579325,\n",
       "   0.04268970129903683,\n",
       "   0.04261264328129035,\n",
       "   0.04259105364153208,\n",
       "   0.042592317457041465,\n",
       "   0.04267288850358695,\n",
       "   0.042598443533763415,\n",
       "   0.04257305833426389,\n",
       "   0.04258512127990565,\n",
       "   0.042637085422011445,\n",
       "   0.042628058052260026,\n",
       "   0.042665157584119436,\n",
       "   0.04264425679671863,\n",
       "   0.0426492067900571,\n",
       "   0.0425422692101849,\n",
       "   0.04262811563724329,\n",
       "   0.042607141969617734,\n",
       "   0.04258972701947551,\n",
       "   0.042636532921436404,\n",
       "   0.04268200358083425,\n",
       "   0.04259472779991213,\n",
       "   0.042617827579994834,\n",
       "   0.0426373956617245,\n",
       "   0.042583890887331374,\n",
       "   0.04256170052142182,\n",
       "   0.042630626101139164,\n",
       "   0.0426639288417564,\n",
       "   0.04253136685564498,\n",
       "   0.04261530888967278,\n",
       "   0.042635247628550886,\n",
       "   0.04261800856137079,\n",
       "   0.04259065424115205,\n",
       "   0.042636003696228844,\n",
       "   0.0426464387207977,\n",
       "   0.042593844616708676,\n",
       "   0.04260304838665261,\n",
       "   0.04263643186447049,\n",
       "   0.04259234624953309,\n",
       "   0.04261751241427808,\n",
       "   0.04260344313195914,\n",
       "   0.04261127124147967,\n",
       "   0.04258775528797434,\n",
       "   0.04259346482182337,\n",
       "   0.04259635722341616,\n",
       "   0.042542053623632954,\n",
       "   0.04266348264434121,\n",
       "   0.042609985308213666,\n",
       "   0.042626433530129675,\n",
       "   0.04265032055949377,\n",
       "   0.042564218891553644,\n",
       "   0.042582901647268245,\n",
       "   0.042614840574500976,\n",
       "   0.04258235000874385,\n",
       "   0.04260320766898226,\n",
       "   0.04264986692381299,\n",
       "   0.04262261986732483,\n",
       "   0.0426587852564725,\n",
       "   0.0426428331816492,\n",
       "   0.042532731260149934,\n",
       "   0.04262281675969273,\n",
       "   0.04266148180015816,\n",
       "   0.042560935660827255,\n",
       "   0.04256580777404722,\n",
       "   0.042498823975728565,\n",
       "   0.04254152961998932,\n",
       "   0.04264554243442441,\n",
       "   0.0426093382529976,\n",
       "   0.042587059243651464,\n",
       "   0.042618538131398605,\n",
       "   0.04259729725270232,\n",
       "   0.04263808596725306,\n",
       "   0.04263041778044267,\n",
       "   0.04265087421767968,\n",
       "   0.04261654130190857,\n",
       "   0.042645854299718684,\n",
       "   0.04265651907310013,\n",
       "   0.04261344651545375,\n",
       "   0.04268144117899177,\n",
       "   0.04259897251267079,\n",
       "   0.042670460427102964,\n",
       "   0.04262656505442848,\n",
       "   0.04255394989793951,\n",
       "   0.042611682119448324,\n",
       "   0.04254149159124075,\n",
       "   0.04260418144139377,\n",
       "   0.04264780630257504,\n",
       "   0.04259440349645851,\n",
       "   0.042540022041186815,\n",
       "   0.042642383979371756,\n",
       "   0.04261119705586394,\n",
       "   0.042621289944845785,\n",
       "   0.04266030064299087,\n",
       "   0.04258016503054248,\n",
       "   0.04260529506305032,\n",
       "   0.04262858850896851,\n",
       "   0.042625599212882934,\n",
       "   0.04263725955624226,\n",
       "   0.04261611222235624,\n",
       "   0.04257477677065479,\n",
       "   0.042589476285887155,\n",
       "   0.04254166491268095,\n",
       "   0.04262625594769628,\n",
       "   0.04264095430531777,\n",
       "   0.042631939669285926,\n",
       "   0.04267780423656968,\n",
       "   0.042641303903800394,\n",
       "   0.04256210103015269,\n",
       "   0.042603908516158744,\n",
       "   0.0426825283972685,\n",
       "   0.0425910230510491,\n",
       "   0.042584592916748744,\n",
       "   0.04266532398452443,\n",
       "   0.042626275553191005,\n",
       "   0.04264779741113836,\n",
       "   0.04256044049893529,\n",
       "   0.04261448045900045,\n",
       "   0.042602289412632464,\n",
       "   0.04264709131776794,\n",
       "   0.042624762013924025,\n",
       "   0.042623501178646876,\n",
       "   0.04261145143469503,\n",
       "   0.04260867962167283,\n",
       "   0.04265137822174829,\n",
       "   0.04262962324067581,\n",
       "   0.042523441285141246,\n",
       "   0.04265483208924286,\n",
       "   0.04253446784886447,\n",
       "   0.04260217074520332,\n",
       "   0.04261136926895331,\n",
       "   0.04256051970907479,\n",
       "   0.04257264028895985,\n",
       "   0.04260007147946634,\n",
       "   0.04261768952874113,\n",
       "   0.04257054149119322,\n",
       "   0.042619454023266626,\n",
       "   0.042680516371057055,\n",
       "   0.042660088726311676,\n",
       "   0.042661419880291644,\n",
       "   0.0425245470498219,\n",
       "   0.0426265114348782,\n",
       "   0.04267021644213968,\n",
       "   0.0426459641496012,\n",
       "   0.04262956947334542,\n",
       "   0.042631474309716345,\n",
       "   0.0425973359710914,\n",
       "   0.042628112681641066,\n",
       "   0.042604297177850706,\n",
       "   0.04253721271664643,\n",
       "   0.04265002869377452,\n",
       "   0.042598894509402185,\n",
       "   0.04264414694683611,\n",
       "   0.04262654633561442,\n",
       "   0.04262947718466609,\n",
       "   0.04264385306145534,\n",
       "   0.042556722523752324,\n",
       "   0.042595525122871084,\n",
       "   0.04258054531802816,\n",
       "   0.042604103462755186,\n",
       "   0.04249628289671969,\n",
       "   0.04262950326785568,\n",
       "   0.04259863862813997,\n",
       "   0.04261719705152117,\n",
       "   0.04261601429340268,\n",
       "   0.04261695062818606,\n",
       "   0.0426556493625168,\n",
       "   0.042547066620558745,\n",
       "   0.04263738878994934,\n",
       "   0.04256220422993021,\n",
       "   0.042602048826611734,\n",
       "   0.04264693787275267,\n",
       "   0.04266026576688467,\n",
       "   0.04261644689504765,\n",
       "   0.0426111233874786,\n",
       "   0.042626425303703495,\n",
       "   0.042575476189290196,\n",
       "   0.0426148601799957,\n",
       "   0.04267544687286881,\n",
       "   0.042587507115907905,\n",
       "   0.04257618553382306,\n",
       "   0.04259485119630482,\n",
       "   0.042626181614300436,\n",
       "   0.04267629724888762,\n",
       "   0.04261734185139995,\n",
       "   0.04262712336768789,\n",
       "   0.042526383390111375,\n",
       "   0.042639009469796804,\n",
       "   0.04260855876217204,\n",
       "   0.042585246671329846,\n",
       "   0.04261156143235766,\n",
       "   0.04262720967127272,\n",
       "   0.04251824967132127,\n",
       "   0.04261043598829222,\n",
       "   0.042576915937021745,\n",
       "   0.04260052940077033,\n",
       "   0.042646864105847254,\n",
       "   0.04267775298150118,\n",
       "   0.042626242992306544,\n",
       "   0.042610362295276864,\n",
       "   0.04262175816149751,\n",
       "   0.0426142845764633,\n",
       "   0.04264878881864311,\n",
       "   0.0426321433102789,\n",
       "   0.042673088400817114,\n",
       "   0.04264401369843601,\n",
       "   0.042613500553714344,\n",
       "   0.042643148839966324,\n",
       "   0.042682921492363794,\n",
       "   0.04261470767092114,\n",
       "   0.04266795880538373,\n",
       "   0.042611085925220456,\n",
       "   0.04259612240081976,\n",
       "   0.042669669113868526,\n",
       "   0.04263532560718946,\n",
       "   0.042649677617490785,\n",
       "   0.04268361029546123,\n",
       "   0.042592272039287345,\n",
       "   0.04258978307739762,\n",
       "   0.0426263049614331,\n",
       "   0.04269042162855795,\n",
       "   0.04258891087918242,\n",
       "   0.042637283545880277,\n",
       "   0.04255122869960533,\n",
       "   0.04264991958279255,\n",
       "   0.04256765539488517,\n",
       "   0.04263645142070518,\n",
       "   0.04261970153032255,\n",
       "   0.04259577304863733,\n",
       "   0.042582574462102465,\n",
       "   0.042620147309027426,\n",
       "   0.04259381368140544,\n",
       "   0.042553899578811706,\n",
       "   0.042658549892015694,\n",
       "   0.04265556069445019,\n",
       "   0.04265452155396958,\n",
       "   0.04260759550677844,\n",
       "   0.04256303605954509,\n",
       "   0.04266749004687159,\n",
       "   0.042632206658686486,\n",
       "   0.04256263146223115,\n",
       "   0.04262219014739202,\n",
       "   0.042578212239525536,\n",
       "   0.042666493368542883,\n",
       "   0.0425547436741758,\n",
       "   0.04258300598002662,\n",
       "   0.042635544420273834,\n",
       "   0.0426306001903597,\n",
       "   0.04261253067284576,\n",
       "   0.04263965221475964,\n",
       "   0.04262167909913812,\n",
       "   0.04254802509772876,\n",
       "   0.04258372566916726,\n",
       "   0.04260880488994693,\n",
       "   0.04260974762853512,\n",
       "   0.04256207213914099,\n",
       "   0.04264599055298104,\n",
       "   0.042652483518458593,\n",
       "   0.042654595246984936,\n",
       "   0.04261038953607733,\n",
       "   0.042650883823386894,\n",
       "   0.04256563831951993,\n",
       "   0.042592444942017235,\n",
       "   0.04264100391017504,\n",
       "   0.04265818507218164,\n",
       "   0.04261893275355504,\n",
       "   0.04261790161783045,\n",
       "   0.042608312240316845,\n",
       "   0.042572949912922445,\n",
       "   0.042656597913789354,\n",
       "   0.04263228493288529,\n",
       "   0.04262326093744641,\n",
       "   0.042600828310674875,\n",
       "   0.042551446355078834,\n",
       "   0.04263160938073781,\n",
       "   0.04262895219582172,\n",
       "   0.042587800607208376,\n",
       "   0.0426373075355183,\n",
       "   0.042587279214346706,\n",
       "   0.042519006699569956,\n",
       "   0.042624731645111213,\n",
       "   0.04262953415389888,\n",
       "   0.042592401987264966,\n",
       "   0.0426788701498804,\n",
       "   0.04254272984079093,\n",
       "   0.04262300153409154,\n",
       "   0.04261009321232473,\n",
       "   0.04264710609577904,\n",
       "   0.04256424864461599,\n",
       "   0.042642001844634694,\n",
       "   0.042609688910571014,\n",
       "   0.042626173338614216,\n",
       "   0.04263159962725048,\n",
       "   0.04260483467874448,\n",
       "   0.04256286505332663,\n",
       "   0.04261973431287718,\n",
       "   0.04265958627393423,\n",
       "   0.04261544947781839,\n",
       "   0.04259477353785649,\n",
       "   0.04262652180411599,\n",
       "   0.04262820364029939,\n",
       "   0.04261359392611448,\n",
       "   0.04260623876220924,\n",
       "   0.04264847586962802,\n",
       "   0.04264283973323412,\n",
       "   0.04260318532955548,\n",
       "   0.042614215489261406,\n",
       "   0.04256162219796299,\n",
       "   0.0426131828757357,\n",
       "   0.04258985947971502,\n",
       "   0.04262608516314798,\n",
       "   0.04260444591853244,\n",
       "   0.04259405865156946,\n",
       "   0.042643060516719976,\n",
       "   0.042617089837050634,\n",
       "   0.04262400166062284,\n",
       "   0.042625584878212164,\n",
       "   0.04262296045122068,\n",
       "   0.04253560848965132,\n",
       "   0.04249661619013006,\n",
       "   0.04264881665056402,\n",
       "   0.042533452500981735,\n",
       "   0.04260338140913278,\n",
       "   0.04253987854669902,\n",
       "   0.04265630594955003,\n",
       "   0.04257269629762193,\n",
       "   0.04257968639539293,\n",
       "   0.04256915903288471,\n",
       "   0.04263630686712659,\n",
       "   0.0425915758964444,\n",
       "   0.04261102774911676,\n",
       "   0.0426417284267993,\n",
       "   0.04263372253780522,\n",
       "   0.04261540699103647,\n",
       "   0.04256527170169452,\n",
       "   0.0426636449069031,\n",
       "   0.0425922458822077,\n",
       "   0.0425955646786808,\n",
       "   0.04262444465613562,\n",
       "   0.04265410845929926,\n",
       "   0.042608592677707516,\n",
       "   0.04264386621388522,\n",
       "   0.042658540434088586,\n",
       "   0.04265028445188664,\n",
       "   0.042598584540619336,\n",
       "   0.0425989863793712,\n",
       "   0.042581800611551146,\n",
       "   0.04269127395034822,\n",
       "   0.04255058817134416,\n",
       "   0.04264228186331505,\n",
       "   0.04249024455212364,\n",
       "   0.04261852194947645,\n",
       "   0.04259676086015938,\n",
       "   0.042597611827298626,\n",
       "   0.04259263720394166,\n",
       "   0.04258059066189222,\n",
       "   0.04266394342272735,\n",
       "   0.04259487395444192,\n",
       "   0.042595893587947874,\n",
       "   0.042639367762675955,\n",
       "   0.04262040129377822,\n",
       "   0.04254130361493954,\n",
       "   0.042643477305893074,\n",
       "   0.0426410443280354,\n",
       "   0.042559529927151264,\n",
       "   0.04260869538488467,\n",
       "   0.042617531650322525,\n",
       "   0.04263036388996219,\n",
       "   0.04259038570006032,\n",
       "   0.042678205878281396,\n",
       "   0.04267466787464363,\n",
       "   0.04264063280968627,\n",
       "   0.04250109200635232,\n",
       "   0.04265695514757771,\n",
       "   0.04264203674537091,\n",
       "   0.042643479029994366,\n",
       "   0.04255909318766318,\n",
       "   0.042633497567216225,\n",
       "   0.04261175433466257,\n",
       "   0.04262144920254542,\n",
       "   0.04262105842267186,\n",
       "   0.042491605533056025,\n",
       "   0.04260701024827878,\n",
       "   0.04251539052025346,\n",
       "   0.04266155307943171,\n",
       "   0.042645777330910864,\n",
       "   0.042553176466098505,\n",
       "   0.04263819633436597,\n",
       "   0.04264921013973961,\n",
       "   0.04260502797512969,\n",
       "   0.042568794533240896,\n",
       "   0.04267493565220478,\n",
       "   0.042576584564752815,\n",
       "   0.04257606327041122,\n",
       "   0.04260385982261217,\n",
       "   0.04261232727815297,\n",
       "   0.04265129083444264,\n",
       "   0.04257555293642785,\n",
       "   0.04265121684586706,\n",
       "   0.042583977043136094,\n",
       "   0.04256939922482514,\n",
       "   0.04263236663065666,\n",
       "   0.04260729743429452,\n",
       "   0.04262512350870558,\n",
       "   0.04255739435676701,\n",
       "   0.04256920159355668,\n",
       "   0.04267786366880433,\n",
       "   0.04262710472276388,\n",
       "   0.04263567658495312,\n",
       "   0.04262607437519988,\n",
       "   0.042626198609013205,\n",
       "   0.04255624268173186,\n",
       "   0.042572479651979175,\n",
       "   0.04254365780136802,\n",
       "   0.04261328299676091,\n",
       "   0.042681446104995475,\n",
       "   0.04265586201809655,\n",
       "   0.04261541504505252,\n",
       "   0.04263156585949512,\n",
       "   0.04260113027470171,\n",
       "   0.042545000580716724,\n",
       "   0.042571805848563016,\n",
       "   0.04254678007492349,\n",
       "   0.0426660434519949,\n",
       "   0.04263281721221514,\n",
       "   0.04254493836528999,\n",
       "   0.04256310256059505,\n",
       "   0.04253915164096296,\n",
       "   0.04260237820384916,\n",
       "   0.04260341517688814,\n",
       "   0.04268338606377278,\n",
       "   0.04264877601103349,\n",
       "   0.042601399234503755,\n",
       "   0.04264818469354929,\n",
       "   0.042602314091911,\n",
       "   0.04261638167475866,\n",
       "   0.04255195506348097,\n",
       "   0.042616193156597044,\n",
       "   0.04269910986758461,\n",
       "   0.042655414293620214,\n",
       "   0.04255836256279433,\n",
       "   0.04258423206234766,\n",
       "   0.04262982700481888,\n",
       "   0.04261800816729049,\n",
       "   0.042492227711953406,\n",
       "   0.04264813016268833,\n",
       "   0.04257499085477561,\n",
       "   0.04255891117182645,\n",
       "   0.042638704574797764,\n",
       "   0.04261051175022913,\n",
       "   0.04264390636081538,\n",
       "   0.04258915629268678,\n",
       "   0.042602877528214254,\n",
       "   0.04259169109104093,\n",
       "   0.04263692759285288,\n",
       "   0.04262014361452465,\n",
       "   0.04261918609792536,\n",
       "   0.042637224064385595,\n",
       "   0.04255466825705914,\n",
       "   0.04261558610053102,\n",
       "   0.042614670873673495,\n",
       "   0.042541941187598485,\n",
       "   0.04267121381010891,\n",
       "   0.04266301570845044,\n",
       "   0.04261791558305094,\n",
       "   0.04258622069004153,\n",
       "   0.04262158619470833,\n",
       "   0.04268066873235151,\n",
       "   0.042627584441634246,\n",
       "   0.042677324911779606,\n",
       "   0.042639406086984744,\n",
       "   0.04255502485046702,\n",
       "   0.04261707665999074,\n",
       "   0.042585169234551676,\n",
       "   0.04262067710072541,\n",
       "   0.042574416064033824,\n",
       "   0.04256403239305354,\n",
       "   0.04262742877991731,\n",
       "   0.042646031020101435,\n",
       "   0.04264725855559357,\n",
       "   0.04265580891577665,\n",
       "   0.04264717279386915,\n",
       "   0.04266493768731425,\n",
       "   0.04258295322252699,\n",
       "   0.042673652625280965,\n",
       "   0.042609610734892285,\n",
       "   0.042604896253790735,\n",
       "   0.0426217356496606,\n",
       "   0.04256336836775473,\n",
       "   0.04264350986677753,\n",
       "   0.0426043745161088,\n",
       "   0.04254686726518899,\n",
       "   0.042561220236061036,\n",
       "   0.042647269540581824,\n",
       "   0.042665280931252096,\n",
       "   0.04259250011325868,\n",
       "   0.04260315727596441,\n",
       "   0.04261578141657774,\n",
       "   0.04257738609944493,\n",
       "   0.04266684518372717,\n",
       "   0.04261655492230881,\n",
       "   0.042576977068727664,\n",
       "   0.042644668438217855,\n",
       "   0.04260775072515503,\n",
       "   0.04259817445081127,\n",
       "   0.04260340862530322,\n",
       "   0.04263914380191772,\n",
       "   0.042614984807889326,\n",
       "   0.042564370834137784,\n",
       "   0.04256108940140275,\n",
       "   0.04262748909883263,\n",
       "   0.042655750542632806,\n",
       "   0.04260864013975317,\n",
       "   0.04265098024990933,\n",
       "   0.042703697799651094,\n",
       "   0.04256159276509088,\n",
       "   0.04258667139475011,\n",
       "   0.04266549831579539,\n",
       "   0.04259382811459628,\n",
       "   0.04260668592019515,\n",
       "   0.042651527134840155,\n",
       "   0.042587499825422426,\n",
       "   0.04259684760708454,\n",
       "   0.04263676390174992,\n",
       "   0.04256041448963575,\n",
       "   0.0426324080829778,\n",
       "   0.042604184791076284,\n",
       "   0.04265646717765115,\n",
       "   0.04263753028940563,\n",
       "   0.042612941772484585,\n",
       "   0.04263274960281435,\n",
       "   0.0425352954913762,\n",
       "   0.042565274115436334,\n",
       "   0.04263110367719792,\n",
       "   0.0426352433921877,\n",
       "   0.042619584611624725,\n",
       "   0.04257982594907776,\n",
       "   0.042638016387450794,\n",
       "   0.042563942641266124,\n",
       "   0.04260217581898713,\n",
       "   0.04264878000109649,\n",
       "   0.042637551791411786,\n",
       "   0.04262044309091962,\n",
       "   0.04262196106358993,\n",
       "   0.04260765939704643,\n",
       "   0.04259389402452579,\n",
       "   0.042601003036026125,\n",
       "   0.042616827921433884,\n",
       "   0.04262974424795671,\n",
       "   0.04265821595822484,\n",
       "   0.04257850782437758,\n",
       "   0.042634908325416,\n",
       "   0.04258384780942901,\n",
       "   0.04265505297124878,\n",
       "   0.042616258918746444,\n",
       "   0.04262028474453067,\n",
       "   0.042633687907999214,\n",
       "   0.042653455517508765,\n",
       "   0.042646770930487264,\n",
       "   0.04262047969112712,\n",
       "   0.04261456471829375,\n",
       "   0.04257402247633816,\n",
       "   0.042632604975345705,\n",
       "   0.042628948944659274,\n",
       "   0.0426546327338731,\n",
       "   0.042574428699233315,\n",
       "   0.04263849593391103,\n",
       "   0.04259218056339863,\n",
       "   0.04257684522423862,\n",
       "   0.042596494486509276,\n",
       "   0.042637381573353915,\n",
       "   0.042552328257521324,\n",
       "   0.04263615384082164,\n",
       "   0.04256843702852233,\n",
       "   0.042697221434806004,\n",
       "   0.042544159490214895,\n",
       "   0.04266082127232197,\n",
       "   0.04256801580594591,\n",
       "   0.042619311021379204,\n",
       "   0.042633627465933806,\n",
       "   0.042546734336979135,\n",
       "   0.04263930222219672,\n",
       "   0.042627999038735695,\n",
       "   0.0426171269298585,\n",
       "   0.042630911809353786,\n",
       "   0.04260338259137367,\n",
       "   0.042661774601818116,\n",
       "   0.042593268249645704,\n",
       "   0.0425586264980726,\n",
       "   0.04259070882127305,\n",
       "   0.04263155524395714,\n",
       "   0.04262896190004901,\n",
       "   0.042668410125842764,\n",
       "   0.042536934027987076,\n",
       "   0.04253432142340447,\n",
       "   0.04271856127691663,\n",
       "   0.04263124052158072,\n",
       "   0.04257799436238186,\n",
       "   0.04265835974827285,\n",
       "   0.042632151758375245,\n",
       "   0.0425724579775629,\n",
       "   0.04259740798926551,\n",
       "   0.04262552990401087,\n",
       "   0.0426682154255465,\n",
       "   0.04248507668164151,\n",
       "   0.042586532235145566,\n",
       "   0.04253982576456937,\n",
       "   0.04264281155649296,\n",
       "   0.04265557704878248,\n",
       "   0.04264365437109608,\n",
       "   0.042587947476008706,\n",
       "   0.04266172662254207,\n",
       "   0.0426096663987341,\n",
       "   0.04264268160851534,\n",
       "   0.04263392063704404,\n",
       "   0.04265981816555843,\n",
       "   0.04265337817925067,\n",
       "   0.042598565797175254,\n",
       "   0.04255906961673547,\n",
       "   0.042636969660924486,\n",
       "   0.0425907976863798,\n",
       "   0.04263528334207771,\n",
       "   0.04262692085967576,\n",
       "   0.042557180124866076,\n",
       "   0.042630946094339546,\n",
       "   0.04260714083663688,\n",
       "   0.04260220921729222,\n",
       "   0.042649029256883735,\n",
       "   0.042624621745968656,\n",
       "   0.0426329750413737,\n",
       "   0.04252152307467027,\n",
       "   0.04255774686159181,\n",
       "   0.042668664800234075,\n",
       "   0.042658944858992395,\n",
       "   0.04260152950267161,\n",
       "   0.04259028383030379,\n",
       "   0.042650012905932656,\n",
       "   0.042636027710496884,\n",
       "   0.04256369631645108,\n",
       "   0.04253220999043835,\n",
       "   0.04267471787358119,\n",
       "   0.04264966372616035,\n",
       "   0.04264430260855304,\n",
       "   0.0425814832045027,\n",
       "   0.042575291932121784,\n",
       "   0.0426375897462703,\n",
       "   0.042566724330925745,\n",
       "   0.04265490849156025,\n",
       "   0.04267480462050635,\n",
       "   0.04262927307570276,\n",
       "   0.04249881188238948,\n",
       "   0.042611850317844674,\n",
       "   0.042654256387190385,\n",
       "   0.04262471189183637,\n",
       "   0.04267795810029527,\n",
       "   0.042671939213413834,\n",
       "   0.04262811108068986,\n",
       "   0.042643515433161715,\n",
       "   0.04257194604262833,\n",
       "   0.042645290395445075,\n",
       "   0.04254629611968994,\n",
       "   0.04266554023608689,\n",
       "   0.04263854396244711,\n",
       "   0.04247579264246728,\n",
       "   0.042608480537233276,\n",
       "   0.04262991976146856,\n",
       "   0.04260457581725002,\n",
       "   0.0425793845791462,\n",
       "   0.04259131437490794,\n",
       "   0.04261414802764073,\n",
       "   0.042621642425040566,\n",
       "   0.042616084119505133,\n",
       "   0.042523207201445395,\n",
       "   0.0426071797274361,\n",
       "   0.04265128450452789,\n",
       "   0.042592999560773864,\n",
       "   0.042636330462684315,\n",
       "   0.04258501899143881,\n",
       "   0.042643272679699355,\n",
       "   0.04257688285890689,\n",
       "   0.042636967739783045,\n",
       "   0.04265051474255964,\n",
       "   0.04264044118814232,\n",
       "   0.042603263332824076,\n",
       "   0.04262059717631537,\n",
       "   0.042568455599556285,\n",
       "   0.042584680279424365,\n",
       "   0.042558055697393814,\n",
       "   0.042586033625051006,\n",
       "   0.042585695011556644,\n",
       "   0.042609554184369805,\n",
       "   0.042636552108220815,\n",
       "   0.04259384589746964,\n",
       "   0.04254528690468181,\n",
       "   0.042594751543249966,\n",
       "   0.04260530304317632,\n",
       "   0.04268657742453016,\n",
       "   0.04263091220343408,\n",
       "   0.04263160277989285,\n",
       "   0.04261949976121099,\n",
       "   0.04265303478753271,\n",
       "   0.042557745482310776,\n",
       "   0.04260483812694707,\n",
       "   0.042617766866999224,\n",
       "   0.042618426409634674,\n",
       "   0.042660802898328166,\n",
       "   0.042593339159468974,\n",
       "   0.04258552592647962,\n",
       "   0.04259255397910914,\n",
       "   0.04261662070908822,\n",
       "   0.042586544624044875,\n",
       "   0.042588076611195715,\n",
       "   0.042620088344763135,\n",
       "   0.04263824111173961,\n",
       "   0.042607759271771455,\n",
       "   0.04253603205207951,\n",
       "   0.042636744345515225,\n",
       "   0.042575801699614724,\n",
       "   0.0426354348166915,\n",
       "   0.042609742259191084,\n",
       "   0.042549002515383,\n",
       "   0.04255896619528778,\n",
       "   0.042607029336543124,\n",
       "   0.04253681016362403,\n",
       "   0.042560034571600354,\n",
       "   0.04256417007485697,\n",
       "   0.04265582019632513,\n",
       "   0.04261277625876025,\n",
       "   0.04264747850165879,\n",
       "   0.042666389528384877,\n",
       "   0.042566924794646334,\n",
       "   0.042571289332445,\n",
       "   0.0426402550590925,\n",
       "   0.04257429168244039,\n",
       "   0.0425911266202769,\n",
       "   0.04261390731846991,\n",
       "   0.042610560616185844,\n",
       "   0.04257820258455828,\n",
       "   0.042574064815339964,\n",
       "   0.04258863627910614,\n",
       "   0.04256994985351878,\n",
       "   0.0426647335537209,\n",
       "   0.0426098479712305,\n",
       "   0.04265414116796383,\n",
       "   0.04258979071270336,\n",
       "   0.04263805759347175,\n",
       "   0.04266431513896658,\n",
       "   0.04269370924342762,\n",
       "   0.04255708140775192,\n",
       "   0.04252469352454193,\n",
       "   0.0426143911998134,\n",
       "   0.042656455158202114,\n",
       "   0.04254342967813665,\n",
       "   0.04264184147858423,\n",
       "   0.042576084944827494,\n",
       "   0.04258537903304928,\n",
       "   0.04259544773535295,\n",
       "   0.04258206265031799,\n",
       "   0.04263130830339164,\n",
       "   0.04255050733562343,\n",
       "   0.04267092955506538,\n",
       "   0.042626350822527546,\n",
       "   0.04263562276836269,\n",
       "   0.04252770794324638,\n",
       "   0.04259722666306929,\n",
       "   0.042614438046108594,\n",
       "   0.04257577901536768,\n",
       "   0.04267058246884464,\n",
       "   0.04261329942498325,\n",
       "   0.04261200772829292,\n",
       "   0.04267453891186675,\n",
       "   0.042657872640396936,\n",
       "   0.04260232123461637,\n",
       "   0.042572129659416265,\n",
       "   0.04260491876562765,\n",
       "   0.04265521804163279,\n",
       "   0.04263104759464579,\n",
       "   0.042652528246572195,\n",
       "   0.04265709987356643,\n",
       "   0.0426173453734926,\n",
       "   0.042672706118299944,\n",
       "   0.04258729684943995,\n",
       "   0.042661901298633294,\n",
       "   0.0426550326268535,\n",
       "   0.04264262958991626,\n",
       "   0.042687990645731776,\n",
       "   0.042590910738164726,\n",
       "   0.04259061128639978,\n",
       "   0.04264761175005889,\n",
       "   0.04260376913488404,\n",
       "   0.04257060195788864,\n",
       "   0.04267923600417523,\n",
       "   0.04261467442039616,\n",
       "   0.04259458634971587,\n",
       "   0.04253720114053774,\n",
       "   0.042584565257237965,\n",
       "   0.04265214529904452,\n",
       "   0.04257017925751111,\n",
       "   0.04260560163289062,\n",
       "   0.04267442733788293,\n",
       "   0.04264928136975312,\n",
       "   0.042590031495764236,\n",
       "   0.042626001716645295,\n",
       "   0.042588377639281846,\n",
       "   0.042655398727448515,\n",
       "   0.04266511889036037,\n",
       "   ...],\n",
       "  'val': []},\n",
       " {'train': [0.0611159299277077,\n",
       "   0.05852254018310673,\n",
       "   0.05848823960654992,\n",
       "   0.05850140437606938,\n",
       "   0.05849787945097143,\n",
       "   0.05849319223530036,\n",
       "   0.058497221952627514,\n",
       "   0.05849310514355494,\n",
       "   0.05849134119088985,\n",
       "   0.05850607634575899,\n",
       "   0.05849424236076922,\n",
       "   0.0584993109722768,\n",
       "   0.05849999965222414,\n",
       "   0.05846718672878486,\n",
       "   0.058531084927645596,\n",
       "   0.05847996089084089,\n",
       "   0.058486564297321415,\n",
       "   0.058504210818897597,\n",
       "   0.05849518386785649,\n",
       "   0.05851387228847535,\n",
       "   0.058494119136786656,\n",
       "   0.05849553297373874,\n",
       "   0.05850983749736439,\n",
       "   0.05850510971605285,\n",
       "   0.058507843081616176,\n",
       "   0.05850793753773713,\n",
       "   0.05849614374893756,\n",
       "   0.05849025399724314,\n",
       "   0.05853506361157441,\n",
       "   0.05846004441749951,\n",
       "   0.05851735457901127,\n",
       "   0.05851079112242076,\n",
       "   0.05850940388588866,\n",
       "   0.058494352900292264,\n",
       "   0.058505888615757964,\n",
       "   0.05847908251049105,\n",
       "   0.058519222889064755,\n",
       "   0.058483356286671534,\n",
       "   0.05849879590932988,\n",
       "   0.058484005189138995,\n",
       "   0.05850055922161449,\n",
       "   0.0585163745998351,\n",
       "   0.058496760533860895,\n",
       "   0.05848284888366037,\n",
       "   0.058490734257973916,\n",
       "   0.05848023295402527,\n",
       "   0.05850378077877454,\n",
       "   0.058512951667643774,\n",
       "   0.058499434368669494,\n",
       "   0.05849606370137743,\n",
       "   0.05848114867348316,\n",
       "   0.05851390849460255,\n",
       "   0.058491816895067196,\n",
       "   0.058492225901154445,\n",
       "   0.058518394113572174,\n",
       "   0.05848149647397443,\n",
       "   0.0585090566272578,\n",
       "   0.05851395423254691,\n",
       "   0.05850062116611102,\n",
       "   0.05842113420982992,\n",
       "   0.05851269962866444,\n",
       "   0.058502360020787264,\n",
       "   0.058514705300331114,\n",
       "   0.058500657396868244,\n",
       "   0.0584968055329047,\n",
       "   0.05848784811240582,\n",
       "   0.05848911094271447,\n",
       "   0.058487514917515526,\n",
       "   0.05851751344263061,\n",
       "   0.05849622441224815,\n",
       "   0.05849040726984828,\n",
       "   0.05849055637998029,\n",
       "   0.05844623175534335,\n",
       "   0.05852764135549876,\n",
       "   0.0584874344758751,\n",
       "   0.05849244407385834,\n",
       "   0.058529499050014276,\n",
       "   0.05852314815048344,\n",
       "   0.058509060173980464,\n",
       "   0.058489104144829364,\n",
       "   0.05850396860729564,\n",
       "   0.058507243168255514,\n",
       "   0.05850904135664633,\n",
       "   0.0584989104142859,\n",
       "   0.058495443813071764,\n",
       "   0.05850130423041414,\n",
       "   0.05848883434760669,\n",
       "   0.058493596216863834,\n",
       "   0.05848644769881382,\n",
       "   0.05852052052158955,\n",
       "   0.058503972375688475,\n",
       "   0.05851295255432444,\n",
       "   0.05850520331012316,\n",
       "   0.05849933156297227,\n",
       "   0.058483675984311694,\n",
       "   0.058485017975499806,\n",
       "   0.058512065208647865,\n",
       "   0.05849887167126679,\n",
       "   0.058506307178292395,\n",
       "   0.05851115599151485,\n",
       "   0.05850969818997974,\n",
       "   0.058490200131392674,\n",
       "   0.058494941360694316,\n",
       "   0.0584186326127407,\n",
       "   0.05854430282411496,\n",
       "   0.05849435524014402,\n",
       "   0.0584991951619298,\n",
       "   0.05851789971521078,\n",
       "   0.05849605695275236,\n",
       "   0.05849290110848167,\n",
       "   0.058504061462465395,\n",
       "   0.05849266983260793,\n",
       "   0.0584938668022471,\n",
       "   0.05847504419728744,\n",
       "   0.05850098936025761,\n",
       "   0.05847544211986636,\n",
       "   0.058518145350385305,\n",
       "   0.05850603491806787,\n",
       "   0.058511334756189144,\n",
       "   0.058506574364733104,\n",
       "   0.05850584250836333,\n",
       "   0.05849414046638268,\n",
       "   0.05848714265941588,\n",
       "   0.058521011791938596,\n",
       "   0.05847391589613985,\n",
       "   0.05850866404939289,\n",
       "   0.05849473146367664,\n",
       "   0.058496840876981246,\n",
       "   0.05848959568610861,\n",
       "   0.05850099448330146,\n",
       "   0.05852117624657213,\n",
       "   0.05848223246818732,\n",
       "   0.05851810875017781,\n",
       "   0.058468880879977515,\n",
       "   0.05850317340251828,\n",
       "   0.05850334192110487,\n",
       "   0.05850877284018462,\n",
       "   0.05851015576646348,\n",
       "   0.05851183420370433,\n",
       "   0.058486831311351996,\n",
       "   0.05850410559945855,\n",
       "   0.05849280042096603,\n",
       "   0.058498485718876864,\n",
       "   0.05847916662200423,\n",
       "   0.05847913184441811,\n",
       "   0.05850216832535326,\n",
       "   0.058515696978766074,\n",
       "   0.058487865156378625,\n",
       "   0.05849407999968726,\n",
       "   0.05849666999391288,\n",
       "   0.058517859149570306,\n",
       "   0.05851174967348083,\n",
       "   0.05848807746713812,\n",
       "   0.05849557050988694,\n",
       "   0.0585024946484684,\n",
       "   0.05849533613063087,\n",
       "   0.058531767080638036,\n",
       "   0.05848416218087693,\n",
       "   0.058498958122631735,\n",
       "   0.05850829954974907,\n",
       "   0.05848799884811906,\n",
       "   0.05850681534483413,\n",
       "   0.05848630718455827,\n",
       "   0.058496379162654405,\n",
       "   0.058510007592272165,\n",
       "   0.05851067314463213,\n",
       "   0.058483088115030085,\n",
       "   0.058528099079762606,\n",
       "   0.05850209239100622,\n",
       "   0.05847245730644415,\n",
       "   0.058515225806512125,\n",
       "   0.05848345539786599,\n",
       "   0.05850785357400406,\n",
       "   0.05849449388251817,\n",
       "   0.05849348488918021,\n",
       "   0.0584889207497116,\n",
       "   0.05847258513624018,\n",
       "   0.058516170318461645,\n",
       "   0.05846920441990056,\n",
       "   0.058504114170704995,\n",
       "   0.058500280902405416,\n",
       "   0.05849154256592112,\n",
       "   0.05851517206381175,\n",
       "   0.058501614568647275,\n",
       "   0.058506584610820804,\n",
       "   0.05847602082678109,\n",
       "   0.058512419412943946,\n",
       "   0.05850159299275107,\n",
       "   0.0585102918473157,\n",
       "   0.05849487348036333,\n",
       "   0.058504453966440245,\n",
       "   0.05849373508090815,\n",
       "   0.058504557116957734,\n",
       "   0.05849769036631939,\n",
       "   0.05849079499559954,\n",
       "   0.058493953623062325,\n",
       "   0.058483828493386264,\n",
       "   0.05848753806973292,\n",
       "   0.05848109165499033,\n",
       "   0.058513671209004296,\n",
       "   0.05851030628050655,\n",
       "   0.05847580181665657,\n",
       "   0.05849298120530184,\n",
       "   0.05849234146520126,\n",
       "   0.058506866378232464,\n",
       "   0.058496636644867823,\n",
       "   0.0584850306106993,\n",
       "   0.05849754258620837,\n",
       "   0.05849145892237829,\n",
       "   0.05848798717349029,\n",
       "   0.058508377331347505,\n",
       "   0.05850097214387468,\n",
       "   0.05850919381646085,\n",
       "   0.05847592250374723,\n",
       "   0.05850619580134873,\n",
       "   0.05849053800598649,\n",
       "   0.058501750230789185,\n",
       "   0.05850311584216504,\n",
       "   0.058475050872022455,\n",
       "   0.058496091533298336,\n",
       "   0.058523538289976515,\n",
       "   0.05849463939666748,\n",
       "   0.05848106958649375,\n",
       "   0.05851143216791232,\n",
       "   0.05848957765693507,\n",
       "   0.058492194399360785,\n",
       "   0.05849120195739525,\n",
       "   0.05849971170267783,\n",
       "   0.058470822144145805,\n",
       "   0.05848256797829935,\n",
       "   0.058508740722640486,\n",
       "   0.05850432574256392,\n",
       "   0.0584999588649135,\n",
       "   0.058502193718902334,\n",
       "   0.05848965186718082,\n",
       "   0.058508835597471755,\n",
       "   0.05849545462564989,\n",
       "   0.0585101502000793,\n",
       "   0.058505447812316834,\n",
       "   0.05849702643954064,\n",
       "   0.05847998507751906,\n",
       "   0.058510897080760356,\n",
       "   0.0585126809344804,\n",
       "   0.05851794803930708,\n",
       "   0.0584950149551896,\n",
       "   0.05850670943575457,\n",
       "   0.058456002335903076,\n",
       "   0.058541781892461224,\n",
       "   0.05847591765163358,\n",
       "   0.05847774571623684,\n",
       "   0.05849615837916855,\n",
       "   0.05850766857793509,\n",
       "   0.05849778425594992,\n",
       "   0.05847399638704032,\n",
       "   0.05848533464364769,\n",
       "   0.05848455783749415,\n",
       "   0.0584984696601048,\n",
       "   0.05851003517789289,\n",
       "   0.05850117768137908,\n",
       "   0.05852816513747223,\n",
       "   0.05851825367320668,\n",
       "   0.05846738135519106,\n",
       "   0.05847503232561852,\n",
       "   0.058508100095859245,\n",
       "   0.058516091945742775,\n",
       "   0.05849038123591872,\n",
       "   0.058507488360089704,\n",
       "   0.05847994557096938,\n",
       "   0.058491779309658966,\n",
       "   0.05851294947557213,\n",
       "   0.0585041916567432,\n",
       "   0.05849007949356205,\n",
       "   0.05842826509278668,\n",
       "   0.058512714529825635,\n",
       "   0.05850346738641912,\n",
       "   0.058474255544095,\n",
       "   0.05850542626105064,\n",
       "   0.05849692355995336,\n",
       "   0.05848635272546248,\n",
       "   0.05848418378140315,\n",
       "   0.058485342254323405,\n",
       "   0.058482826396453476,\n",
       "   0.05849490653384815,\n",
       "   0.058502034264162554,\n",
       "   0.05849688491545433,\n",
       "   0.058483483771647304,\n",
       "   0.05850597373710191,\n",
       "   0.058493241864787646,\n",
       "   0.058503234682004315,\n",
       "   0.05847147506130628,\n",
       "   0.05853847486913697,\n",
       "   0.05848620457590119,\n",
       "   0.05849468584888238,\n",
       "   0.05850930287818278,\n",
       "   0.05849621492969103,\n",
       "   0.05850895165411894,\n",
       "   0.058499394394149466,\n",
       "   0.058500912514599886,\n",
       "   0.05850136560842025,\n",
       "   0.058512330744877335,\n",
       "   0.05848014384261833,\n",
       "   0.05846400034329123,\n",
       "   0.05847978606696956,\n",
       "   0.05851038524434586,\n",
       "   0.05848080658715619,\n",
       "   0.05849764366780431,\n",
       "   0.05849540470060238,\n",
       "   0.05849585513438075,\n",
       "   0.058496189363731826,\n",
       "   0.05850818418274241,\n",
       "   0.058507877982352394,\n",
       "   0.058485682394878925,\n",
       "   0.05853877126677962,\n",
       "   0.058493389891198844,\n",
       "   0.058488782329007616,\n",
       "   0.05850137826824976,\n",
       "   0.05852727766864556,\n",
       "   0.058520301462204986,\n",
       "   0.058486255929489764,\n",
       "   0.0584975418226778,\n",
       "   0.058499624980382685,\n",
       "   0.05848688384718146,\n",
       "   0.05851391196743516,\n",
       "   0.058492410478513106,\n",
       "   0.05850671928776197,\n",
       "   0.058477521336768284,\n",
       "   0.05850978030646143,\n",
       "   0.05850190259208364,\n",
       "   0.058494440780198276,\n",
       "   0.058506755050548837,\n",
       "   0.05850658067001784,\n",
       "   0.05851033926010132,\n",
       "   0.05848749649426169,\n",
       "   0.05848973792446546,\n",
       "   0.058509387703966506,\n",
       "   0.058517972989515825,\n",
       "   0.05848654818928931,\n",
       "   0.05849576858449573,\n",
       "   0.058477598822806495,\n",
       "   0.05848653804172169,\n",
       "   0.0584863856311672,\n",
       "   0.058485787220237666,\n",
       "   0.05849654112965608,\n",
       "   0.058514734240602856,\n",
       "   0.05851349318323056,\n",
       "   0.05849414347124494,\n",
       "   0.05849421462736839,\n",
       "   0.058494781905954536,\n",
       "   0.058452636299054485,\n",
       "   0.05851212070007955,\n",
       "   0.05851108865304427,\n",
       "   0.058513929971978684,\n",
       "   0.0584986774635709,\n",
       "   0.058508358415493296,\n",
       "   0.058510906489427424,\n",
       "   0.058517145445524166,\n",
       "   0.05849983088733736,\n",
       "   0.05849488574611254,\n",
       "   0.058487759321189124,\n",
       "   0.05849507736765649,\n",
       "   0.05847774891813925,\n",
       "   0.05850367900753809,\n",
       "   0.05850264750236322,\n",
       "   0.05849464171188922,\n",
       "   0.05849532694363397,\n",
       "   0.05847062163116518,\n",
       "   0.05850350305068591,\n",
       "   0.05850714469744154,\n",
       "   0.058495431941402845,\n",
       "   0.05850923928347501,\n",
       "   0.05843780658954431,\n",
       "   0.05853779335652501,\n",
       "   0.05848686488206722,\n",
       "   0.058513239592560065,\n",
       "   0.05849957924243832,\n",
       "   0.058500845520949564,\n",
       "   0.05849149207438319,\n",
       "   0.05848689645775094,\n",
       "   0.05849486934252022,\n",
       "   0.058479186276759,\n",
       "   0.05850887113858846,\n",
       "   0.05850267287128228,\n",
       "   0.058517575608797306,\n",
       "   0.0584858676618781,\n",
       "   0.058489704723200524,\n",
       "   0.05849203743225287,\n",
       "   0.05851577106586173,\n",
       "   0.05849897033912091,\n",
       "   0.058491709557446564,\n",
       "   0.058497913341876887,\n",
       "   0.05850625550451358,\n",
       "   0.05848886006134601,\n",
       "   0.05848810933838206,\n",
       "   0.05851605034564152,\n",
       "   0.05850172239886828,\n",
       "   0.05849554597838851,\n",
       "   0.058489243205913824,\n",
       "   0.05848635895685716,\n",
       "   0.058490047228237814,\n",
       "   0.05848169972088711,\n",
       "   0.05849986327581169,\n",
       "   0.05849589134050795,\n",
       "   0.0585073377967866,\n",
       "   0.058474961095605016,\n",
       "   0.05849688880699725,\n",
       "   0.05849475358143326,\n",
       "   0.05849118858329521,\n",
       "   0.05847592191262679,\n",
       "   0.05850312212281976,\n",
       "   0.05849826148718842,\n",
       "   0.05849452338928034,\n",
       "   0.05851561047814109,\n",
       "   0.05851976319778064,\n",
       "   0.05848519279937114,\n",
       "   0.058473248422638444,\n",
       "   0.05853779783918838,\n",
       "   0.05850893076786325,\n",
       "   0.058496594281236006,\n",
       "   0.0584955668646442,\n",
       "   0.05850651995702223,\n",
       "   0.05848189695807528,\n",
       "   0.058517452507964834,\n",
       "   0.058500938523899426,\n",
       "   0.05850406579734865,\n",
       "   0.058515673186168196,\n",
       "   0.058476614459487035,\n",
       "   0.05852101750610288,\n",
       "   0.05851197213180794,\n",
       "   0.05848914439027959,\n",
       "   0.05847892805564502,\n",
       "   0.05850646239666899,\n",
       "   0.058508133789724553,\n",
       "   0.05849158788515516,\n",
       "   0.05849448442459106,\n",
       "   0.058521836626628215,\n",
       "   0.05848269713811638,\n",
       "   0.05849193620287682,\n",
       "   0.058497967503287576,\n",
       "   0.05851977336997828,\n",
       "   0.05847718705815717,\n",
       "   0.05851088924841447,\n",
       "   0.058504680932060744,\n",
       "   0.058503017888581456,\n",
       "   0.05848935827736027,\n",
       "   0.05848727876489813,\n",
       "   0.05844636372298249,\n",
       "   0.05851426040830691,\n",
       "   0.058504688986076796,\n",
       "   0.058490576231775206,\n",
       "   0.05853805679920291,\n",
       "   0.05848902133870716,\n",
       "   0.05847885219518803,\n",
       "   0.05850021080537276,\n",
       "   0.058512359857559204,\n",
       "   0.058497921124962736,\n",
       "   0.05852611109244922,\n",
       "   0.05845425114158757,\n",
       "   0.0585039669570844,\n",
       "   0.058511087914143715,\n",
       "   0.05852041732181202,\n",
       "   0.058485251714375394,\n",
       "   0.05849579666271683,\n",
       "   0.058458887865720705,\n",
       "   0.05850858550426388,\n",
       "   0.058491976103506794,\n",
       "   0.058516525729628635,\n",
       "   0.058494450041085236,\n",
       "   0.05849020057473301,\n",
       "   0.05847028163838978,\n",
       "   0.05849151064541714,\n",
       "   0.05853465362028642,\n",
       "   0.058489594971838074,\n",
       "   0.05849278118492158,\n",
       "   0.05848452328157819,\n",
       "   0.05849147498115035,\n",
       "   0.05849750618304103,\n",
       "   0.05848894597085054,\n",
       "   0.058485168514172896,\n",
       "   0.0585068497776,\n",
       "   0.05850031912819413,\n",
       "   0.058493749218538775,\n",
       "   0.05849994140223038,\n",
       "   0.058514301885258066,\n",
       "   0.058477447840793076,\n",
       "   0.058517581914082045,\n",
       "   0.05846603892066262,\n",
       "   0.05851533728197587,\n",
       "   0.05849326565738552,\n",
       "   0.05849173761103764,\n",
       "   0.05851030664995682,\n",
       "   0.058495745530798414,\n",
       "   0.058513307399001006,\n",
       "   0.05850055759603327,\n",
       "   0.05848450729669618,\n",
       "   0.058489463447539275,\n",
       "   0.05849456055597826,\n",
       "   0.05850941745702885,\n",
       "   0.058504087176204714,\n",
       "   0.05850799265971854,\n",
       "   0.058485410036134326,\n",
       "   0.05851001808466005,\n",
       "   0.05849327617440342,\n",
       "   0.05850017789966804,\n",
       "   0.058449424199821534,\n",
       "   0.05852826474126705,\n",
       "   0.058506067528212365,\n",
       "   0.05852247407613707,\n",
       "   0.058512721820311114,\n",
       "   0.0584633831642876,\n",
       "   0.05852959843213893,\n",
       "   0.0584929350979072,\n",
       "   0.05850294882600958,\n",
       "   0.05849641815197369,\n",
       "   0.05851544612202762,\n",
       "   0.05848881543175248,\n",
       "   0.05849606876284623,\n",
       "   0.05850124117756678,\n",
       "   0.05849498906904016,\n",
       "   0.05851052329559957,\n",
       "   0.058473172907001714,\n",
       "   0.05851120860123437,\n",
       "   0.05852164788679643,\n",
       "   0.05849880120478386,\n",
       "   0.05847648864935252,\n",
       "   0.05854742177261794,\n",
       "   0.058492150927378125,\n",
       "   0.058494901804884605,\n",
       "   0.058484911179739585,\n",
       "   0.05851009933909109,\n",
       "   0.05850168678386152,\n",
       "   0.05848790855447123,\n",
       "   0.05851321077543842,\n",
       "   0.058503348472689794,\n",
       "   0.058507736655306224,\n",
       "   0.05848138140252799,\n",
       "   0.05850276740129329,\n",
       "   0.058506241810223285,\n",
       "   0.05848396558406924,\n",
       "   0.05849949050048166,\n",
       "   0.058499978199478025,\n",
       "   0.058487077192826704,\n",
       "   0.05850307746859621,\n",
       "   0.058493693653217035,\n",
       "   0.0585237259707175,\n",
       "   0.05850865781799821,\n",
       "   0.058506237697010195,\n",
       "   0.058489202492493245,\n",
       "   0.0585039791982036,\n",
       "   0.058508327825010316,\n",
       "   0.05850804512165795,\n",
       "   0.05849855990449259,\n",
       "   0.05850307581838497,\n",
       "   0.05851205976541377,\n",
       "   0.058496596399417594,\n",
       "   0.058498282053253865,\n",
       "   0.05848598056588291,\n",
       "   0.058491943444102266,\n",
       "   0.05848447705103346,\n",
       "   0.058492922191777506,\n",
       "   0.058492998298534674,\n",
       "   0.05850258124761345,\n",
       "   0.05850134644626586,\n",
       "   0.05849819207979628,\n",
       "   0.05849361148747531,\n",
       "   0.05847686120301239,\n",
       "   0.058475710611698054,\n",
       "   0.05851205582461081,\n",
       "   0.05849281712011858,\n",
       "   0.05850472016768022,\n",
       "   0.058498056319134295,\n",
       "   0.0585047306600681,\n",
       "   0.05849971342677913,\n",
       "   0.05848842095737615,\n",
       "   0.058495494082939525,\n",
       "   0.05851077072876544,\n",
       "   0.058493344990675114,\n",
       "   0.058515959879583565,\n",
       "   0.058496956687328246,\n",
       "   0.05849211201194889,\n",
       "   0.05848241771055647,\n",
       "   0.05852451861397294,\n",
       "   0.058497797753200055,\n",
       "   0.058485797614105474,\n",
       "   0.05849506212167503,\n",
       "   0.05850394779493001,\n",
       "   0.05851172666904355,\n",
       "   0.05851142086273382,\n",
       "   0.058501389795098425,\n",
       "   0.05850000553879856,\n",
       "   0.05845412350883169,\n",
       "   0.05850985582209816,\n",
       "   0.0584968090057373,\n",
       "   0.0584942049231411,\n",
       "   0.05848181449677333,\n",
       "   0.058509847447891866,\n",
       "   0.0585037391786733,\n",
       "   0.05847282838230291,\n",
       "   0.05849121301627356,\n",
       "   0.05848585510056866,\n",
       "   0.058509589374557996,\n",
       "   0.05849202738320532,\n",
       "   0.05850368816990498,\n",
       "   0.058497101462577,\n",
       "   0.05849362897478844,\n",
       "   0.058467985332504774,\n",
       "   0.05849718375146882,\n",
       "   0.05851113904606212,\n",
       "   0.0585006188755193,\n",
       "   0.05848041447726163,\n",
       "   0.058470170384596204,\n",
       "   0.05848624484598144,\n",
       "   0.058492595105131794,\n",
       "   0.05849819698116996,\n",
       "   0.05850330758685908,\n",
       "   0.058514325677855944,\n",
       "   0.05850054759624576,\n",
       "   0.05849749130650985,\n",
       "   0.05849221072906305,\n",
       "   0.058520074964554844,\n",
       "   0.05849901312146305,\n",
       "   0.05850853284528433,\n",
       "   0.05850860038079506,\n",
       "   0.058493783183334286,\n",
       "   0.05849994322485175,\n",
       "   0.0584947782607118,\n",
       "   0.058486899216313004,\n",
       "   0.05852607104403913,\n",
       "   0.05848123135645528,\n",
       "   0.05851324001127038,\n",
       "   0.058488639080820005,\n",
       "   0.058496303129787285,\n",
       "   0.058489688221088125,\n",
       "   0.058487361226200074,\n",
       "   0.05849810144132819,\n",
       "   0.05851312097439096,\n",
       "   0.05849888758225875,\n",
       "   0.058504760117570234,\n",
       "   0.058503400195728646,\n",
       "   0.05853368169512631,\n",
       "   0.05851566072337883,\n",
       "   0.058494946138917904,\n",
       "   0.05853912756462728,\n",
       "   0.0584912130655336,\n",
       "   0.058510383865064824,\n",
       "   0.058491761329745456,\n",
       "   0.0584695429841349,\n",
       "   0.058502312928191885,\n",
       "   0.058483733815595136,\n",
       "   0.05850233149922584,\n",
       "   0.05849418620432704,\n",
       "   0.058519675170094515,\n",
       "   0.05849741345102137,\n",
       "   0.058483589877767014,\n",
       "   0.05851689744586787,\n",
       "   0.05850357270437824,\n",
       "   0.058510730581835285,\n",
       "   0.058499197723451725,\n",
       "   0.058483757608193014,\n",
       "   0.05850458317551731,\n",
       "   0.05851532545956698,\n",
       "   0.058499991548948055,\n",
       "   0.05849814217937879,\n",
       "   0.05848956152427295,\n",
       "   0.05849043864849185,\n",
       "   0.0584987293836499,\n",
       "   0.05850541975872576,\n",
       "   0.05847863032798137,\n",
       "   0.0585129555591867,\n",
       "   0.058487637969087965,\n",
       "   0.058499104769761896,\n",
       "   0.05850333837438221,\n",
       "   0.05850353162150738,\n",
       "   0.05849816882905881,\n",
       "   0.058496732849720096,\n",
       "   0.05848951187015565,\n",
       "   0.05849921920082786,\n",
       "   0.05848260763262914,\n",
       "   0.058525931244054114,\n",
       "   0.058493178245449855,\n",
       "   0.0585075883333348,\n",
       "   0.05848745984479416,\n",
       "   0.05850126708834624,\n",
       "   0.05851058447656553,\n",
       "   0.05846697111760289,\n",
       "   0.05852884133000019,\n",
       "   0.058489373892792,\n",
       "   0.058521822513627614,\n",
       "   0.05850805127916257,\n",
       "   0.05848426134133142,\n",
       "   0.058498443503025147,\n",
       "   0.058502668093058686,\n",
       "   0.0585027807261333,\n",
       "   0.05850187835614543,\n",
       "   0.05849464895311466,\n",
       "   0.05847793039211557,\n",
       "   0.05849212654365981,\n",
       "   0.0584889561676782,\n",
       "   0.05847248104978199,\n",
       "   0.05849705868023486,\n",
       "   0.05849574907752108,\n",
       "   0.05850868737402041,\n",
       "   0.05850008070961503,\n",
       "   0.05848491218957034,\n",
       "   0.05850279853363668,\n",
       "   0.0584802825096225,\n",
       "   0.05850619210684595,\n",
       "   0.05849178231452122,\n",
       "   0.05849382631049668,\n",
       "   0.058479172286908486,\n",
       "   0.05847460198993525,\n",
       "   0.05853072956573865,\n",
       "   0.05848782161050592,\n",
       "   0.05850367900753809,\n",
       "   0.05846782592702503,\n",
       "   0.058495297954102195,\n",
       "   0.058503536153430784,\n",
       "   0.05848439862905455,\n",
       "   0.05850554566738034,\n",
       "   0.05848233983043797,\n",
       "   0.058510235543093404,\n",
       "   0.05848911980952113,\n",
       "   0.05847826787262909,\n",
       "   0.05849819060199517,\n",
       "   0.058503930135206744,\n",
       "   0.05849760155047267,\n",
       "   0.058486427354418544,\n",
       "   0.058476317298313804,\n",
       "   0.058506952287737,\n",
       "   0.058497367171216605,\n",
       "   0.05850154989021869,\n",
       "   0.058489665413690994,\n",
       "   0.05849068499793691,\n",
       "   0.05849001609589443,\n",
       "   0.05848851102935381,\n",
       "   0.05850469204019909,\n",
       "   0.05849823193116622,\n",
       "   0.05849744172628261,\n",
       "   0.0584453754681201,\n",
       "   0.058510082418268376,\n",
       "   0.058508954314160935,\n",
       "   0.058492444664978785,\n",
       "   0.058501262310122655,\n",
       "   0.05850596383583447,\n",
       "   0.05848954462808026,\n",
       "   0.05848428796638142,\n",
       "   0.05849624655463479,\n",
       "   0.058468486922831575,\n",
       "   0.058509507553636535,\n",
       "   0.05850403274386382,\n",
       "   0.05847988739486568,\n",
       "   0.05848811362400528,\n",
       "   0.058517265369084255,\n",
       "   0.05850160181029769,\n",
       "   0.058497959941871895,\n",
       "   0.05849180472783806,\n",
       "   0.058498933640393345,\n",
       "   0.058489667384092475,\n",
       "   0.05850268385627053,\n",
       "   0.05847434165063969,\n",
       "   0.058508403340647046,\n",
       "   0.05848495364189148,\n",
       "   0.05846451225359578,\n",
       "   0.0584959188276086,\n",
       "   0.05851375290677567,\n",
       "   0.05850056005903512,\n",
       "   0.0584400541526227,\n",
       "   0.058490485913497356,\n",
       "   0.05850246622542704,\n",
       "   0.058485264374204905,\n",
       "   0.05850124566023015,\n",
       "   0.05848336638497912,\n",
       "   0.058508222457791165,\n",
       "   0.058479943797608056,\n",
       "   0.05851483778520064,\n",
       "   0.05849595468891554,\n",
       "   0.058491546457464044,\n",
       "   0.05848983979422199,\n",
       "   0.05850554473143964,\n",
       "   0.05849336385726929,\n",
       "   0.05848645740304111,\n",
       "   0.05850367418005447,\n",
       "   0.058491021665659816,\n",
       "   0.05850375437539471,\n",
       "   0.05847346063487786,\n",
       "   0.058510759694517155,\n",
       "   0.058522321123722174,\n",
       "   0.05849365414666735,\n",
       "   0.058505844749695016,\n",
       "   0.058501495827328075,\n",
       "   0.05849729766530439,\n",
       "   0.05849334829109759,\n",
       "   0.058513653967991346,\n",
       "   0.058478413238998286,\n",
       "   0.05847828910370503,\n",
       "   0.05849894639874293,\n",
       "   0.05849536529257278,\n",
       "   0.05846970933527986,\n",
       "   0.05850103822621432,\n",
       "   0.058519911076411725,\n",
       "   0.05849280598735021,\n",
       "   0.058432261313288665,\n",
       "   0.05852961195401909,\n",
       "   0.05848598879230909,\n",
       "   0.058492207724200794,\n",
       "   0.05850807251023852,\n",
       "   0.058478915986935956,\n",
       "   0.058493522474588444,\n",
       "   0.058458866708534805,\n",
       "   0.05849821818761589,\n",
       "   0.05848584079052791,\n",
       "   0.05847484897976079,\n",
       "   0.0585108444217808,\n",
       "   0.05849024830770887,\n",
       "   0.05848631637155517,\n",
       "   0.058520674804025445,\n",
       "   0.058495619080283424,\n",
       "   0.058503788340190224,\n",
       "   0.05851134214519469,\n",
       "   0.05849449368547802,\n",
       "   0.05849241609415732,\n",
       "   0.05848943522153807,\n",
       "   0.058494941065134094,\n",
       "   0.058496640831970974,\n",
       "   0.05847564600715952,\n",
       "   0.05853766604395937,\n",
       "   0.05852407266285794,\n",
       "   0.05847933839175327,\n",
       "   0.05848268566052776,\n",
       "   0.058473367656558015,\n",
       "   0.05847299276304639,\n",
       "   0.05849516475496213,\n",
       "   0.05848262019393858,\n",
       "   0.05852730205236387,\n",
       "   0.058518616965979584,\n",
       "   0.058501132411405074,\n",
       "   0.05848453384785613,\n",
       "   0.05850098497611432,\n",
       "   0.05846862398888454,\n",
       "   0.05852202263252794,\n",
       "   0.05846422196419771,\n",
       "   0.05850535670587839,\n",
       "   0.05849260860238194,\n",
       "   0.05849620517620371,\n",
       "   0.05846166216637477,\n",
       "   0.058503657751832125,\n",
       "   0.05852159707506826,\n",
       "   0.05851378465486952,\n",
       "   0.05847357408074308,\n",
       "   0.0584863763210202,\n",
       "   0.05848509523986785,\n",
       "   0.05852629872393017,\n",
       "   0.05848306126830992,\n",
       "   0.058509242510007435,\n",
       "   0.05851267482623581,\n",
       "   0.05849747990281129,\n",
       "   0.05849541115366723,\n",
       "   0.05847794502234656,\n",
       "   0.05851166450287685,\n",
       "   0.0584987040886209,\n",
       "   0.058493797247074854,\n",
       "   0.058488553910216026,\n",
       "   0.058500569738632394,\n",
       "   0.058493190412678996,\n",
       "   0.058502983899155925,\n",
       "   0.05850364403291182,\n",
       "   0.0584994418315651,\n",
       "   0.058497977158254826,\n",
       "   0.05849789812052546,\n",
       "   0.058492331810234006,\n",
       "   0.05849543711370673,\n",
       "   0.058500405382518925,\n",
       "   0.05852623660702351,\n",
       "   0.05847716700932211,\n",
       "   0.058493185338895186,\n",
       "   0.05851069422792797,\n",
       "   0.058504762531312045,\n",
       "   0.05847764525039137,\n",
       "   0.05850239526634374,\n",
       "   0.058501392184210216,\n",
       "   0.05850737444625413,\n",
       "   0.0585027675490734,\n",
       "   0.05851527134741633,\n",
       "   0.05850574384050921,\n",
       "   0.05850293109239626,\n",
       "   0.05850657695088505,\n",
       "   0.05850454492509858,\n",
       "   0.05848264157279464,\n",
       "   0.05852130257393703,\n",
       "   0.05850978671026624,\n",
       "   0.058497467612432054,\n",
       "   0.05847699004263917,\n",
       "   0.058480684914864786,\n",
       "   0.0585109522520018,\n",
       "   0.058479336667651975,\n",
       "   0.05849458003832289,\n",
       "   0.05850944248112765,\n",
       "   0.05850921815091913,\n",
       "   0.05848081781844462,\n",
       "   0.058501556318653516,\n",
       "   0.0584769576049048,\n",
       "   0.0584911142991594,\n",
       "   0.05850392171174042,\n",
       "   0.05849965882202811,\n",
       "   0.05849605323361956,\n",
       "   0.05849360720185209,\n",
       "   0.058476164813869254,\n",
       "   0.058495330958326985,\n",
       "   0.05850120250843773,\n",
       "   0.058498735762824695,\n",
       "   0.058510728414393656,\n",
       "   0.05848194001134762,\n",
       "   0.05851143588704511,\n",
       "   0.058524839887934284,\n",
       "   0.058488112047684095,\n",
       "   0.05850814167133048,\n",
       "   0.05851578714926381,\n",
       "   0.058500857097058254,\n",
       "   0.05849228550579922,\n",
       "   0.058460774254207766,\n",
       "   0.058538991754705255,\n",
       "   0.05849379626187411,\n",
       "   0.05849238781889608,\n",
       "   0.058465969119190185,\n",
       "   0.058500369028611617,\n",
       "   0.05848313175942287,\n",
       "   0.0584784988529426,\n",
       "   0.05852195246160523,\n",
       "   0.058498965659417396,\n",
       "   0.05850876953976213,\n",
       "   0.05848744152006039,\n",
       "   0.05848445951446029,\n",
       "   0.058493918426765885,\n",
       "   0.0584891571732592,\n",
       "   0.05849616645781462,\n",
       "   0.058494918011436775,\n",
       "   0.05851399612820838,\n",
       "   0.05849622056996527,\n",
       "   0.0584840575032983,\n",
       "   0.05849158079170984,\n",
       "   0.05847722269779394,\n",
       "   0.05848586608555691,\n",
       "   0.05847542630739448,\n",
       "   0.0584993655523978,\n",
       "   0.058484706750586014,\n",
       "   0.058488195789747004,\n",
       "   0.05848800539970398,\n",
       "   0.05847985111484843,\n",
       "   0.05851227902183848,\n",
       "   0.0584701739805789,\n",
       "   0.05852427999835369,\n",
       "   0.05849246205377184,\n",
       "   0.05848489201758519,\n",
       "   0.05851969147516676,\n",
       "   0.0584875660001739,\n",
       "   0.0584552517114592,\n",
       "   0.058510318595515795,\n",
       "   0.05849374953872901,\n",
       "   0.05848740410706228,\n",
       "   0.05850789975528874,\n",
       "   0.058509613339566,\n",
       "   0.058479542254416414,\n",
       "   0.05849680922740747,\n",
       "   0.05848756762575512,\n",
       "   0.058504967403805945,\n",
       "   0.058502431891181254,\n",
       "   0.05849240055261565,\n",
       "   0.05849296549135003,\n",
       "   0.0584768869167517,\n",
       "   0.05850790610983352,\n",
       "   0.058485074033421916,\n",
       "   0.058490645097306934,\n",
       "   0.05849404522210113,\n",
       "   0.058486991283322166,\n",
       "   0.058466658267107875,\n",
       "   0.058489737284084985,\n",
       "   0.0585001022362512,\n",
       "   0.05849621719565273,\n",
       "   0.05850708524057688,\n",
       "   0.05847692243323838,\n",
       "   0.05849696777083657,\n",
       "   0.05852010461909712,\n",
       "   0.05848931157884519,\n",
       "   0.05849271608778268,\n",
       "   0.05850392363288186,\n",
       "   0.058498746944853096,\n",
       "   0.058486711141491725,\n",
       "   0.05849189280478422,\n",
       "   0.058518373276576524,\n",
       "   0.05845129159856434,\n",
       "   0.05850083946196501,\n",
       "   0.05849406512315608,\n",
       "   0.05850925908600989,\n",
       "   0.058481337413314946,\n",
       "   0.05851990294850562,\n",
       "   0.05849547942807852,\n",
       "   0.058502218989301316,\n",
       "   0.05848266115365935,\n",
       "   0.05853441071904395,\n",
       "   0.05848089018143898,\n",
       "   0.05848312806492009,\n",
       "   ...],\n",
       "  'val': []},\n",
       " 0,\n",
       " inf)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val(model = m1_mlp11, params = m1_mlp11_params)\n",
    "train_val(model = m1_mlp12, params = m1_mlp12_params)\n",
    "train_val(model = m1_mlp13, params = m1_mlp13_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m1_mlp11.state_dict(), '../model/MLP2LGBM_V2/round1/m1_mlp11.pt')\n",
    "torch.save(m1_mlp12.state_dict(), '../model/MLP2LGBM_V2/round1/m1_mlp12.pt')\n",
    "torch.save(m1_mlp13.state_dict(), '../model/MLP2LGBM_V2/round1/m1_mlp13.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.externals \n",
    "import joblib\n",
    "scaler_filename1 = \"../model/MLP2LGBM_V2/round1/mms_Y_r1_M1_T1.pkl\"\n",
    "scaler_filename2 = \"../model/MLP2LGBM_V2/round1/mms_Y_r1_M1_T2.pkl\"\n",
    "scaler_filename3 = \"../model/MLP2LGBM_V2/round1/mms_Y_r1_M1_T3.pkl\"\n",
    "joblib.dump(mms_Y_111, scaler_filename1) \n",
    "joblib.dump(mms_Y_112, scaler_filename2) \n",
    "joblib.dump(mms_Y_113, scaler_filename3) \n",
    "\n",
    "# And now to load...\n",
    "\n",
    "# scaler = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### round2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "seed_everything(seed)\n",
    "m1_mlp21 = MLP_regressor(num_features = X_r2_M1_T1.shape[1]).to(DEVICE) # num_features = 6\n",
    "m1_mlp22 = MLP_regressor(num_features = X_r2_M1_T1.shape[1]).to(DEVICE) # num_features = 6\n",
    "m1_mlp23 = MLP_regressor(num_features = X_r2_M1_T1.shape[1]).to(DEVICE) # num_features = 6\n",
    "\n",
    "loss_func = nn.MSELoss(reduction = 'sum').to(DEVICE) # utils 내에서 데이터 개수로 나누기 때문에 -> 기본값 평균이 아닌 sum으로 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(m1_mlp21.parameters(), lr = 0.01, momentum = 0.0) # Adam에 모델의 학습 파라미터를 넣어줌\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor = 0.5, patience = 250)\n",
    "\n",
    "m1_mlp21_params = {\n",
    "    'num_epochs': 5000,\n",
    "    'loss_func': loss_func,\n",
    "    'optimizer': opt,\n",
    "\n",
    "    'train_dl': train_dl211,\n",
    "    'val_dl': None,\n",
    "    'batch_size': 32,\n",
    "\n",
    "    'sanity_check': False, # True인 경우 데이터 조금만(batch 1개) 사용해서 학습 코드가 잘 돌아가는지 확인 / False면 데이터 전체 사용\n",
    "    'lr_scheduler': lr_scheduler,\n",
    "    'path2weights': '../model/MLP2LGBM_V2/round2/m1_mlp21.pt', # 모델 파라미터를 저장할 경로\n",
    "    'DEVICE': DEVICE, # cpu / cuda\n",
    "    'early_stopping': False, # 학습 조기 종료 적용 여부 -> True: 적용 / False: 미적용\n",
    "    'patience_limit': 500,\n",
    "    'num_folds': n_split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(m1_mlp22.parameters(), lr = 0.01, momentum = 0.0) # Adam에 모델의 학습 파라미터를 넣어줌\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor = 0.5, patience = 250)\n",
    "\n",
    "m1_mlp22_params = {\n",
    "    'num_epochs': 5000,\n",
    "    'loss_func': loss_func,\n",
    "    'optimizer': opt,\n",
    "\n",
    "    'train_dl': train_dl212,\n",
    "    'val_dl': None,\n",
    "    'batch_size': 32,\n",
    "\n",
    "    'sanity_check': False, # True인 경우 데이터 조금만(batch 1개) 사용해서 학습 코드가 잘 돌아가는지 확인 / False면 데이터 전체 사용\n",
    "    'lr_scheduler': lr_scheduler,\n",
    "    'path2weights': '../model/MLP2LGBM_V2/round2/m1_mlp22.pt', # 모델 파라미터를 저장할 경로\n",
    "    'DEVICE': DEVICE, # cpu / cuda\n",
    "    'early_stopping': False, # 학습 조기 종료 적용 여부 -> True: 적용 / False: 미적용\n",
    "    'patience_limit': 500,\n",
    "    'num_folds': n_split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(m1_mlp23.parameters(), lr = 0.01, momentum = 0.0) # Adam에 모델의 학습 파라미터를 넣어줌\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor = 0.5, patience = 250)\n",
    "\n",
    "m1_mlp23_params = {\n",
    "    'num_epochs': 5000,\n",
    "    'loss_func': loss_func,\n",
    "    'optimizer': opt,\n",
    "\n",
    "    'train_dl': train_dl213,\n",
    "    'val_dl': None,\n",
    "    'batch_size': 32,\n",
    "\n",
    "    'sanity_check': False, # True인 경우 데이터 조금만(batch 1개) 사용해서 학습 코드가 잘 돌아가는지 확인 / False면 데이터 전체 사용\n",
    "    'lr_scheduler': lr_scheduler,\n",
    "    'path2weights': '../model/MLP2LGBM_V2/round2/m1_mlp23.pt', # 모델 파라미터를 저장할 경로\n",
    "    'DEVICE': DEVICE, # cpu / cuda\n",
    "    'early_stopping': False, # 학습 조기 종료 적용 여부 -> True: 적용 / False: 미적용\n",
    "    'patience_limit': 500,\n",
    "    'num_folds': n_split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4999, current lr=0.01\n",
      "train loss: 0.023765, val loss: 0.000000, mse: 0.00, time: 0.0050 min\n",
      "----------\n",
      "Epoch 100/4999, current lr=0.01\n",
      "train loss: 0.023731, val loss: 0.000000, mse: 0.00, time: 0.0729 min\n",
      "----------\n",
      "Epoch 200/4999, current lr=0.01\n",
      "train loss: 0.023768, val loss: 0.000000, mse: 0.00, time: 0.1404 min\n",
      "----------\n",
      "Epoch 300/4999, current lr=0.01\n",
      "train loss: 0.023636, val loss: 0.000000, mse: 0.00, time: 0.2079 min\n",
      "----------\n",
      "Epoch 400/4999, current lr=0.01\n",
      "train loss: 0.023622, val loss: 0.000000, mse: 0.00, time: 0.2762 min\n",
      "----------\n",
      "Epoch 500/4999, current lr=0.01\n",
      "train loss: 0.023733, val loss: 0.000000, mse: 0.00, time: 0.3387 min\n",
      "----------\n",
      "Epoch 600/4999, current lr=0.01\n",
      "train loss: 0.023745, val loss: 0.000000, mse: 0.00, time: 0.3992 min\n",
      "----------\n",
      "Epoch 700/4999, current lr=0.01\n",
      "train loss: 0.023734, val loss: 0.000000, mse: 0.00, time: 0.4649 min\n",
      "----------\n",
      "Epoch 800/4999, current lr=0.01\n",
      "train loss: 0.023631, val loss: 0.000000, mse: 0.00, time: 0.5334 min\n",
      "----------\n",
      "Epoch 900/4999, current lr=0.01\n",
      "train loss: 0.023707, val loss: 0.000000, mse: 0.00, time: 0.5987 min\n",
      "----------\n",
      "Epoch 1000/4999, current lr=0.01\n",
      "train loss: 0.023748, val loss: 0.000000, mse: 0.00, time: 0.6643 min\n",
      "----------\n",
      "Epoch 1100/4999, current lr=0.01\n",
      "train loss: 0.023743, val loss: 0.000000, mse: 0.00, time: 0.7280 min\n",
      "----------\n",
      "Epoch 1200/4999, current lr=0.01\n",
      "train loss: 0.023762, val loss: 0.000000, mse: 0.00, time: 0.7939 min\n",
      "----------\n",
      "Epoch 1300/4999, current lr=0.01\n",
      "train loss: 0.023806, val loss: 0.000000, mse: 0.00, time: 0.8594 min\n",
      "----------\n",
      "Epoch 1400/4999, current lr=0.01\n",
      "train loss: 0.023725, val loss: 0.000000, mse: 0.00, time: 0.9236 min\n",
      "----------\n",
      "Epoch 1500/4999, current lr=0.01\n",
      "train loss: 0.023768, val loss: 0.000000, mse: 0.00, time: 0.9854 min\n",
      "----------\n",
      "Epoch 1600/4999, current lr=0.01\n",
      "train loss: 0.023621, val loss: 0.000000, mse: 0.00, time: 1.0466 min\n",
      "----------\n",
      "Epoch 1700/4999, current lr=0.01\n",
      "train loss: 0.023724, val loss: 0.000000, mse: 0.00, time: 1.1082 min\n",
      "----------\n",
      "Epoch 1800/4999, current lr=0.01\n",
      "train loss: 0.023695, val loss: 0.000000, mse: 0.00, time: 1.1671 min\n",
      "----------\n",
      "Epoch 1900/4999, current lr=0.01\n",
      "train loss: 0.023641, val loss: 0.000000, mse: 0.00, time: 1.2284 min\n",
      "----------\n",
      "Epoch 2000/4999, current lr=0.01\n",
      "train loss: 0.023810, val loss: 0.000000, mse: 0.00, time: 1.2982 min\n",
      "----------\n",
      "Epoch 2100/4999, current lr=0.01\n",
      "train loss: 0.023666, val loss: 0.000000, mse: 0.00, time: 1.3693 min\n",
      "----------\n",
      "Epoch 2200/4999, current lr=0.01\n",
      "train loss: 0.023633, val loss: 0.000000, mse: 0.00, time: 1.4413 min\n",
      "----------\n",
      "Epoch 2300/4999, current lr=0.01\n",
      "train loss: 0.023621, val loss: 0.000000, mse: 0.00, time: 1.5125 min\n",
      "----------\n",
      "Epoch 2400/4999, current lr=0.01\n",
      "train loss: 0.023719, val loss: 0.000000, mse: 0.00, time: 1.5768 min\n",
      "----------\n",
      "Epoch 2500/4999, current lr=0.01\n",
      "train loss: 0.023765, val loss: 0.000000, mse: 0.00, time: 1.6377 min\n",
      "----------\n",
      "Epoch 2600/4999, current lr=0.01\n",
      "train loss: 0.023745, val loss: 0.000000, mse: 0.00, time: 1.7007 min\n",
      "----------\n",
      "Epoch 2700/4999, current lr=0.01\n",
      "train loss: 0.023844, val loss: 0.000000, mse: 0.00, time: 1.7639 min\n",
      "----------\n",
      "Epoch 2800/4999, current lr=0.01\n",
      "train loss: 0.023663, val loss: 0.000000, mse: 0.00, time: 1.8244 min\n",
      "----------\n",
      "Epoch 2900/4999, current lr=0.01\n",
      "train loss: 0.023762, val loss: 0.000000, mse: 0.00, time: 1.8834 min\n",
      "----------\n",
      "Epoch 3000/4999, current lr=0.01\n",
      "train loss: 0.023647, val loss: 0.000000, mse: 0.00, time: 1.9477 min\n",
      "----------\n",
      "Epoch 3100/4999, current lr=0.01\n",
      "train loss: 0.023626, val loss: 0.000000, mse: 0.00, time: 2.0144 min\n",
      "----------\n",
      "Epoch 3200/4999, current lr=0.01\n",
      "train loss: 0.023677, val loss: 0.000000, mse: 0.00, time: 2.0768 min\n",
      "----------\n",
      "Epoch 3300/4999, current lr=0.01\n",
      "train loss: 0.023689, val loss: 0.000000, mse: 0.00, time: 2.1382 min\n",
      "----------\n",
      "Epoch 3400/4999, current lr=0.01\n",
      "train loss: 0.023758, val loss: 0.000000, mse: 0.00, time: 2.2000 min\n",
      "----------\n",
      "Epoch 3500/4999, current lr=0.01\n",
      "train loss: 0.023704, val loss: 0.000000, mse: 0.00, time: 2.2646 min\n",
      "----------\n",
      "Epoch 3600/4999, current lr=0.01\n",
      "train loss: 0.023809, val loss: 0.000000, mse: 0.00, time: 2.3273 min\n",
      "----------\n",
      "Epoch 3700/4999, current lr=0.01\n",
      "train loss: 0.023641, val loss: 0.000000, mse: 0.00, time: 2.3883 min\n",
      "----------\n",
      "Epoch 3800/4999, current lr=0.01\n",
      "train loss: 0.023679, val loss: 0.000000, mse: 0.00, time: 2.4560 min\n",
      "----------\n",
      "Epoch 3900/4999, current lr=0.01\n",
      "train loss: 0.023747, val loss: 0.000000, mse: 0.00, time: 2.5265 min\n",
      "----------\n",
      "Epoch 4000/4999, current lr=0.01\n",
      "train loss: 0.023680, val loss: 0.000000, mse: 0.00, time: 2.5964 min\n",
      "----------\n",
      "Epoch 4100/4999, current lr=0.01\n",
      "train loss: 0.023702, val loss: 0.000000, mse: 0.00, time: 2.6646 min\n",
      "----------\n",
      "Epoch 4200/4999, current lr=0.01\n",
      "train loss: 0.023813, val loss: 0.000000, mse: 0.00, time: 2.7357 min\n",
      "----------\n",
      "Epoch 4300/4999, current lr=0.01\n",
      "train loss: 0.023723, val loss: 0.000000, mse: 0.00, time: 2.8002 min\n",
      "----------\n",
      "Epoch 4400/4999, current lr=0.01\n",
      "train loss: 0.023689, val loss: 0.000000, mse: 0.00, time: 2.8671 min\n",
      "----------\n",
      "Epoch 4500/4999, current lr=0.01\n",
      "train loss: 0.023716, val loss: 0.000000, mse: 0.00, time: 2.9376 min\n",
      "----------\n",
      "Epoch 4600/4999, current lr=0.01\n",
      "train loss: 0.023728, val loss: 0.000000, mse: 0.00, time: 3.0072 min\n",
      "----------\n",
      "Epoch 4700/4999, current lr=0.01\n",
      "train loss: 0.023661, val loss: 0.000000, mse: 0.00, time: 3.0719 min\n",
      "----------\n",
      "Epoch 4800/4999, current lr=0.01\n",
      "train loss: 0.023652, val loss: 0.000000, mse: 0.00, time: 3.1340 min\n",
      "----------\n",
      "Epoch 4900/4999, current lr=0.01\n",
      "train loss: 0.023720, val loss: 0.000000, mse: 0.00, time: 3.1991 min\n",
      "----------\n",
      "Epoch 0/4999, current lr=0.01\n",
      "train loss: 0.022690, val loss: 0.000000, mse: 0.00, time: 0.0006 min\n",
      "----------\n",
      "Epoch 100/4999, current lr=0.01\n",
      "train loss: 0.021671, val loss: 0.000000, mse: 0.00, time: 0.0649 min\n",
      "----------\n",
      "Epoch 200/4999, current lr=0.01\n",
      "train loss: 0.021622, val loss: 0.000000, mse: 0.00, time: 0.1277 min\n",
      "----------\n",
      "Epoch 300/4999, current lr=0.01\n",
      "train loss: 0.021811, val loss: 0.000000, mse: 0.00, time: 0.1948 min\n",
      "----------\n",
      "Epoch 400/4999, current lr=0.01\n",
      "train loss: 0.021649, val loss: 0.000000, mse: 0.00, time: 0.2626 min\n",
      "----------\n",
      "Epoch 500/4999, current lr=0.01\n",
      "train loss: 0.021665, val loss: 0.000000, mse: 0.00, time: 0.3266 min\n",
      "----------\n",
      "Epoch 600/4999, current lr=0.01\n",
      "train loss: 0.021725, val loss: 0.000000, mse: 0.00, time: 0.3887 min\n",
      "----------\n",
      "Epoch 700/4999, current lr=0.01\n",
      "train loss: 0.021581, val loss: 0.000000, mse: 0.00, time: 0.4569 min\n",
      "----------\n",
      "Epoch 800/4999, current lr=0.01\n",
      "train loss: 0.021720, val loss: 0.000000, mse: 0.00, time: 0.5285 min\n",
      "----------\n",
      "Epoch 900/4999, current lr=0.01\n",
      "train loss: 0.021584, val loss: 0.000000, mse: 0.00, time: 0.6003 min\n",
      "----------\n",
      "Epoch 1000/4999, current lr=0.01\n",
      "train loss: 0.021704, val loss: 0.000000, mse: 0.00, time: 0.6723 min\n",
      "----------\n",
      "Epoch 1100/4999, current lr=0.01\n",
      "train loss: 0.021644, val loss: 0.000000, mse: 0.00, time: 0.7475 min\n",
      "----------\n",
      "Epoch 1200/4999, current lr=0.01\n",
      "train loss: 0.021704, val loss: 0.000000, mse: 0.00, time: 0.8133 min\n",
      "----------\n",
      "Epoch 1300/4999, current lr=0.01\n",
      "train loss: 0.021666, val loss: 0.000000, mse: 0.00, time: 0.8777 min\n",
      "----------\n",
      "Epoch 1400/4999, current lr=0.01\n",
      "train loss: 0.021734, val loss: 0.000000, mse: 0.00, time: 0.9456 min\n",
      "----------\n",
      "Epoch 1500/4999, current lr=0.01\n",
      "train loss: 0.021662, val loss: 0.000000, mse: 0.00, time: 1.0098 min\n",
      "----------\n",
      "Epoch 1600/4999, current lr=0.01\n",
      "train loss: 0.021702, val loss: 0.000000, mse: 0.00, time: 1.0765 min\n",
      "----------\n",
      "Epoch 1700/4999, current lr=0.01\n",
      "train loss: 0.021591, val loss: 0.000000, mse: 0.00, time: 1.1429 min\n",
      "----------\n",
      "Epoch 1800/4999, current lr=0.01\n",
      "train loss: 0.021726, val loss: 0.000000, mse: 0.00, time: 1.2124 min\n",
      "----------\n",
      "Epoch 1900/4999, current lr=0.01\n",
      "train loss: 0.021718, val loss: 0.000000, mse: 0.00, time: 1.2779 min\n",
      "----------\n",
      "Epoch 2000/4999, current lr=0.01\n",
      "train loss: 0.021682, val loss: 0.000000, mse: 0.00, time: 1.3376 min\n",
      "----------\n",
      "Epoch 2100/4999, current lr=0.01\n",
      "train loss: 0.021634, val loss: 0.000000, mse: 0.00, time: 1.3994 min\n",
      "----------\n",
      "Epoch 2200/4999, current lr=0.01\n",
      "train loss: 0.021581, val loss: 0.000000, mse: 0.00, time: 1.4656 min\n",
      "----------\n",
      "Epoch 2300/4999, current lr=0.01\n",
      "train loss: 0.021713, val loss: 0.000000, mse: 0.00, time: 1.5272 min\n",
      "----------\n",
      "Epoch 2400/4999, current lr=0.01\n",
      "train loss: 0.021677, val loss: 0.000000, mse: 0.00, time: 1.5913 min\n",
      "----------\n",
      "Epoch 2500/4999, current lr=0.01\n",
      "train loss: 0.021640, val loss: 0.000000, mse: 0.00, time: 1.6625 min\n",
      "----------\n",
      "Epoch 2600/4999, current lr=0.01\n",
      "train loss: 0.021646, val loss: 0.000000, mse: 0.00, time: 1.7369 min\n",
      "----------\n",
      "Epoch 2700/4999, current lr=0.01\n",
      "train loss: 0.021668, val loss: 0.000000, mse: 0.00, time: 1.8106 min\n",
      "----------\n",
      "Epoch 2800/4999, current lr=0.01\n",
      "train loss: 0.021574, val loss: 0.000000, mse: 0.00, time: 1.8830 min\n",
      "----------\n",
      "Epoch 2900/4999, current lr=0.01\n",
      "train loss: 0.021697, val loss: 0.000000, mse: 0.00, time: 1.9533 min\n",
      "----------\n",
      "Epoch 3000/4999, current lr=0.01\n",
      "train loss: 0.021654, val loss: 0.000000, mse: 0.00, time: 2.0157 min\n",
      "----------\n",
      "Epoch 3100/4999, current lr=0.01\n",
      "train loss: 0.021752, val loss: 0.000000, mse: 0.00, time: 2.0767 min\n",
      "----------\n",
      "Epoch 3200/4999, current lr=0.01\n",
      "train loss: 0.021768, val loss: 0.000000, mse: 0.00, time: 2.1385 min\n",
      "----------\n",
      "Epoch 3300/4999, current lr=0.01\n",
      "train loss: 0.021721, val loss: 0.000000, mse: 0.00, time: 2.2051 min\n",
      "----------\n",
      "Epoch 3400/4999, current lr=0.01\n",
      "train loss: 0.021665, val loss: 0.000000, mse: 0.00, time: 2.2675 min\n",
      "----------\n",
      "Epoch 3500/4999, current lr=0.01\n",
      "train loss: 0.021626, val loss: 0.000000, mse: 0.00, time: 2.3295 min\n",
      "----------\n",
      "Epoch 3600/4999, current lr=0.01\n",
      "train loss: 0.021653, val loss: 0.000000, mse: 0.00, time: 2.3918 min\n",
      "----------\n",
      "Epoch 3700/4999, current lr=0.01\n",
      "train loss: 0.021675, val loss: 0.000000, mse: 0.00, time: 2.4534 min\n",
      "----------\n",
      "Epoch 3800/4999, current lr=0.01\n",
      "train loss: 0.021655, val loss: 0.000000, mse: 0.00, time: 2.5156 min\n",
      "----------\n",
      "Epoch 3900/4999, current lr=0.01\n",
      "train loss: 0.021674, val loss: 0.000000, mse: 0.00, time: 2.5790 min\n",
      "----------\n",
      "Epoch 4000/4999, current lr=0.01\n",
      "train loss: 0.021698, val loss: 0.000000, mse: 0.00, time: 2.6442 min\n",
      "----------\n",
      "Epoch 4100/4999, current lr=0.01\n",
      "train loss: 0.021632, val loss: 0.000000, mse: 0.00, time: 2.7112 min\n",
      "----------\n",
      "Epoch 4200/4999, current lr=0.01\n",
      "train loss: 0.021689, val loss: 0.000000, mse: 0.00, time: 2.7740 min\n",
      "----------\n",
      "Epoch 4300/4999, current lr=0.01\n",
      "train loss: 0.021668, val loss: 0.000000, mse: 0.00, time: 2.8337 min\n",
      "----------\n",
      "Epoch 4400/4999, current lr=0.01\n",
      "train loss: 0.021719, val loss: 0.000000, mse: 0.00, time: 2.9006 min\n",
      "----------\n",
      "Epoch 4500/4999, current lr=0.01\n",
      "train loss: 0.021632, val loss: 0.000000, mse: 0.00, time: 2.9715 min\n",
      "----------\n",
      "Epoch 4600/4999, current lr=0.01\n",
      "train loss: 0.021660, val loss: 0.000000, mse: 0.00, time: 3.0433 min\n",
      "----------\n",
      "Epoch 4700/4999, current lr=0.01\n",
      "train loss: 0.021590, val loss: 0.000000, mse: 0.00, time: 3.1148 min\n",
      "----------\n",
      "Epoch 4800/4999, current lr=0.01\n",
      "train loss: 0.021726, val loss: 0.000000, mse: 0.00, time: 3.1829 min\n",
      "----------\n",
      "Epoch 4900/4999, current lr=0.01\n",
      "train loss: 0.021608, val loss: 0.000000, mse: 0.00, time: 3.2507 min\n",
      "----------\n",
      "Epoch 0/4999, current lr=0.01\n",
      "train loss: 0.022359, val loss: 0.000000, mse: 0.00, time: 0.0009 min\n",
      "----------\n",
      "Epoch 100/4999, current lr=0.01\n",
      "train loss: 0.021431, val loss: 0.000000, mse: 0.00, time: 0.0865 min\n",
      "----------\n",
      "Epoch 200/4999, current lr=0.01\n",
      "train loss: 0.021473, val loss: 0.000000, mse: 0.00, time: 0.1692 min\n",
      "----------\n",
      "Epoch 300/4999, current lr=0.01\n",
      "train loss: 0.021451, val loss: 0.000000, mse: 0.00, time: 0.2458 min\n",
      "----------\n",
      "Epoch 400/4999, current lr=0.01\n",
      "train loss: 0.021476, val loss: 0.000000, mse: 0.00, time: 0.3193 min\n",
      "----------\n",
      "Epoch 500/4999, current lr=0.01\n",
      "train loss: 0.021457, val loss: 0.000000, mse: 0.00, time: 0.3935 min\n",
      "----------\n",
      "Epoch 600/4999, current lr=0.01\n",
      "train loss: 0.021468, val loss: 0.000000, mse: 0.00, time: 0.4691 min\n",
      "----------\n",
      "Epoch 700/4999, current lr=0.01\n",
      "train loss: 0.021449, val loss: 0.000000, mse: 0.00, time: 0.5426 min\n",
      "----------\n",
      "Epoch 800/4999, current lr=0.01\n",
      "train loss: 0.021463, val loss: 0.000000, mse: 0.00, time: 0.6141 min\n",
      "----------\n",
      "Epoch 900/4999, current lr=0.01\n",
      "train loss: 0.021453, val loss: 0.000000, mse: 0.00, time: 0.6852 min\n",
      "----------\n",
      "Epoch 1000/4999, current lr=0.01\n",
      "train loss: 0.021458, val loss: 0.000000, mse: 0.00, time: 0.7590 min\n",
      "----------\n",
      "Epoch 1100/4999, current lr=0.01\n",
      "train loss: 0.021471, val loss: 0.000000, mse: 0.00, time: 0.8366 min\n",
      "----------\n",
      "Epoch 1200/4999, current lr=0.01\n",
      "train loss: 0.021463, val loss: 0.000000, mse: 0.00, time: 0.9192 min\n",
      "----------\n",
      "Epoch 1300/4999, current lr=0.01\n",
      "train loss: 0.021465, val loss: 0.000000, mse: 0.00, time: 0.9991 min\n",
      "----------\n",
      "Epoch 1400/4999, current lr=0.01\n",
      "train loss: 0.021472, val loss: 0.000000, mse: 0.00, time: 1.0780 min\n",
      "----------\n",
      "Epoch 1500/4999, current lr=0.01\n",
      "train loss: 0.021467, val loss: 0.000000, mse: 0.00, time: 1.1535 min\n",
      "----------\n",
      "Epoch 1600/4999, current lr=0.01\n",
      "train loss: 0.021474, val loss: 0.000000, mse: 0.00, time: 1.2285 min\n",
      "----------\n",
      "Epoch 1700/4999, current lr=0.01\n",
      "train loss: 0.021449, val loss: 0.000000, mse: 0.00, time: 1.3009 min\n",
      "----------\n",
      "Epoch 1800/4999, current lr=0.01\n",
      "train loss: 0.021460, val loss: 0.000000, mse: 0.00, time: 1.3790 min\n",
      "----------\n",
      "Epoch 1900/4999, current lr=0.01\n",
      "train loss: 0.021471, val loss: 0.000000, mse: 0.00, time: 1.4559 min\n",
      "----------\n",
      "Epoch 2000/4999, current lr=0.01\n",
      "train loss: 0.021459, val loss: 0.000000, mse: 0.00, time: 1.5308 min\n",
      "----------\n",
      "Epoch 2100/4999, current lr=0.01\n",
      "train loss: 0.021448, val loss: 0.000000, mse: 0.00, time: 1.6051 min\n",
      "----------\n",
      "Epoch 2200/4999, current lr=0.01\n",
      "train loss: 0.021457, val loss: 0.000000, mse: 0.00, time: 1.6828 min\n",
      "----------\n",
      "Epoch 2300/4999, current lr=0.01\n",
      "train loss: 0.021467, val loss: 0.000000, mse: 0.00, time: 1.7564 min\n",
      "----------\n",
      "Epoch 2400/4999, current lr=0.01\n",
      "train loss: 0.021466, val loss: 0.000000, mse: 0.00, time: 1.8337 min\n",
      "----------\n",
      "Epoch 2500/4999, current lr=0.01\n",
      "train loss: 0.021458, val loss: 0.000000, mse: 0.00, time: 1.9173 min\n",
      "----------\n",
      "Epoch 2600/4999, current lr=0.01\n",
      "train loss: 0.021469, val loss: 0.000000, mse: 0.00, time: 1.9906 min\n",
      "----------\n",
      "Epoch 2700/4999, current lr=0.01\n",
      "train loss: 0.021443, val loss: 0.000000, mse: 0.00, time: 2.0661 min\n",
      "----------\n",
      "Epoch 2800/4999, current lr=0.01\n",
      "train loss: 0.021455, val loss: 0.000000, mse: 0.00, time: 2.1488 min\n",
      "----------\n",
      "Epoch 2900/4999, current lr=0.01\n",
      "train loss: 0.021459, val loss: 0.000000, mse: 0.00, time: 2.2342 min\n",
      "----------\n",
      "Epoch 3000/4999, current lr=0.01\n",
      "train loss: 0.021448, val loss: 0.000000, mse: 0.00, time: 2.3161 min\n",
      "----------\n",
      "Epoch 3100/4999, current lr=0.01\n",
      "train loss: 0.021463, val loss: 0.000000, mse: 0.00, time: 2.3929 min\n",
      "----------\n",
      "Epoch 3200/4999, current lr=0.01\n",
      "train loss: 0.021477, val loss: 0.000000, mse: 0.00, time: 2.4656 min\n",
      "----------\n",
      "Epoch 3300/4999, current lr=0.01\n",
      "train loss: 0.021466, val loss: 0.000000, mse: 0.00, time: 2.5332 min\n",
      "----------\n",
      "Epoch 3400/4999, current lr=0.01\n",
      "train loss: 0.021467, val loss: 0.000000, mse: 0.00, time: 2.6076 min\n",
      "----------\n",
      "Epoch 3500/4999, current lr=0.01\n",
      "train loss: 0.021441, val loss: 0.000000, mse: 0.00, time: 2.6836 min\n",
      "----------\n",
      "Epoch 3600/4999, current lr=0.01\n",
      "train loss: 0.021480, val loss: 0.000000, mse: 0.00, time: 2.7579 min\n",
      "----------\n",
      "Epoch 3700/4999, current lr=0.01\n",
      "train loss: 0.021460, val loss: 0.000000, mse: 0.00, time: 2.8346 min\n",
      "----------\n",
      "Epoch 3800/4999, current lr=0.01\n",
      "train loss: 0.021470, val loss: 0.000000, mse: 0.00, time: 2.9115 min\n",
      "----------\n",
      "Epoch 3900/4999, current lr=0.01\n",
      "train loss: 0.021449, val loss: 0.000000, mse: 0.00, time: 2.9843 min\n",
      "----------\n",
      "Epoch 4000/4999, current lr=0.01\n",
      "train loss: 0.021463, val loss: 0.000000, mse: 0.00, time: 3.0573 min\n",
      "----------\n",
      "Epoch 4100/4999, current lr=0.01\n",
      "train loss: 0.021466, val loss: 0.000000, mse: 0.00, time: 3.1301 min\n",
      "----------\n",
      "Epoch 4200/4999, current lr=0.01\n",
      "train loss: 0.021464, val loss: 0.000000, mse: 0.00, time: 3.2020 min\n",
      "----------\n",
      "Epoch 4300/4999, current lr=0.01\n",
      "train loss: 0.021470, val loss: 0.000000, mse: 0.00, time: 3.2760 min\n",
      "----------\n",
      "Epoch 4400/4999, current lr=0.01\n",
      "train loss: 0.021468, val loss: 0.000000, mse: 0.00, time: 3.3574 min\n",
      "----------\n",
      "Epoch 4500/4999, current lr=0.01\n",
      "train loss: 0.021472, val loss: 0.000000, mse: 0.00, time: 3.4392 min\n",
      "----------\n",
      "Epoch 4600/4999, current lr=0.01\n",
      "train loss: 0.021444, val loss: 0.000000, mse: 0.00, time: 3.5176 min\n",
      "----------\n",
      "Epoch 4700/4999, current lr=0.01\n",
      "train loss: 0.021457, val loss: 0.000000, mse: 0.00, time: 3.5903 min\n",
      "----------\n",
      "Epoch 4800/4999, current lr=0.01\n",
      "train loss: 0.021465, val loss: 0.000000, mse: 0.00, time: 3.6595 min\n",
      "----------\n",
      "Epoch 4900/4999, current lr=0.01\n",
      "train loss: 0.021469, val loss: 0.000000, mse: 0.00, time: 3.7300 min\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MLP_regressor(\n",
       "   (fc1): Linear(in_features=6, out_features=32, bias=True)\n",
       "   (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
       "   (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
       "   (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
       "   (fc5): Linear(in_features=64, out_features=32, bias=True)\n",
       "   (fc6): Linear(in_features=32, out_features=16, bias=True)\n",
       "   (fc7): Linear(in_features=16, out_features=8, bias=True)\n",
       "   (fc8): Linear(in_features=8, out_features=1, bias=True)\n",
       "   (dropout): Dropout(p=0.4, inplace=False)\n",
       " ),\n",
       " {'train': [0.02235936465599725,\n",
       "   0.0215628539872862,\n",
       "   0.021481196712161494,\n",
       "   0.021465978780722716,\n",
       "   0.02147086646052317,\n",
       "   0.021460339736147045,\n",
       "   0.021459246671051407,\n",
       "   0.02145256867547253,\n",
       "   0.021455561075962432,\n",
       "   0.021459195920540585,\n",
       "   0.021480510640441135,\n",
       "   0.02146362229501558,\n",
       "   0.021457414805147162,\n",
       "   0.021473379946348577,\n",
       "   0.021454621152759094,\n",
       "   0.02145419318646316,\n",
       "   0.021465806248771698,\n",
       "   0.0214652403756296,\n",
       "   0.021470121427195694,\n",
       "   0.02145594027032496,\n",
       "   0.021449139405088306,\n",
       "   0.021460436785369495,\n",
       "   0.021463127254945116,\n",
       "   0.02146933761375079,\n",
       "   0.02146228784347471,\n",
       "   0.021457305785531323,\n",
       "   0.021460091028965362,\n",
       "   0.021452857349918097,\n",
       "   0.021474792254910925,\n",
       "   0.02143843272909584,\n",
       "   0.021472160648013546,\n",
       "   0.021480867971523175,\n",
       "   0.021450232470183947,\n",
       "   0.021466695718250828,\n",
       "   0.021468413123451328,\n",
       "   0.021472774996302433,\n",
       "   0.021456152967397602,\n",
       "   0.021458781903215464,\n",
       "   0.021463434033374074,\n",
       "   0.021476019170769023,\n",
       "   0.02143769927044627,\n",
       "   0.021454063490713285,\n",
       "   0.02146998193748759,\n",
       "   0.0214636989648906,\n",
       "   0.021475374945961093,\n",
       "   0.021468627205528164,\n",
       "   0.021450908154372852,\n",
       "   0.02146644760464237,\n",
       "   0.02146670966722164,\n",
       "   0.021466280019134903,\n",
       "   0.02146052186419855,\n",
       "   0.021480208017024757,\n",
       "   0.0214697346153101,\n",
       "   0.021466145970514702,\n",
       "   0.021462920592533602,\n",
       "   0.021464778476730915,\n",
       "   0.0214538536625779,\n",
       "   0.021450236526267655,\n",
       "   0.021446705853790663,\n",
       "   0.021457435085565718,\n",
       "   0.021471950424162677,\n",
       "   0.0214566540421292,\n",
       "   0.021469797336214312,\n",
       "   0.02147474912192317,\n",
       "   0.021458721259817543,\n",
       "   0.021453100319225264,\n",
       "   0.021454888458568525,\n",
       "   0.02145646231797721,\n",
       "   0.021473055954296064,\n",
       "   0.02146554537333888,\n",
       "   0.021449540165944712,\n",
       "   0.021461091991282103,\n",
       "   0.021462610153736416,\n",
       "   0.021461491268205443,\n",
       "   0.021450389964946574,\n",
       "   0.021476140655422606,\n",
       "   0.021456080947179518,\n",
       "   0.02146556001481179,\n",
       "   0.02146923037485463,\n",
       "   0.02145347516071747,\n",
       "   0.021467371798155218,\n",
       "   0.021465569610912276,\n",
       "   0.02146243683035443,\n",
       "   0.021475070047180682,\n",
       "   0.021453040566187184,\n",
       "   0.021461983736125265,\n",
       "   0.021467239530254697,\n",
       "   0.021468764914516592,\n",
       "   0.021468835747588226,\n",
       "   0.021463919378415183,\n",
       "   0.02146665713599114,\n",
       "   0.021445008531150975,\n",
       "   0.021457261069681634,\n",
       "   0.021469171808963,\n",
       "   0.021455482031794506,\n",
       "   0.02146938035102306,\n",
       "   0.0214718572331662,\n",
       "   0.02146909236907959,\n",
       "   0.02144382356113418,\n",
       "   0.021444504884268734,\n",
       "   0.02143089682234768,\n",
       "   0.021461528069745456,\n",
       "   0.021465709793122477,\n",
       "   0.021472042724799318,\n",
       "   0.021474410983042102,\n",
       "   0.02147224543005599,\n",
       "   0.02145829289780613,\n",
       "   0.02146400950261666,\n",
       "   0.02145629680997603,\n",
       "   0.021467562137303015,\n",
       "   0.02146856151675782,\n",
       "   0.021428484936472786,\n",
       "   0.02147112090557937,\n",
       "   0.021462432774270717,\n",
       "   0.021457138694668212,\n",
       "   0.02147321018440595,\n",
       "   0.021445850613700898,\n",
       "   0.021461726323202934,\n",
       "   0.02146608038067323,\n",
       "   0.02147455452883392,\n",
       "   0.021467073032964808,\n",
       "   0.02146638992911058,\n",
       "   0.021456605171266928,\n",
       "   0.021435915978617687,\n",
       "   0.021463610819266545,\n",
       "   0.02147144113833479,\n",
       "   0.021457064695872708,\n",
       "   0.02145448492770373,\n",
       "   0.021462399830956675,\n",
       "   0.02147748826450332,\n",
       "   0.021463625856454935,\n",
       "   0.021470194634560232,\n",
       "   0.02145443724398791,\n",
       "   0.021469740353184618,\n",
       "   0.02147942103785598,\n",
       "   0.021470832330062677,\n",
       "   0.021450691895860855,\n",
       "   0.021449784718113814,\n",
       "   0.021449904619905463,\n",
       "   0.02145762384185158,\n",
       "   0.02147804810298429,\n",
       "   0.021463229646326597,\n",
       "   0.02145876280994336,\n",
       "   0.02143967675964862,\n",
       "   0.021460149199141507,\n",
       "   0.021465287663629935,\n",
       "   0.021466342937896857,\n",
       "   0.021468164515198514,\n",
       "   0.021458597203013314,\n",
       "   0.021473188914698686,\n",
       "   0.021475729902750228,\n",
       "   0.02147186148710765,\n",
       "   0.02146442144243549,\n",
       "   0.02146741839365346,\n",
       "   0.021459793846636887,\n",
       "   0.02145274882494661,\n",
       "   0.0214586525042522,\n",
       "   0.021459443044860332,\n",
       "   0.021461609389277415,\n",
       "   0.02146472841872219,\n",
       "   0.021464267212325607,\n",
       "   0.02146809387998462,\n",
       "   0.021446707337723726,\n",
       "   0.021460350915109467,\n",
       "   0.021468095067131075,\n",
       "   0.02146035378404673,\n",
       "   0.021466115896137918,\n",
       "   0.021456425219650584,\n",
       "   0.021449427189174034,\n",
       "   0.02146805312128977,\n",
       "   0.021456773449276493,\n",
       "   0.021445547495640167,\n",
       "   0.02146676803525553,\n",
       "   0.02147821796385579,\n",
       "   0.021455971531848195,\n",
       "   0.021455060001230833,\n",
       "   0.021459480044258086,\n",
       "   0.021475642350699396,\n",
       "   0.021473415263955523,\n",
       "   0.021471565788712243,\n",
       "   0.021469046169296836,\n",
       "   0.021454049146026992,\n",
       "   0.021457676274153206,\n",
       "   0.021467187097953068,\n",
       "   0.02147745828905541,\n",
       "   0.021453435885955684,\n",
       "   0.02147005939879358,\n",
       "   0.02146139560398719,\n",
       "   0.021452194328624677,\n",
       "   0.021459129242481532,\n",
       "   0.0214674445108754,\n",
       "   0.021435229906897328,\n",
       "   0.021449586959300694,\n",
       "   0.021472013837568988,\n",
       "   0.02148141851069027,\n",
       "   0.021456415227834615,\n",
       "   0.02142603713941772,\n",
       "   0.021458517565272162,\n",
       "   0.021456342712972174,\n",
       "   0.021466906139959437,\n",
       "   0.02147297503047959,\n",
       "   0.02145807950823139,\n",
       "   0.0214694955042289,\n",
       "   0.02144360136688992,\n",
       "   0.021468412529878102,\n",
       "   0.02145717935443419,\n",
       "   0.021457219816342427,\n",
       "   0.0214572785800918,\n",
       "   0.021469429716529688,\n",
       "   0.021464698838989764,\n",
       "   0.02145946926101115,\n",
       "   0.021480726997882005,\n",
       "   0.021467379217820545,\n",
       "   0.021464269883405124,\n",
       "   0.02147409975281395,\n",
       "   0.021458623320235258,\n",
       "   0.021441218763960844,\n",
       "   0.021449737726900092,\n",
       "   0.021478219052073373,\n",
       "   0.021471829038437966,\n",
       "   0.021472987396588462,\n",
       "   0.021460764437790235,\n",
       "   0.02145567810881682,\n",
       "   0.02147115948783906,\n",
       "   0.0214662738855449,\n",
       "   0.021471940135560096,\n",
       "   0.02146304563862654,\n",
       "   0.021450775391827975,\n",
       "   0.02144419662190671,\n",
       "   0.021470742601576682,\n",
       "   0.021466621026953226,\n",
       "   0.021437020420533495,\n",
       "   0.02147105017143661,\n",
       "   0.021457488408227185,\n",
       "   0.021467720918140965,\n",
       "   0.021452598947707055,\n",
       "   0.021467064426153034,\n",
       "   0.02145885491272226,\n",
       "   0.02147283920113971,\n",
       "   0.021441686895378398,\n",
       "   0.02147023420610863,\n",
       "   0.02147803929831477,\n",
       "   0.021438882855458873,\n",
       "   0.021456206685774554,\n",
       "   0.021421933668777655,\n",
       "   0.021460021086253566,\n",
       "   0.02147556924226373,\n",
       "   0.021472645696268042,\n",
       "   0.021475518491752908,\n",
       "   0.021456549276454816,\n",
       "   0.021457781336614204,\n",
       "   0.021459948373533384,\n",
       "   0.0214512930866099,\n",
       "   0.02145310882710817,\n",
       "   0.021474916212786282,\n",
       "   0.02147415238297332,\n",
       "   0.021451337011028623,\n",
       "   0.02147199424965253,\n",
       "   0.02146123484457182,\n",
       "   0.021452468163739597,\n",
       "   0.021483987891327792,\n",
       "   0.021470696105007313,\n",
       "   0.021463287915431613,\n",
       "   0.02146418539814929,\n",
       "   0.02147458658178812,\n",
       "   0.021465925161274636,\n",
       "   0.02146254021102462,\n",
       "   0.021466257661210054,\n",
       "   0.021457054901914477,\n",
       "   0.02146643781068414,\n",
       "   0.02145838856202438,\n",
       "   0.02146058033116131,\n",
       "   0.021461010671750144,\n",
       "   0.021477377464167804,\n",
       "   0.021471011984892405,\n",
       "   0.021469221669113983,\n",
       "   0.021446215067661648,\n",
       "   0.02145705025225754,\n",
       "   0.021463519408989745,\n",
       "   0.021444658026161037,\n",
       "   0.021447054083416572,\n",
       "   0.021444208592300097,\n",
       "   0.02145415618706541,\n",
       "   0.02143488573335513,\n",
       "   0.021455945711412867,\n",
       "   0.02148118731391875,\n",
       "   0.021452702130519502,\n",
       "   0.02146159039493418,\n",
       "   0.021469658835794915,\n",
       "   0.021454296764991096,\n",
       "   0.021456311847164424,\n",
       "   0.021464019098717146,\n",
       "   0.02146396557819794,\n",
       "   0.021456840127335543,\n",
       "   0.021466996165232045,\n",
       "   0.02147699193835753,\n",
       "   0.02146665604777356,\n",
       "   0.02147633049992605,\n",
       "   0.02146314872251012,\n",
       "   0.021460505738792577,\n",
       "   0.021451414670192355,\n",
       "   0.021463904934800018,\n",
       "   0.021467024162102537,\n",
       "   0.02146406421028232,\n",
       "   0.02145810720831527,\n",
       "   0.021477361437690703,\n",
       "   0.021487609578366103,\n",
       "   0.02147300470914089,\n",
       "   0.021448667613302524,\n",
       "   0.02144981014283366,\n",
       "   0.02147058847039567,\n",
       "   0.021446570618024012,\n",
       "   0.021464508499841967,\n",
       "   0.021466859841247812,\n",
       "   0.021457885508715364,\n",
       "   0.021466290307737484,\n",
       "   0.02145532859311559,\n",
       "   0.021466538520274817,\n",
       "   0.02146078283856024,\n",
       "   0.021462223836495174,\n",
       "   0.021465536568669362,\n",
       "   0.02145311070675672,\n",
       "   0.021465930800220284,\n",
       "   0.021475088250092946,\n",
       "   0.021463825395987735,\n",
       "   0.02147188918719153,\n",
       "   0.02143727743774034,\n",
       "   0.021464057680976836,\n",
       "   0.021438425705145998,\n",
       "   0.021449937464290636,\n",
       "   0.02147213730079999,\n",
       "   0.021455694827796017,\n",
       "   0.021449330733524813,\n",
       "   0.02146293394793119,\n",
       "   0.021441346184346687,\n",
       "   0.021468453684288437,\n",
       "   0.02145162271760806,\n",
       "   0.021447998458419104,\n",
       "   0.02146921266658672,\n",
       "   0.021459237371737532,\n",
       "   0.02145168118457082,\n",
       "   0.02143087495906719,\n",
       "   0.021462192080327583,\n",
       "   0.02146032341288333,\n",
       "   0.021463284452921127,\n",
       "   0.021454993521029524,\n",
       "   0.02146613024082421,\n",
       "   0.02143483913785689,\n",
       "   0.021471190353646814,\n",
       "   0.02147125465741296,\n",
       "   0.0214441202488183,\n",
       "   0.0214513814300917,\n",
       "   0.021466963221918003,\n",
       "   0.021446822392000697,\n",
       "   0.02145768062702353,\n",
       "   0.021439471680099045,\n",
       "   0.021473110464103985,\n",
       "   0.021448836583814187,\n",
       "   0.021457615432897544,\n",
       "   0.02146971987490832,\n",
       "   0.021464630479139907,\n",
       "   0.02147929401318562,\n",
       "   0.021467609425303354,\n",
       "   0.021477737070613876,\n",
       "   0.02145492169866918,\n",
       "   0.02146339208753277,\n",
       "   0.021461423996573168,\n",
       "   0.021453916581339855,\n",
       "   0.021464403437380968,\n",
       "   0.021460240213702825,\n",
       "   0.021473339880155824,\n",
       "   0.021454862440275453,\n",
       "   0.021462423277099102,\n",
       "   0.021463096586995106,\n",
       "   0.02145483117875222,\n",
       "   0.021473305156122106,\n",
       "   0.02146440274487887,\n",
       "   0.02145409485116539,\n",
       "   0.02147283801399326,\n",
       "   0.021457149181128538,\n",
       "   0.021469939299144192,\n",
       "   0.021467266241049866,\n",
       "   0.021465215445554107,\n",
       "   0.02147468264172186,\n",
       "   0.021455853509705098,\n",
       "   0.021465578217724053,\n",
       "   0.021473554555805886,\n",
       "   0.0214564476765043,\n",
       "   0.02146277417780453,\n",
       "   0.021470115392534565,\n",
       "   0.021456236859080206,\n",
       "   0.021463203034460296,\n",
       "   0.021457492563239765,\n",
       "   0.02146410655183911,\n",
       "   0.021455428313417554,\n",
       "   0.021473049622848323,\n",
       "   0.021460948346561416,\n",
       "   0.021478569655992184,\n",
       "   0.021474630802993458,\n",
       "   0.021461606421411285,\n",
       "   0.021476062303756777,\n",
       "   0.021461433889460268,\n",
       "   0.021478263767923063,\n",
       "   0.021451156465839054,\n",
       "   0.021458959876254385,\n",
       "   0.021457240294618724,\n",
       "   0.021455074444845998,\n",
       "   0.02146657769610773,\n",
       "   0.02146148206782044,\n",
       "   0.02146254990605398,\n",
       "   0.021478177403018682,\n",
       "   0.02146794083702119,\n",
       "   0.02146607761066484,\n",
       "   0.02145998972580146,\n",
       "   0.02143942350173887,\n",
       "   0.02147211622895047,\n",
       "   0.02145326958652354,\n",
       "   0.021456346867984755,\n",
       "   0.021471189364358104,\n",
       "   0.021464626324127323,\n",
       "   0.021465304283680264,\n",
       "   0.02145310615602865,\n",
       "   0.021438852187508864,\n",
       "   0.021475379298831417,\n",
       "   0.02146602171585273,\n",
       "   0.021469368677416282,\n",
       "   0.02147688974483379,\n",
       "   0.021459600143907475,\n",
       "   0.021474357363594022,\n",
       "   0.021470513976955808,\n",
       "   0.02144952028124164,\n",
       "   0.02147951561385665,\n",
       "   0.021465696437724894,\n",
       "   0.021453450725286333,\n",
       "   0.021459054946899415,\n",
       "   0.021461414103686068,\n",
       "   0.021469273408913515,\n",
       "   0.021462254999089537,\n",
       "   0.021467078968697068,\n",
       "   0.021432158066524015,\n",
       "   0.021467626737855777,\n",
       "   0.021450107127304392,\n",
       "   0.021463290883297743,\n",
       "   0.02145574745795539,\n",
       "   0.021469865102490944,\n",
       "   0.02145110462711065,\n",
       "   0.021462267563056154,\n",
       "   0.02147991132934064,\n",
       "   0.021454457623335334,\n",
       "   0.021476626692965812,\n",
       "   0.021458380746643572,\n",
       "   0.021466688397514373,\n",
       "   0.021478793729884992,\n",
       "   0.02147077811704137,\n",
       "   0.021472968797960717,\n",
       "   0.02148165475283421,\n",
       "   0.021467055918270125,\n",
       "   0.021466161106631965,\n",
       "   0.021470599946144704,\n",
       "   0.02144406356257522,\n",
       "   0.021457286098685996,\n",
       "   0.021473250745243058,\n",
       "   0.021454126607332983,\n",
       "   0.021458041618473797,\n",
       "   0.021471107055537432,\n",
       "   0.02147030958990833,\n",
       "   0.02146720797194485,\n",
       "   0.021455404966204,\n",
       "   0.02144604055713321,\n",
       "   0.021447491546884117,\n",
       "   0.021478903046287443,\n",
       "   0.021462530021350908,\n",
       "   0.02146302070855105,\n",
       "   0.021460422638540942,\n",
       "   0.02145029835681203,\n",
       "   0.02145902764253102,\n",
       "   0.021470252606878636,\n",
       "   0.021464339529330304,\n",
       "   0.021469415668630008,\n",
       "   0.021464361095824183,\n",
       "   0.0214397273123017,\n",
       "   0.021452813326570504,\n",
       "   0.021447003629692363,\n",
       "   0.021465473254191923,\n",
       "   0.02147056729961728,\n",
       "   0.02146785447211681,\n",
       "   0.021441738931964542,\n",
       "   0.021461504030029804,\n",
       "   0.02146659352472709,\n",
       "   0.02146426473910383,\n",
       "   0.021472380270107158,\n",
       "   0.02148030358231414,\n",
       "   0.021463023280701696,\n",
       "   0.021479522044233267,\n",
       "   0.021472596132903674,\n",
       "   0.021474566202440697,\n",
       "   0.021443827914004503,\n",
       "   0.02146562560465326,\n",
       "   0.021448805718006433,\n",
       "   0.021466049910580962,\n",
       "   0.021457123855337563,\n",
       "   0.02147158418948225,\n",
       "   0.021443209509631904,\n",
       "   0.021474195614889943,\n",
       "   0.021454917049012243,\n",
       "   0.021439795276436073,\n",
       "   0.021471114178416146,\n",
       "   0.021476714442874387,\n",
       "   0.021459098871318135,\n",
       "   0.02146408993178878,\n",
       "   0.02147099447448224,\n",
       "   0.021453804593857886,\n",
       "   0.021475171449273453,\n",
       "   0.021464316478903362,\n",
       "   0.0214613775000038,\n",
       "   0.021472209321018076,\n",
       "   0.021471483380962705,\n",
       "   0.021452123099837562,\n",
       "   0.021469484819910834,\n",
       "   0.02146291841609844,\n",
       "   0.021465727402461516,\n",
       "   0.021468546578498303,\n",
       "   0.02145349910150425,\n",
       "   0.021445454898216913,\n",
       "   0.021463810457728216,\n",
       "   0.021470280208033644,\n",
       "   0.02145879149931595,\n",
       "   0.021460100328279233,\n",
       "   0.0214457356583528,\n",
       "   0.021464626225198454,\n",
       "   0.021468745623386746,\n",
       "   0.021455279920111058,\n",
       "   0.021464398095221934,\n",
       "   0.021455321173450264,\n",
       "   0.02144942788167613,\n",
       "   0.0214607553363341,\n",
       "   0.02147071163684006,\n",
       "   0.021463115482409464,\n",
       "   0.021471091325846944,\n",
       "   0.021461671120892915,\n",
       "   0.021456352408001533,\n",
       "   0.021462862422357457,\n",
       "   0.021448610135628474,\n",
       "   0.02145259152804173,\n",
       "   0.021459895842302883,\n",
       "   0.02146729967900826,\n",
       "   0.021467334403041983,\n",
       "   0.02146038494664109,\n",
       "   0.021459203340205908,\n",
       "   0.02147253618200785,\n",
       "   0.02147182567485635,\n",
       "   0.021466588083639184,\n",
       "   0.02145569700423118,\n",
       "   0.021448867350693065,\n",
       "   0.02144828495642951,\n",
       "   0.02145215287742773,\n",
       "   0.02146900372881118,\n",
       "   0.021435785887152326,\n",
       "   0.02145612279409195,\n",
       "   0.021449713093611215,\n",
       "   0.021463523761860066,\n",
       "   0.021468877297714045,\n",
       "   0.021461034711465796,\n",
       "   0.021450207144392972,\n",
       "   0.021472223171060015,\n",
       "   0.021475978807789658,\n",
       "   0.021438536604410384,\n",
       "   0.021464028496959893,\n",
       "   0.02147710788299434,\n",
       "   0.02147965025605008,\n",
       "   0.021438232991705296,\n",
       "   0.0214778403523552,\n",
       "   0.02148181383045877,\n",
       "   0.021466540103136752,\n",
       "   0.02145222430407259,\n",
       "   0.021462305452813746,\n",
       "   0.021452561354736076,\n",
       "   0.02146367878340092,\n",
       "   0.021482087170929334,\n",
       "   0.02146605159237177,\n",
       "   0.021461593659586928,\n",
       "   0.021468644518080587,\n",
       "   0.021465639157908587,\n",
       "   0.02147233703819053,\n",
       "   0.02146911739808395,\n",
       "   0.021472153426205964,\n",
       "   0.02146324992674515,\n",
       "   0.021464549951038914,\n",
       "   0.021451781399517138,\n",
       "   0.02146184236676861,\n",
       "   0.02147527354386832,\n",
       "   0.021464972179460327,\n",
       "   0.021460388310222706,\n",
       "   0.021474971711882913,\n",
       "   0.021474173652680582,\n",
       "   0.021481832725873132,\n",
       "   0.021434620307194247,\n",
       "   0.02146248253549283,\n",
       "   0.021461063104051773,\n",
       "   0.02147479205705318,\n",
       "   0.021468271853023545,\n",
       "   0.021475569637979215,\n",
       "   0.02147047945077983,\n",
       "   0.021444307817957708,\n",
       "   0.021468477625075217,\n",
       "   0.02147632169525653,\n",
       "   0.021455874086910263,\n",
       "   0.02145605245566467,\n",
       "   0.02144674413926374,\n",
       "   0.021445575591439527,\n",
       "   0.021467142777818863,\n",
       "   0.021466794647121826,\n",
       "   0.021470479846495315,\n",
       "   0.02146646185039979,\n",
       "   0.021460220328999753,\n",
       "   0.02147542510289869,\n",
       "   0.02144152722418061,\n",
       "   0.021482482292840097,\n",
       "   0.021471013666683213,\n",
       "   0.021465689018059567,\n",
       "   0.021458201091813844,\n",
       "   0.021449831511469796,\n",
       "   0.021474371411493706,\n",
       "   0.02145650723168464,\n",
       "   0.021474967161154847,\n",
       "   0.02144356476320765,\n",
       "   0.021458027471645245,\n",
       "   0.021467390495711836,\n",
       "   0.021464422134937586,\n",
       "   0.021465071207260194,\n",
       "   0.02145020190116281,\n",
       "   0.02147745146296331,\n",
       "   0.021466336408591368,\n",
       "   0.021485708857967648,\n",
       "   0.021458512915615225,\n",
       "   0.021470344808586405,\n",
       "   0.02145625001662005,\n",
       "   0.021474416325201136,\n",
       "   0.02146201618479495,\n",
       "   0.021446510073554963,\n",
       "   0.02145323179569482,\n",
       "   0.021461235240287306,\n",
       "   0.02146774525464323,\n",
       "   0.021444887442212877,\n",
       "   0.021442723373159826,\n",
       "   0.02147106214183,\n",
       "   0.021469499857099224,\n",
       "   0.021472331300316016,\n",
       "   0.021464656002788622,\n",
       "   0.021463233603481437,\n",
       "   0.021469901112599988,\n",
       "   0.021467637125387233,\n",
       "   0.021460892649607044,\n",
       "   0.02147779603222099,\n",
       "   0.021454097917960394,\n",
       "   0.021463689566647858,\n",
       "   0.021457246131422116,\n",
       "   0.021449993062316135,\n",
       "   0.021455465906388534,\n",
       "   0.021463235779916597,\n",
       "   0.02146036595229786,\n",
       "   0.02145747891105557,\n",
       "   0.021462609362305447,\n",
       "   0.02147245575283573,\n",
       "   0.021458664771432205,\n",
       "   0.02148162299666662,\n",
       "   0.021469928120181767,\n",
       "   0.021472578523564635,\n",
       "   0.021457168670116124,\n",
       "   0.02145251416566461,\n",
       "   0.021472241769687764,\n",
       "   0.021468391853744065,\n",
       "   0.0214735282407262,\n",
       "   0.021448779798642233,\n",
       "   0.021456938165846703,\n",
       "   0.021457088240944003,\n",
       "   0.021461694962750827,\n",
       "   0.0214631535700248,\n",
       "   0.021469089104426846,\n",
       "   0.021464533726704073,\n",
       "   0.021466729255138095,\n",
       "   0.021473475115922475,\n",
       "   0.021460639193839552,\n",
       "   0.02146480063679802,\n",
       "   0.02147094382290029,\n",
       "   0.021459892874436753,\n",
       "   0.021452547702551877,\n",
       "   0.021446563989789655,\n",
       "   0.021471883746103627,\n",
       "   0.02146151847364497,\n",
       "   0.021478425516627143,\n",
       "   0.021448143488143983,\n",
       "   0.021465800016252826,\n",
       "   0.021469287555742067,\n",
       "   0.021474499623310515,\n",
       "   0.02143403781400182,\n",
       "   0.021468267203366607,\n",
       "   0.02145958154527973,\n",
       "   0.021467593992399475,\n",
       "   0.02145605522567306,\n",
       "   0.02144912001502959,\n",
       "   0.021467155242856608,\n",
       "   0.021461779942651013,\n",
       "   0.021475382464555288,\n",
       "   0.021458747080252874,\n",
       "   0.021466320184256527,\n",
       "   0.021477350357657153,\n",
       "   0.021475489010949353,\n",
       "   0.02145171788718196,\n",
       "   0.0214464980042327,\n",
       "   0.021462184067089034,\n",
       "   0.02145011286517891,\n",
       "   0.02145904940688264,\n",
       "   0.021469627772129424,\n",
       "   0.021474479441820835,\n",
       "   0.021462940576165542,\n",
       "   0.02146417303204042,\n",
       "   0.021466181782766002,\n",
       "   0.02145465340357104,\n",
       "   0.021479877297809015,\n",
       "   0.021468655103469785,\n",
       "   0.02145595916573932,\n",
       "   0.021478279101898066,\n",
       "   0.021466975390169136,\n",
       "   0.021457530452997357,\n",
       "   0.021467800061237764,\n",
       "   0.021471221714098918,\n",
       "   0.021458594729791537,\n",
       "   0.021472977701559107,\n",
       "   0.02146652813274336,\n",
       "   0.02147131302544685,\n",
       "   0.021471674709399213,\n",
       "   0.02147123872986473,\n",
       "   0.02144294487490199,\n",
       "   0.021468720792240128,\n",
       "   0.021460043938822768,\n",
       "   0.0214653774910448,\n",
       "   0.021459176233695254,\n",
       "   0.021454821780509475,\n",
       "   0.021443408356662606,\n",
       "   0.02147275946446969,\n",
       "   0.02146498840379517,\n",
       "   0.021476414094822042,\n",
       "   0.021445705880762632,\n",
       "   0.021477594217324158,\n",
       "   0.02146802838907202,\n",
       "   0.021473625883521877,\n",
       "   0.021457829218187768,\n",
       "   0.02145841814175681,\n",
       "   0.02145057535765082,\n",
       "   0.021449696176774273,\n",
       "   0.02144899388071907,\n",
       "   0.02147861605363268,\n",
       "   0.02146881526931193,\n",
       "   0.021474203529199622,\n",
       "   0.021462633599878843,\n",
       "   0.021474557892415532,\n",
       "   0.021461241373877307,\n",
       "   0.021474197692396235,\n",
       "   0.02146059526942083,\n",
       "   0.02145868129255366,\n",
       "   0.021472369091144736,\n",
       "   0.021460652054592782,\n",
       "   0.02146705433540819,\n",
       "   0.02145290246148327,\n",
       "   0.0214523723016636,\n",
       "   0.021469824343796094,\n",
       "   0.02146963529072362,\n",
       "   0.021464755426303973,\n",
       "   0.021470565914613084,\n",
       "   0.02146404521593909,\n",
       "   0.02145357042922024,\n",
       "   0.021467854768903425,\n",
       "   0.021462483722639283,\n",
       "   0.021447395783736993,\n",
       "   0.021470996947704015,\n",
       "   0.021454106030127814,\n",
       "   0.02146769836235838,\n",
       "   0.02146689931386734,\n",
       "   0.02145547955857273,\n",
       "   0.021472234745737923,\n",
       "   0.021460442820030623,\n",
       "   0.021469303582219167,\n",
       "   0.02145825441447531,\n",
       "   0.02147492016994112,\n",
       "   0.02146555922338082,\n",
       "   0.021464763142755913,\n",
       "   0.021462168337398543,\n",
       "   0.021476000176425793,\n",
       "   0.021478942815693583,\n",
       "   0.021457495234319283,\n",
       "   0.021465017884598727,\n",
       "   0.02146005709636261,\n",
       "   0.021470926213561253,\n",
       "   0.021462451570756207,\n",
       "   0.02147042573240288,\n",
       "   0.02144504177125163,\n",
       "   0.02146170614171325,\n",
       "   0.02146699893524043,\n",
       "   0.021461220598814397,\n",
       "   0.0214629761905591,\n",
       "   0.021465076450490356,\n",
       "   0.02146376109222159,\n",
       "   0.021446726628853573,\n",
       "   0.021465000670975176,\n",
       "   0.02147104997357887,\n",
       "   0.02146466371924056,\n",
       "   0.02145651672885626,\n",
       "   0.021459730532159448,\n",
       "   0.021440357489209947,\n",
       "   0.021445812526085566,\n",
       "   0.021459641595104422,\n",
       "   0.02146181842598183,\n",
       "   0.021449749202649127,\n",
       "   0.021468537674899913,\n",
       "   0.021457457542419435,\n",
       "   0.02145670370442244,\n",
       "   0.021443209707489648,\n",
       "   0.021476752827276333,\n",
       "   0.021479450419730664,\n",
       "   0.021468326758546947,\n",
       "   0.021456352408001533,\n",
       "   0.021456811240105213,\n",
       "   0.021461705548140024,\n",
       "   0.021474501107243582,\n",
       "   0.0214474919425996,\n",
       "   0.021452290586416158,\n",
       "   0.02144704201409431,\n",
       "   0.021443191306719643,\n",
       "   0.021463162077907705,\n",
       "   0.02144266995156949,\n",
       "   0.02147500623805889,\n",
       "   0.021468050746996866,\n",
       "   0.02146641189131994,\n",
       "   0.02147939442598968,\n",
       "   0.02147005316627471,\n",
       "   0.021455158732244087,\n",
       "   0.021472860174060363,\n",
       "   0.02146552647792452,\n",
       "   0.02145524539393508,\n",
       "   0.021449213107097197,\n",
       "   0.021456045926359184,\n",
       "   0.021457412134067644,\n",
       "   0.021450576050152916,\n",
       "   0.021447610459387055,\n",
       "   0.021472160549084674,\n",
       "   0.02146844923248924,\n",
       "   0.02146970374950235,\n",
       "   0.021454249081275276,\n",
       "   0.02146038089055738,\n",
       "   0.02146753859223172,\n",
       "   0.021465421712250136,\n",
       "   0.021461026599298376,\n",
       "   0.021450179543237965,\n",
       "   0.02146899383592408,\n",
       "   0.021441843598710056,\n",
       "   0.021460686086124406,\n",
       "   0.021472994321609432,\n",
       "   0.021463435913022622,\n",
       "   0.021453825467849668,\n",
       "   0.021479259091294157,\n",
       "   0.021469876281453366,\n",
       "   0.021456491501994152,\n",
       "   0.021478685204913507,\n",
       "   0.02145385860902145,\n",
       "   0.021466645462384363,\n",
       "   0.021475489010949353,\n",
       "   0.021441884060618293,\n",
       "   0.021451214141370845,\n",
       "   0.02146792777841022,\n",
       "   0.021474577282474247,\n",
       "   0.021461675968407597,\n",
       "   0.021452573918702693,\n",
       "   0.02147110458231566,\n",
       "   0.02146249074658912,\n",
       "   0.021465754212185554,\n",
       "   0.02147365625468527,\n",
       "   0.02147323808234757,\n",
       "   0.02146733806341021,\n",
       "   0.021453686472785918,\n",
       "   0.021470361923281087,\n",
       "   0.02147678715559457,\n",
       "   0.021468982558032784,\n",
       "   0.021470024377973247,\n",
       "   0.02146213717480418,\n",
       "   0.021461984824342847,\n",
       "   0.021447546056692038,\n",
       "   0.02147424409003673,\n",
       "   0.02147830027267646,\n",
       "   0.02146385853715952,\n",
       "   0.021462928111127797,\n",
       "   0.021465093169469558,\n",
       "   0.021465325751245268,\n",
       "   0.021483117416191893,\n",
       "   0.02146093706867012,\n",
       "   0.021465272329654932,\n",
       "   0.02148085461612559,\n",
       "   0.02146743343084185,\n",
       "   0.02145819495822384,\n",
       "   0.021477966585594588,\n",
       "   0.02145266315254433,\n",
       "   0.021469551399041012,\n",
       "   0.02146900610310408,\n",
       "   0.021477041204935286,\n",
       "   0.021481022894135154,\n",
       "   0.021455300398387354,\n",
       "   0.021460476554775634,\n",
       "   0.02144683060309699,\n",
       "   0.021467056412914482,\n",
       "   0.021466864787691362,\n",
       "   0.021465256698893313,\n",
       "   0.021472781030963565,\n",
       "   0.021461407772238324,\n",
       "   0.02145783228498277,\n",
       "   0.021465948805274804,\n",
       "   0.02145298852960104,\n",
       "   0.021455188707692,\n",
       "   0.021460842888384932,\n",
       "   0.021457530057281873,\n",
       "   0.021474575699612312,\n",
       "   0.02147852434656927,\n",
       "   0.021468825656843382,\n",
       "   0.021459191171954776,\n",
       "   0.021466397348775902,\n",
       "   0.021459937392428702,\n",
       "   0.021471584486268864,\n",
       "   0.021450024818483723,\n",
       "   0.02147712479983128,\n",
       "   0.021458617087716382,\n",
       "   0.02145272389487112,\n",
       "   0.021454808820827374,\n",
       "   0.021464212603588818,\n",
       "   0.021472050243393513,\n",
       "   0.02146481448683996,\n",
       "   0.021454090201508454,\n",
       "   0.021452941538387314,\n",
       "   0.021461509471117707,\n",
       "   0.021459288616892707,\n",
       "   0.021455811563863794,\n",
       "   0.02145586181973026,\n",
       "   0.0214616653830184,\n",
       "   0.021472633429088037,\n",
       "   0.021477851036673264,\n",
       "   0.021464416100276457,\n",
       "   0.021478278211538228,\n",
       "   0.02147548000842209,\n",
       "   0.021460704981538764,\n",
       "   0.02144770226537934,\n",
       "   0.021467386538556996,\n",
       "   0.021458887163534203,\n",
       "   0.0214813133493004,\n",
       "   0.02143859744566605,\n",
       "   0.021464138308006697,\n",
       "   0.02145587072332865,\n",
       "   0.02145995114354177,\n",
       "   0.021467555706926402,\n",
       "   0.02146106191690532,\n",
       "   0.021478064228390263,\n",
       "   0.02146032529253188,\n",
       "   0.02144929868057061,\n",
       "   0.021458915457191308,\n",
       "   0.02146054066068404,\n",
       "   0.021458243631228373,\n",
       "   0.02146331126264517,\n",
       "   0.02147106679148694,\n",
       "   0.021466929586101864,\n",
       "   0.021464419266000328,\n",
       "   0.021464740883759936,\n",
       "   0.021472888467717468,\n",
       "   0.021455664753419236,\n",
       "   0.02147039862589223,\n",
       "   0.02147222851321905,\n",
       "   0.02147405325624458,\n",
       "   0.021453359512867275,\n",
       "   0.021468329231768724,\n",
       "   0.021480159442949096,\n",
       "   0.021454784088609624,\n",
       "   0.021438123576373976,\n",
       "   0.02145102330757869,\n",
       "   0.02147308019186946,\n",
       "   0.02146605752810403,\n",
       "   0.021473096614062045,\n",
       "   0.021462860542708906,\n",
       "   0.021477549501474468,\n",
       "   0.021454432792188716,\n",
       "   0.02145106169198064,\n",
       "   0.021467675905504663,\n",
       "   0.021471182637194875,\n",
       "   0.021463087089823488,\n",
       "   0.021463666318363175,\n",
       "   0.021468998485581012,\n",
       "   0.021468643034147524,\n",
       "   0.021474133289701217,\n",
       "   0.021419342325930773,\n",
       "   0.0214544659333605,\n",
       "   0.021451015591126756,\n",
       "   0.021462329492529398,\n",
       "   0.021468878583789368,\n",
       "   0.021467094500529815,\n",
       "   0.021469423582939687,\n",
       "   ...],\n",
       "  'val': []},\n",
       " {'train': [0.0635546506193169,\n",
       "   0.05905575791829849,\n",
       "   0.05867568724382963,\n",
       "   0.058633545523362536,\n",
       "   0.05861075310291591,\n",
       "   0.05860234810603605,\n",
       "   0.05859608709564842,\n",
       "   0.05859740977465364,\n",
       "   0.058606621833263095,\n",
       "   0.05861502821514715,\n",
       "   0.05860084636577432,\n",
       "   0.05859729521502103,\n",
       "   0.05860285897472587,\n",
       "   0.058601737121328774,\n",
       "   0.058601344373710916,\n",
       "   0.05859545652302469,\n",
       "   0.05859941664573068,\n",
       "   0.058593999696470396,\n",
       "   0.058610033692166025,\n",
       "   0.05860844509235556,\n",
       "   0.058609307158537426,\n",
       "   0.05859686606157865,\n",
       "   0.058606643597614715,\n",
       "   0.05860206398231855,\n",
       "   0.05861313867371112,\n",
       "   0.05861261929713839,\n",
       "   0.058608770964056624,\n",
       "   0.05861393505112264,\n",
       "   0.058606941373516415,\n",
       "   0.058613223752540175,\n",
       "   0.05859794062697541,\n",
       "   0.058612219228784075,\n",
       "   0.0586068687597251,\n",
       "   0.058602278064395386,\n",
       "   0.058606914860578986,\n",
       "   0.0586187916672576,\n",
       "   0.05861252551256868,\n",
       "   0.05861025568855254,\n",
       "   0.05860599383279001,\n",
       "   0.058597790057233756,\n",
       "   0.05861254292404998,\n",
       "   0.058609298848512265,\n",
       "   0.05861530224811981,\n",
       "   0.05861266282584163,\n",
       "   0.058607999318862854,\n",
       "   0.05860361993560158,\n",
       "   0.05860482607639677,\n",
       "   0.05861351460342091,\n",
       "   0.05860448912466215,\n",
       "   0.05861721296033424,\n",
       "   0.05862001798954247,\n",
       "   0.05859536531060563,\n",
       "   0.05860391296291747,\n",
       "   0.0586037986011426,\n",
       "   0.05861117196775571,\n",
       "   0.05860210553244436,\n",
       "   0.05860180518439202,\n",
       "   0.05859327909857406,\n",
       "   0.05860917993600932,\n",
       "   0.058609492749099414,\n",
       "   0.0585969883376632,\n",
       "   0.058602252342888926,\n",
       "   0.05860095419824371,\n",
       "   0.05859824948291066,\n",
       "   0.05859713336738808,\n",
       "   0.05859750751637819,\n",
       "   0.05861116227272635,\n",
       "   0.058598825446797606,\n",
       "   0.05861277125188424,\n",
       "   0.058597620097433384,\n",
       "   0.05860889126156375,\n",
       "   0.05860295849717009,\n",
       "   0.058616843362072193,\n",
       "   0.05861377399492066,\n",
       "   0.05859814105686805,\n",
       "   0.05859987725855404,\n",
       "   0.058602484627878024,\n",
       "   0.05860165224035746,\n",
       "   0.05860503956490038,\n",
       "   0.058597905210439595,\n",
       "   0.05860234711674734,\n",
       "   0.05860856479628947,\n",
       "   0.05861238819929574,\n",
       "   0.058625535648393436,\n",
       "   0.05861418138401142,\n",
       "   0.05859629464841977,\n",
       "   0.05861626126459525,\n",
       "   0.05860475385832094,\n",
       "   0.058598349796785854,\n",
       "   0.05860736716337719,\n",
       "   0.05859116696717828,\n",
       "   0.058600652267329426,\n",
       "   0.05860204498797531,\n",
       "   0.058611154556274414,\n",
       "   0.05860554073856085,\n",
       "   0.05860195713913787,\n",
       "   0.058593462314843144,\n",
       "   0.05860434646923018,\n",
       "   0.05860344265506475,\n",
       "   0.05859990416720695,\n",
       "   0.05859808684384674,\n",
       "   0.058604600518570894,\n",
       "   0.05861069057986944,\n",
       "   0.05863495446834327,\n",
       "   0.05860733115326814,\n",
       "   0.05860071854967299,\n",
       "   0.05860685688826058,\n",
       "   0.058601938738367866,\n",
       "   0.05860837920572748,\n",
       "   0.058605228519043984,\n",
       "   0.05859975161888788,\n",
       "   0.05861657882627115,\n",
       "   0.05861237514068477,\n",
       "   0.058599202761511586,\n",
       "   0.05861221566734472,\n",
       "   0.05860583574445416,\n",
       "   0.05860363319207029,\n",
       "   0.05861130077314575,\n",
       "   0.05861243014513704,\n",
       "   0.05861553077381181,\n",
       "   0.058602740853653905,\n",
       "   0.05859786306674055,\n",
       "   0.05859493140857744,\n",
       "   0.058601263647752186,\n",
       "   0.058603036650978185,\n",
       "   0.05860805293831093,\n",
       "   0.05861428268717532,\n",
       "   0.05860115799171796,\n",
       "   0.05859515241567525,\n",
       "   0.058596262397607825,\n",
       "   0.058616708423092155,\n",
       "   0.058608982078267334,\n",
       "   0.058603521996019294,\n",
       "   0.05861199960669047,\n",
       "   0.05859507841687974,\n",
       "   0.05861359453794867,\n",
       "   0.05860337340485505,\n",
       "   0.05861482323452645,\n",
       "   0.058606551000191465,\n",
       "   0.05860536800875209,\n",
       "   0.05860801574105544,\n",
       "   0.058602590283912244,\n",
       "   0.05859662012440535,\n",
       "   0.05859989981433663,\n",
       "   0.05861083917103368,\n",
       "   0.05861780969928409,\n",
       "   0.058605618892368934,\n",
       "   0.05859887372408665,\n",
       "   0.05860167637900198,\n",
       "   0.05862465043285576,\n",
       "   0.0586132045603392,\n",
       "   0.05860507082642361,\n",
       "   0.05860520101681784,\n",
       "   0.05859808169954545,\n",
       "   0.058602046768694994,\n",
       "   0.058610985981478236,\n",
       "   0.058607706687262444,\n",
       "   0.05861373105979065,\n",
       "   0.0586034353343283,\n",
       "   0.058607585598324345,\n",
       "   0.058608297886195516,\n",
       "   0.05861899308643895,\n",
       "   0.05861090406837305,\n",
       "   0.058612815572018445,\n",
       "   0.058603447205792816,\n",
       "   0.05860944704396102,\n",
       "   0.05860711212474776,\n",
       "   0.058596500816186926,\n",
       "   0.05859929911823194,\n",
       "   0.05860220564846181,\n",
       "   0.05860911286223479,\n",
       "   0.058600416223043225,\n",
       "   0.05862116457515732,\n",
       "   0.0586085008882388,\n",
       "   0.05861804970072513,\n",
       "   0.058607195224999395,\n",
       "   0.05861002280999021,\n",
       "   0.058608009211749953,\n",
       "   0.05859979930260369,\n",
       "   0.058601467441226435,\n",
       "   0.058596083138493585,\n",
       "   0.05860675301294604,\n",
       "   0.05860323905944824,\n",
       "   0.058605868786697074,\n",
       "   0.05860310530761465,\n",
       "   0.05860123337551766,\n",
       "   0.05859708152865968,\n",
       "   0.05860259582392902,\n",
       "   0.05860278220592198,\n",
       "   0.05862057416765522,\n",
       "   0.058606493621446286,\n",
       "   0.05859854706095462,\n",
       "   0.058613864020193265,\n",
       "   0.05861731248277846,\n",
       "   0.05860848070674912,\n",
       "   0.05859959847699557,\n",
       "   0.05862333586601796,\n",
       "   0.05859722082051004,\n",
       "   0.05859647173109885,\n",
       "   0.058594412229862454,\n",
       "   0.05860900918477798,\n",
       "   0.05860770965512858,\n",
       "   0.05860573780487187,\n",
       "   0.05861035620028547,\n",
       "   0.05859824156860098,\n",
       "   0.05861087142184562,\n",
       "   0.05859989130645372,\n",
       "   0.058614660199747044,\n",
       "   0.05860619030552781,\n",
       "   0.05861517482773397,\n",
       "   0.058598672898478525,\n",
       "   0.05859729046643522,\n",
       "   0.058611573816829694,\n",
       "   0.058597967733486064,\n",
       "   0.05859559680416376,\n",
       "   0.05859536808061402,\n",
       "   0.05859805657161222,\n",
       "   0.05861433848305857,\n",
       "   0.05860124682984411,\n",
       "   0.05860607119516713,\n",
       "   0.05860453621480475,\n",
       "   0.05860714516699067,\n",
       "   0.058598056373754474,\n",
       "   0.058616584168430186,\n",
       "   0.05860113543593537,\n",
       "   0.058618736464947585,\n",
       "   0.05860088573946498,\n",
       "   0.05860967695465721,\n",
       "   0.05860641764407336,\n",
       "   0.058598301123781324,\n",
       "   0.05861041397474613,\n",
       "   0.05860762972060081,\n",
       "   0.058592962328329126,\n",
       "   0.058605655693908944,\n",
       "   0.05860058103854231,\n",
       "   0.058603823729075834,\n",
       "   0.058615115668269116,\n",
       "   0.058600267829736735,\n",
       "   0.058604932326004215,\n",
       "   0.05859779322295763,\n",
       "   0.058604937866020994,\n",
       "   0.05860915441236061,\n",
       "   0.05861400271847041,\n",
       "   0.05860117085247119,\n",
       "   0.058598456046393306,\n",
       "   0.058598137297570956,\n",
       "   0.05860752762600594,\n",
       "   0.05859570542806412,\n",
       "   0.05860212175677921,\n",
       "   0.05860944170180198,\n",
       "   0.058610718675668805,\n",
       "   0.05860951866846362,\n",
       "   0.05861134291684479,\n",
       "   0.05859906069965284,\n",
       "   0.058604248925363374,\n",
       "   0.05860365574785288,\n",
       "   0.05862743943558689,\n",
       "   0.05859611815931391,\n",
       "   0.058597524730001745,\n",
       "   0.058599054961778314,\n",
       "   0.05860205013227661,\n",
       "   0.05860169774763812,\n",
       "   0.0585959814396142,\n",
       "   0.05860730859748555,\n",
       "   0.05861078218800398,\n",
       "   0.05861644883373466,\n",
       "   0.058610695724170735,\n",
       "   0.058601730789881026,\n",
       "   0.05860393235297619,\n",
       "   0.05860545328543888,\n",
       "   0.058601450425460624,\n",
       "   0.05860187898532978,\n",
       "   0.05861887733965988,\n",
       "   0.05861607844404165,\n",
       "   0.0586118702077272,\n",
       "   0.05860112791734118,\n",
       "   0.058611439669280625,\n",
       "   0.05859997064740826,\n",
       "   0.058599000353041525,\n",
       "   0.058612438653019946,\n",
       "   0.05860400002032395,\n",
       "   0.05862514923222332,\n",
       "   0.05860540302957242,\n",
       "   0.058616182715071685,\n",
       "   0.05860851651900042,\n",
       "   0.05861324116402147,\n",
       "   0.058598294396618095,\n",
       "   0.058603844701996484,\n",
       "   0.058603195530745,\n",
       "   0.05860251767012094,\n",
       "   0.05859352226573897,\n",
       "   0.05860506113139426,\n",
       "   0.058602016298602726,\n",
       "   0.05860215598616857,\n",
       "   0.058599362630567114,\n",
       "   0.05861990164919018,\n",
       "   0.058607519909554005,\n",
       "   0.05860671245210893,\n",
       "   0.058596017845438726,\n",
       "   0.058599005101627334,\n",
       "   0.058594861169079034,\n",
       "   0.058606479375688865,\n",
       "   0.05861901584007928,\n",
       "   0.0585888423365676,\n",
       "   0.0586265484821747,\n",
       "   0.058588775658508556,\n",
       "   0.058600713998944925,\n",
       "   0.05859186481143429,\n",
       "   0.05861326371980406,\n",
       "   0.058604956860364224,\n",
       "   0.05859117923435828,\n",
       "   0.058602744612951,\n",
       "   0.058593239329167913,\n",
       "   0.05860175354352136,\n",
       "   0.058612596147782574,\n",
       "   0.058600233006774145,\n",
       "   0.058612104471293724,\n",
       "   0.058606719179272154,\n",
       "   0.05860810022631127,\n",
       "   0.05860541648389887,\n",
       "   0.05859775088140084,\n",
       "   0.05859799503785446,\n",
       "   0.05860384153627261,\n",
       "   0.05860756719755434,\n",
       "   0.0586001352650496,\n",
       "   0.05859498384087907,\n",
       "   0.058591520736820966,\n",
       "   0.058612303516182164,\n",
       "   0.05861258368274483,\n",
       "   0.05862966889662367,\n",
       "   0.05860193517692851,\n",
       "   0.058613916254637154,\n",
       "   0.05861092523915144,\n",
       "   0.05859423534504111,\n",
       "   0.0585891254709964,\n",
       "   0.05859475056660126,\n",
       "   0.0585995001416978,\n",
       "   0.05860171674198135,\n",
       "   0.05859270669612647,\n",
       "   0.058604221225279494,\n",
       "   0.05860777930105376,\n",
       "   0.058613281131285354,\n",
       "   0.05859727503353135,\n",
       "   0.05860435438353986,\n",
       "   0.05861602185672744,\n",
       "   0.058596203040285226,\n",
       "   0.05860484942361032,\n",
       "   0.05860947790976877,\n",
       "   0.05860006106839635,\n",
       "   0.05860425644395757,\n",
       "   0.05860824307960099,\n",
       "   0.058603741815970646,\n",
       "   0.058598713657173375,\n",
       "   0.05861498706073682,\n",
       "   0.058606835321766707,\n",
       "   0.05860651162650081,\n",
       "   0.05860629655513526,\n",
       "   0.058622595284489674,\n",
       "   0.05860056916707779,\n",
       "   0.058610274880753516,\n",
       "   0.05859886482048826,\n",
       "   0.05860299529871011,\n",
       "   0.05860108478435342,\n",
       "   0.05860948760479812,\n",
       "   0.05861731762707975,\n",
       "   0.05861326787481664,\n",
       "   0.058607922945774445,\n",
       "   0.05860197811205852,\n",
       "   0.05860812100137418,\n",
       "   0.05859016679629251,\n",
       "   0.05860671185853571,\n",
       "   0.058612644622929366,\n",
       "   0.05861075725792849,\n",
       "   0.058603977860256846,\n",
       "   0.0586005046654539,\n",
       "   0.058619330038173566,\n",
       "   0.0586064932257308,\n",
       "   0.05859391976194263,\n",
       "   0.05859345064123637,\n",
       "   0.058600479537520664,\n",
       "   0.05861584200403997,\n",
       "   0.05859647568825369,\n",
       "   0.05860022845604608,\n",
       "   0.05860417156298626,\n",
       "   0.058601766799990074,\n",
       "   0.05860096290398435,\n",
       "   0.058607300089602646,\n",
       "   0.058615706669344445,\n",
       "   0.058599608567740415,\n",
       "   0.05861514376406848,\n",
       "   0.05860607831804584,\n",
       "   0.05860278141449101,\n",
       "   0.058603815023335185,\n",
       "   0.05860059904359683,\n",
       "   0.058618742004964364,\n",
       "   0.05860085507151497,\n",
       "   0.058596704807518926,\n",
       "   0.05860112079446247,\n",
       "   0.05860727021308361,\n",
       "   0.05860226658864635,\n",
       "   0.058614573538056054,\n",
       "   0.058614161796094966,\n",
       "   0.058601434596841265,\n",
       "   0.05859828747159713,\n",
       "   0.05859879359170114,\n",
       "   0.058608693008106276,\n",
       "   0.05860430254481146,\n",
       "   0.05860403583257525,\n",
       "   0.0586026852556284,\n",
       "   0.05859861769616851,\n",
       "   0.05860682345030219,\n",
       "   0.0586051428466417,\n",
       "   0.05859647450110724,\n",
       "   0.05860268050704259,\n",
       "   0.058623909851327476,\n",
       "   0.05861166483139101,\n",
       "   0.05860980714505144,\n",
       "   0.05860553915569891,\n",
       "   0.05860667208912956,\n",
       "   0.05859367046118772,\n",
       "   0.05861638037495593,\n",
       "   0.05861050103215261,\n",
       "   0.058598661027014005,\n",
       "   0.05861668923089118,\n",
       "   0.05859853499163236,\n",
       "   0.05860848169603783,\n",
       "   0.05859778788079859,\n",
       "   0.05859872295648725,\n",
       "   0.05860712320478131,\n",
       "   0.05859379788157356,\n",
       "   0.05860598473133388,\n",
       "   0.058604261192543375,\n",
       "   0.05861203898038112,\n",
       "   0.05860714932200325,\n",
       "   0.05860629180654945,\n",
       "   0.058604774039810625,\n",
       "   0.05860186572886107,\n",
       "   0.0586108991219295,\n",
       "   0.058596844495084774,\n",
       "   0.05861469680342932,\n",
       "   0.05860179133435008,\n",
       "   0.058619052839477034,\n",
       "   0.058610646259735236,\n",
       "   0.05860451088901377,\n",
       "   0.05860364348067287,\n",
       "   0.05861513505832783,\n",
       "   0.0586025024350748,\n",
       "   0.05860936790086422,\n",
       "   0.05862318114126372,\n",
       "   0.05860087307656949,\n",
       "   0.0586177032518189,\n",
       "   0.05860238233542541,\n",
       "   0.05859338772247441,\n",
       "   0.058624958299502296,\n",
       "   0.05859735754020976,\n",
       "   0.05862330460449472,\n",
       "   0.058609213769683206,\n",
       "   0.058597159286752284,\n",
       "   0.05860085705009239,\n",
       "   0.058603029923814956,\n",
       "   0.05860129946000348,\n",
       "   0.058606889534788015,\n",
       "   0.058599222547285786,\n",
       "   0.05861243469586511,\n",
       "   0.058608840609981804,\n",
       "   0.05860153629572065,\n",
       "   0.05860387715066617,\n",
       "   0.05859218158167922,\n",
       "   0.0586227367527752,\n",
       "   0.058613090396422075,\n",
       "   0.05860326656167438,\n",
       "   0.058601788960057175,\n",
       "   0.05860458093065444,\n",
       "   0.05860776604458504,\n",
       "   0.058614230254873695,\n",
       "   0.05861486557608323,\n",
       "   0.05858994855920309,\n",
       "   0.05861686987500962,\n",
       "   0.05859832783457649,\n",
       "   0.058601360993761245,\n",
       "   0.05861906748094994,\n",
       "   0.05859791094831411,\n",
       "   0.05860129075426284,\n",
       "   0.058601897979673016,\n",
       "   0.05859533899552594,\n",
       "   0.058603433751466365,\n",
       "   0.0586125425283345,\n",
       "   0.0586032746738418,\n",
       "   0.05862617967534362,\n",
       "   0.058614183560446585,\n",
       "   0.05860803236110577,\n",
       "   0.05860631990234882,\n",
       "   0.058589698071301724,\n",
       "   0.05860867737734466,\n",
       "   0.05861061974679781,\n",
       "   0.058601399378163194,\n",
       "   0.05860784103266926,\n",
       "   0.058601060052135674,\n",
       "   0.05858814290944966,\n",
       "   0.05860870943029887,\n",
       "   0.05859426818942628,\n",
       "   0.05860706068173484,\n",
       "   0.05859775800427955,\n",
       "   0.058608860395756,\n",
       "   0.05861049747071326,\n",
       "   0.05861614413281199,\n",
       "   0.05861581588681803,\n",
       "   0.05859661319938438,\n",
       "   0.05860152580926033,\n",
       "   0.05859141705936416,\n",
       "   0.05859788245679927,\n",
       "   0.058611481219406444,\n",
       "   0.05859968513868656,\n",
       "   0.058597810634438925,\n",
       "   0.05861594963865161,\n",
       "   0.0586158732655632,\n",
       "   0.05859647014823692,\n",
       "   0.05859322330269082,\n",
       "   0.05860058499569715,\n",
       "   0.058601059260704705,\n",
       "   0.05861025370997512,\n",
       "   0.05860145992263224,\n",
       "   0.05860389495786295,\n",
       "   0.05860800386959092,\n",
       "   0.05860349409807767,\n",
       "   0.05859961034846009,\n",
       "   0.05859702118204837,\n",
       "   0.05860190886184882,\n",
       "   0.05860027673333512,\n",
       "   0.058609379574470995,\n",
       "   0.05859335131664988,\n",
       "   0.05860521842829914,\n",
       "   0.05859792460049831,\n",
       "   0.058602782799495205,\n",
       "   0.058599221755854816,\n",
       "   0.058608120605658696,\n",
       "   0.05860152442425613,\n",
       "   0.058596664246681814,\n",
       "   0.05861185299410365,\n",
       "   0.058607525845286265,\n",
       "   0.058605442007547595,\n",
       "   0.05859549609457309,\n",
       "   0.05863340544008121,\n",
       "   0.05861589859135418,\n",
       "   0.05860509417363717,\n",
       "   0.05859816183193096,\n",
       "   0.05861038350465387,\n",
       "   0.0585986525191311,\n",
       "   0.058619673717071406,\n",
       "   0.058599587199104276,\n",
       "   0.05859368668552256,\n",
       "   0.05860174048491039,\n",
       "   0.058600060276965384,\n",
       "   0.058601978309916265,\n",
       "   0.05860851493613849,\n",
       "   0.05859719114184874,\n",
       "   0.05859248944832576,\n",
       "   0.05863642692565918,\n",
       "   0.058610446225558076,\n",
       "   0.058606461568492085,\n",
       "   0.05859727028494554,\n",
       "   0.05860267140558646,\n",
       "   0.05859492527498744,\n",
       "   0.05860810161131546,\n",
       "   0.058604887214439044,\n",
       "   0.058604680057383174,\n",
       "   0.05861235377204863,\n",
       "   0.05860335441051182,\n",
       "   0.05862103022975051,\n",
       "   0.05860317040281177,\n",
       "   0.058608139797859664,\n",
       "   0.05860586067452965,\n",
       "   0.05861020028838478,\n",
       "   0.05860505104064941,\n",
       "   0.058614159026086576,\n",
       "   0.05859928269603935,\n",
       "   0.058617619557994036,\n",
       "   0.05860321610795017,\n",
       "   0.058600743677606225,\n",
       "   0.058605210316131716,\n",
       "   0.058603064351062065,\n",
       "   0.05862252979357708,\n",
       "   0.058609125327272533,\n",
       "   0.058608490203920734,\n",
       "   0.058608465273845244,\n",
       "   0.058597502174219156,\n",
       "   0.05860556250291247,\n",
       "   0.058616215955172336,\n",
       "   0.05860474277828739,\n",
       "   0.05863248302728803,\n",
       "   0.05860940945099004,\n",
       "   0.05860649856788983,\n",
       "   0.05861227502466732,\n",
       "   0.058606582261714696,\n",
       "   0.05859800908575414,\n",
       "   0.05859530516185207,\n",
       "   0.05862621608116815,\n",
       "   0.05859721330191585,\n",
       "   0.05861325184833954,\n",
       "   0.058601131676638274,\n",
       "   0.05860163680745358,\n",
       "   0.058600263081150926,\n",
       "   0.05859864836411852,\n",
       "   0.05861598723162259,\n",
       "   0.058612448150191565,\n",
       "   0.058614157047509156,\n",
       "   0.05859293284752557,\n",
       "   0.05859937786561325,\n",
       "   0.05860368898795353,\n",
       "   0.05861211139631469,\n",
       "   0.058614132908864636,\n",
       "   0.058612485347447056,\n",
       "   0.058596899895252526,\n",
       "   0.05859633797926527,\n",
       "   0.05860960513229687,\n",
       "   0.05861334563290924,\n",
       "   0.05861354744780608,\n",
       "   0.05859395854206006,\n",
       "   0.05860025951971157,\n",
       "   0.05861115158840829,\n",
       "   0.058603453932956046,\n",
       "   0.05860353169104865,\n",
       "   0.0586105914531407,\n",
       "   0.058603485590194765,\n",
       "   0.05859773861422084,\n",
       "   0.05862348030216961,\n",
       "   0.05860874464897694,\n",
       "   0.058609064387088,\n",
       "   0.05860396994594717,\n",
       "   0.058617271921941355,\n",
       "   0.05860142509966965,\n",
       "   0.05859550420674051,\n",
       "   0.05859885393831245,\n",
       "   0.05861441643900891,\n",
       "   0.058614291986489195,\n",
       "   0.05860908219428478,\n",
       "   0.058604024554683956,\n",
       "   0.05859795249843993,\n",
       "   0.05859951082601587,\n",
       "   0.05860718493639681,\n",
       "   0.05861993904430342,\n",
       "   0.05860440285868664,\n",
       "   0.05860265379624743,\n",
       "   0.05861803980783803,\n",
       "   0.058615890083471275,\n",
       "   0.05860690793555802,\n",
       "   0.058614420791879236,\n",
       "   0.05859219028741987,\n",
       "   0.05860532804148821,\n",
       "   0.05861794206611348,\n",
       "   0.058600216386723816,\n",
       "   0.05860086120510497,\n",
       "   0.05862349415221155,\n",
       "   0.0585949755308539,\n",
       "   0.05860508527003878,\n",
       "   0.058602481066438665,\n",
       "   0.058598301123781324,\n",
       "   0.05860084221076174,\n",
       "   0.058611697477918445,\n",
       "   0.05860856083913463,\n",
       "   0.05859860503327302,\n",
       "   0.05859969859301302,\n",
       "   0.058605820509408026,\n",
       "   0.058611417113498036,\n",
       "   0.05860670433994151,\n",
       "   0.05859390986905553,\n",
       "   0.0586111054875544,\n",
       "   0.058603444831499915,\n",
       "   0.05860344700793508,\n",
       "   0.058612154727160186,\n",
       "   0.058608269394680675,\n",
       "   0.058601992159958205,\n",
       "   0.05859157158626066,\n",
       "   0.05860067502096975,\n",
       "   0.058596540387735326,\n",
       "   0.058596342727851074,\n",
       "   0.0586001574251167,\n",
       "   0.05860529697782271,\n",
       "   0.05860172188628264,\n",
       "   0.0586168975750935,\n",
       "   0.05861246298952221,\n",
       "   0.05859832150312875,\n",
       "   0.05859153478472064,\n",
       "   0.05859678612705088,\n",
       "   0.0586034834137596,\n",
       "   0.05859707974794,\n",
       "   0.05860266269984582,\n",
       "   0.05861274434323133,\n",
       "   0.058594438742799876,\n",
       "   0.058602822371043604,\n",
       "   0.05859745884337366,\n",
       "   0.058601972374184005,\n",
       "   0.05859828193158035,\n",
       "   0.05859506535826877,\n",
       "   0.058597981583528004,\n",
       "   0.058601355058028985,\n",
       "   0.05861019178050188,\n",
       "   0.05861153563028549,\n",
       "   0.05861285296713168,\n",
       "   0.05861144224143127,\n",
       "   0.05860178460718685,\n",
       "   0.05861090822338563,\n",
       "   0.05860627122934429,\n",
       "   0.05859499551448585,\n",
       "   0.05862352521587704,\n",
       "   0.058595112646269105,\n",
       "   0.05861416792968497,\n",
       "   0.05860984869517726,\n",
       "   0.05861053387653778,\n",
       "   0.05861932964245808,\n",
       "   0.05860287974978878,\n",
       "   0.05860110852728246,\n",
       "   0.058605861268102875,\n",
       "   0.058602001261414335,\n",
       "   0.058598046480867375,\n",
       "   0.05859985153704758,\n",
       "   0.05860167519185553,\n",
       "   0.058615314713157556,\n",
       "   0.05859774652853052,\n",
       "   0.05859788087393733,\n",
       "   0.05863112176602312,\n",
       "   0.05860869835026531,\n",
       "   0.05859360378312867,\n",
       "   0.05859993622016115,\n",
       "   0.05860812634353321,\n",
       "   0.05861244162088608,\n",
       "   0.0585970882558229,\n",
       "   0.05860437753289567,\n",
       "   0.058605569427933435,\n",
       "   0.05859445397784601,\n",
       "   0.05860738338771203,\n",
       "   0.058614760117906756,\n",
       "   0.05860094865822693,\n",
       "   0.05860898979471927,\n",
       "   0.058603189397155,\n",
       "   0.058610501625725835,\n",
       "   0.05860228479155861,\n",
       "   0.05861753942560853,\n",
       "   0.058603668410748364,\n",
       "   0.05860390722504295,\n",
       "   0.058603866466348103,\n",
       "   0.05859394944060393,\n",
       "   0.0585941269189985,\n",
       "   0.05860123416694863,\n",
       "   0.058594784400275136,\n",
       "   0.058599258161679345,\n",
       "   0.05862286832817363,\n",
       "   0.05859794062697541,\n",
       "   0.05860770767655116,\n",
       "   0.05861067653196976,\n",
       "   0.05862261269597097,\n",
       "   0.0586078238190457,\n",
       "   0.0586070468316929,\n",
       "   0.058601672619704885,\n",
       "   0.05859960441272783,\n",
       "   0.05860720927289907,\n",
       "   0.05859810069388868,\n",
       "   0.05861080276520915,\n",
       "   0.058591571388402916,\n",
       "   0.058599607380593964,\n",
       "   0.05860536721732112,\n",
       "   0.058600683133137176,\n",
       "   0.0585929753869401,\n",
       "   0.05860776545101182,\n",
       "   0.05862239485459703,\n",
       "   0.05860879549841663,\n",
       "   0.05860246187423769,\n",
       "   0.05861545539001212,\n",
       "   0.05859979593902208,\n",
       "   0.05861613879065296,\n",
       "   0.05860074407332171,\n",
       "   0.058610770514397205,\n",
       "   0.05860212769251147,\n",
       "   0.05860119756326636,\n",
       "   0.05862142733023869,\n",
       "   0.058591018573871785,\n",
       "   0.05860815443933257,\n",
       "   0.05860611274529295,\n",
       "   0.05860273749007229,\n",
       "   0.05860322995799211,\n",
       "   0.058599165959971576,\n",
       "   0.058601093687951814,\n",
       "   0.05860002822401118,\n",
       "   0.058600775334844944,\n",
       "   0.05860192647118786,\n",
       "   0.05860635195530302,\n",
       "   0.05860735885335202,\n",
       "   0.058618373791706514,\n",
       "   0.0585980965388761,\n",
       "   0.0586210711863031,\n",
       "   0.05858655549678565,\n",
       "   0.058601775901446204,\n",
       "   0.05859461819977187,\n",
       "   0.05860853412833946,\n",
       "   0.05860835783709134,\n",
       "   0.05862245678407028,\n",
       "   0.058614770010793855,\n",
       "   0.05860998482130375,\n",
       "   0.0585966684016944,\n",
       "   0.05860720195216262,\n",
       "   0.05860372460234709,\n",
       "   0.05859042796851194,\n",
       "   0.058609901523194376,\n",
       "   0.0585999844974502,\n",
       "   0.05861534419396112,\n",
       "   0.058599644973564936,\n",
       "   0.05860749715591367,\n",
       "   0.05860291397917815,\n",
       "   0.05859096099726887,\n",
       "   0.05861856808800915,\n",
       "   0.058613542897078014,\n",
       "   0.05861136250476125,\n",
       "   0.058608602389260446,\n",
       "   0.05860293198423267,\n",
       "   0.058607896037121535,\n",
       "   0.058621070988445365,\n",
       "   0.05860343038788475,\n",
       "   0.058605078938591036,\n",
       "   0.05860532171004046,\n",
       "   0.0585973417115904,\n",
       "   0.058602724233603576,\n",
       "   0.058595839971328674,\n",
       "   0.05861469403342093,\n",
       "   0.058606336720256884,\n",
       "   0.058598286086592934,\n",
       "   0.05860417156298626,\n",
       "   0.058608315693392296,\n",
       "   0.058603826894799704,\n",
       "   0.0586214109080461,\n",
       "   0.058597251884175534,\n",
       "   0.05862195541255207,\n",
       "   0.05859251299339706,\n",
       "   0.05860195614984916,\n",
       "   0.05860679456307186,\n",
       "   0.05859393479913102,\n",
       "   0.058610703242764926,\n",
       "   0.05862028232748578,\n",
       "   0.05860532210575595,\n",
       "   0.0585979952357122,\n",
       "   0.058604189172325294,\n",
       "   0.058611211143588623,\n",
       "   0.05860948305407006,\n",
       "   0.05860966923820527,\n",
       "   0.05861027033002545,\n",
       "   0.0586045548134325,\n",
       "   0.05860384766986261,\n",
       "   0.05861272752532326,\n",
       "   0.05861236326922025,\n",
       "   0.05860842273443072,\n",
       "   0.0586109782650263,\n",
       "   0.05861008375017475,\n",
       "   0.058598172120533545,\n",
       "   0.05860808499126514,\n",
       "   0.05861583408973029,\n",
       "   0.05861668092086602,\n",
       "   0.058621454238891604,\n",
       "   0.05861342358885959,\n",
       "   0.05859703245993966,\n",
       "   0.058613333563586986,\n",
       "   0.05859862521476271,\n",
       "   0.05861241095293607,\n",
       "   0.058599912675089856,\n",
       "   0.05859947719019973,\n",
       "   0.058607625565588226,\n",
       "   0.05858435116368211,\n",
       "   0.058615922136425476,\n",
       "   0.05860229864160055,\n",
       "   0.0586065563423505,\n",
       "   0.058601172435333124,\n",
       "   0.058616683493016665,\n",
       "   0.058597303722903935,\n",
       "   0.05860712379835453,\n",
       "   0.05859278603708101,\n",
       "   0.058608377820723284,\n",
       "   0.05860620039627265,\n",
       "   0.05861761105011113,\n",
       "   0.05860404137259202,\n",
       "   0.05859131852620865,\n",
       "   0.058601101206546005,\n",
       "   0.05860653853515372,\n",
       "   0.05860666793411698,\n",
       "   0.058602991143697526,\n",
       "   0.05860627953936945,\n",
       "   0.05860373034022161,\n",
       "   0.05860132359864801,\n",
       "   0.05861508895747394,\n",
       "   0.0586073742862559,\n",
       "   0.05860883803783116,\n",
       "   0.058604141686467215,\n",
       "   0.05861074063787816,\n",
       "   0.058602655379109365,\n",
       "   0.05860701359159224,\n",
       "   0.05859514450136557,\n",
       "   0.05859429964880725,\n",
       "   0.05859695371255835,\n",
       "   0.05860168389759618,\n",
       "   0.05858772345103663,\n",
       "   0.058597109822316785,\n",
       "   0.058608137819282244,\n",
       "   0.05860699439939127,\n",
       "   0.058604182445162065,\n",
       "   0.0586030696932211,\n",
       "   0.05860576807710639,\n",
       "   0.0586103134630132,\n",
       "   0.05860192389903721,\n",
       "   0.058590556576044235,\n",
       "   0.05861460420600606,\n",
       "   0.05860367790791998,\n",
       "   0.0586005244512281,\n",
       "   0.05859646302535821,\n",
       "   0.058591869955735586,\n",
       "   0.05859903853958573,\n",
       "   0.0586143871560631,\n",
       "   0.05861792089533509,\n",
       "   0.058618469554853635,\n",
       "   0.058607177417802615,\n",
       "   0.058615624756239264,\n",
       "   0.05860152323710968,\n",
       "   0.05859918337145287,\n",
       "   0.05860920031535675,\n",
       "   0.058607159610605836,\n",
       "   0.05861883677882278,\n",
       "   0.05859911451695866,\n",
       "   0.05860821498380162,\n",
       "   0.0586070147787387,\n",
       "   0.05860275608870004,\n",
       "   0.0586087331732279,\n",
       "   0.05860501028195456,\n",
       "   0.058598452089238466,\n",
       "   0.05861205560043145,\n",
       "   0.05860514383593041,\n",
       "   0.058613325649277306,\n",
       "   0.058601311133610265,\n",
       "   0.05861819433473452,\n",
       "   0.05859774059279826,\n",
       "   0.058610640324002976,\n",
       "   0.058598623829758514,\n",
       "   0.058597642257500485,\n",
       "   0.05860579993220286,\n",
       "   0.05860779038108731,\n",
       "   0.05860239974690671,\n",
       "   0.058588865485923416,\n",
       "   0.058606132926782635,\n",
       "   0.05860179252149653,\n",
       "   0.05860195337984077,\n",
       "   0.05860244090131704,\n",
       "   0.05861932766388066,\n",
       "   0.058600114094271204,\n",
       "   0.05860140195031384,\n",
       "   0.05859445714356988,\n",
       "   0.058603255481640824,\n",
       "   0.058603092446861425,\n",
       "   0.05861487151181549,\n",
       "   0.05860910831150672,\n",
       "   0.058606624207556,\n",
       "   0.05860531676359691,\n",
       "   0.058597991476415104,\n",
       "   0.0586065511980492,\n",
       "   0.05860257999530966,\n",
       "   0.05860775536026697,\n",
       "   0.05860746490510173,\n",
       "   0.05859945384298618,\n",
       "   0.05860029651910932,\n",
       "   0.05859362970249287,\n",
       "   0.058599635872108806,\n",
       "   0.058606721157849574,\n",
       "   0.05860805590617706,\n",
       "   0.058598819511065346,\n",
       "   0.058616237323808475,\n",
       "   0.058609068739958324,\n",
       "   0.05860760795624919,\n",
       "   0.05859945047940456,\n",
       "   0.05860196406415884,\n",
       "   0.058600054341233124,\n",
       "   0.058613119283652405,\n",
       "   0.0586015412421642,\n",
       "   0.05860582783014448,\n",
       "   0.0585982732258397,\n",
       "   0.058596714700406026,\n",
       "   0.05861139871272803,\n",
       "   0.05859157376269582,\n",
       "   0.058607982698812525,\n",
       "   0.05860563373169958,\n",
       "   0.05860044016383001,\n",
       "   0.058608116846361594,\n",
       "   0.058612020183895636,\n",
       "   0.05860430234695371,\n",
       "   0.05860057292637488,\n",
       "   0.058593113095928524,\n",
       "   0.05860294998928719,\n",
       "   0.058608564400573986,\n",
       "   0.05859321459695017,\n",
       "   0.05860343474075507,\n",
       "   0.05863061465663039,\n",
       "   0.0586139706655162,\n",
       "   0.05861091495054886,\n",
       "   0.058598851168304066,\n",
       "   0.05858965276187881,\n",
       "   0.05860584563734126,\n",
       "   0.0586057025861938,\n",
       "   ...],\n",
       "  'val': []},\n",
       " 0,\n",
       " inf)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val(model = m1_mlp21, params = m1_mlp21_params)\n",
    "train_val(model = m1_mlp22, params = m1_mlp22_params)\n",
    "train_val(model = m1_mlp23, params = m1_mlp23_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m1_mlp21.state_dict(), '../model/MLP2LGBM_V2/round2/m1_mlp21.pt')\n",
    "torch.save(m1_mlp22.state_dict(), '../model/MLP2LGBM_V2/round2/m1_mlp22.pt')\n",
    "torch.save(m1_mlp23.state_dict(), '../model/MLP2LGBM_V2/round2/m1_mlp23.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/MLP/round2/mms_Y_r2_M1_T3.pkl']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.externals \n",
    "import joblib\n",
    "scaler_filename1 = \"../model/MLP2LGBM_V2/round2/mms_Y_r2_M1_T1.pkl\"\n",
    "scaler_filename2 = \"../model/MLP2LGBM_V2/round2/mms_Y_r2_M1_T2.pkl\"\n",
    "scaler_filename3 = \"../model/MLP2LGBM_V2/round2/mms_Y_r2_M1_T3.pkl\"\n",
    "joblib.dump(mms_Y_211, scaler_filename1) \n",
    "joblib.dump(mms_Y_212, scaler_filename2) \n",
    "joblib.dump(mms_Y_213, scaler_filename3) \n",
    "\n",
    "# And now to load...\n",
    "\n",
    "# scaler = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2차 모델링에 사용할 y값을 만들어야함.\n",
    "## 2차 모델링에서 사용할 y는 Y-Y_hat = e 임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1_mlp11 = MLP_regressor(6)\n",
    "m1_mlp12 = MLP_regressor(6)\n",
    "m1_mlp13 = MLP_regressor(6)\n",
    "m1_mlp11.load_state_dict(torch.load('../model/MLP2LGBM_V2/round1/m1_mlp11.pt', map_location=torch.device('cpu')))\n",
    "m1_mlp12.load_state_dict(torch.load('../model/MLP2LGBM_V2/round1/m1_mlp12.pt', map_location=torch.device('cpu')))\n",
    "m1_mlp13.load_state_dict(torch.load('../model/MLP2LGBM_V2/round1/m1_mlp13.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1_mlp21 = MLP_regressor(6)\n",
    "m1_mlp22 = MLP_regressor(6)\n",
    "m1_mlp23 = MLP_regressor(6)\n",
    "m1_mlp21.load_state_dict(torch.load('../model/MLP2LGBM_V2/round2/m1_mlp21.pt', map_location=torch.device('cpu')))\n",
    "m1_mlp22.load_state_dict(torch.load('../model/MLP2LGBM_V2/round2/m1_mlp22.pt', map_location=torch.device('cpu')))\n",
    "m1_mlp23.load_state_dict(torch.load('../model/MLP2LGBM_V2/round2/m1_mlp23.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlpPredict(model, df):\n",
    "    model.to(DEVICE)\n",
    "    X = df.to_numpy()\n",
    "    X = torch.tensor(X, dtype = torch.float32, device = DEVICE)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # if DEVICE == 'cpu':\n",
    "        pred = model(X).to('cpu').detach()\n",
    "    \n",
    "    model.train()\n",
    "    return pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2차 모델링에 사용할 Y 데이터 셋 생성\n",
    "pred_m1_mlp11 = mlpPredict(m1_mlp11, x_df11_1)\n",
    "pred_m1_mlp12 = mlpPredict(m1_mlp12, x_df11_2)\n",
    "pred_m1_mlp13 = mlpPredict(m1_mlp13, x_df11_3)\n",
    "\n",
    "pred_m1_mlp11 = mms_Y_111.inverse_transform(pred_m1_mlp11).reshape(-1,)\n",
    "pred_m1_mlp12 = mms_Y_112.inverse_transform(pred_m1_mlp12).reshape(-1,)\n",
    "pred_m1_mlp13 = mms_Y_113.inverse_transform(pred_m1_mlp13).reshape(-1,)\n",
    "\n",
    "y_df12_1 = y_df11_1 - pred_m1_mlp11\n",
    "y_df12_2 = y_df11_2 - pred_m1_mlp12\n",
    "y_df12_3 = y_df11_3 - pred_m1_mlp13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2차 모델링에 사용할 Y 데이터 셋 생성\n",
    "pred_m1_mlp21 = mlpPredict(m1_mlp21, x_df21_1)\n",
    "pred_m1_mlp22 = mlpPredict(m1_mlp22, x_df21_2)\n",
    "pred_m1_mlp23 = mlpPredict(m1_mlp23, x_df21_3)\n",
    "\n",
    "pred_m1_mlp21 = mms_Y_111.inverse_transform(pred_m1_mlp21).reshape(-1,)\n",
    "pred_m1_mlp22 = mms_Y_112.inverse_transform(pred_m1_mlp22).reshape(-1,)\n",
    "pred_m1_mlp23 = mms_Y_113.inverse_transform(pred_m1_mlp23).reshape(-1,)\n",
    "\n",
    "y_df22_1 = y_df21_1 - pred_m1_mlp21\n",
    "y_df22_2 = y_df21_2 - pred_m1_mlp22\n",
    "y_df22_3 = y_df21_3 - pred_m1_mlp23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링2 잔차에 대하여 fitting (target variable Y = y_df2_1, y_df2_2, y_df2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. TimeSeriesSplit 기본\n",
    "n_split = 5\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=n_split)\n",
    "\n",
    "plot_cv_indices(tscv, x_df12_1, n_splits=n_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Blocked TimeSeriesSplit\n",
    "btscv = BlockingTimeSeriesSplit(n_splits=n_split)\n",
    "plot_cv_indices(btscv, x_df2_1, n_splits=n_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나중에 쓸꺼같음 일단 두셈\n",
    "score_df = abs(y_pred-y_test)*100/99\n",
    "score = 0\n",
    "for i in score_df:\n",
    "    if i <=6:\n",
    "        score += y_test * 4\n",
    "    elif (i <=8) & (i > 6):\n",
    "        score += y_test * 3\n",
    "    else:\n",
    "        score += 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optuna 트리모델 설정\n",
    "\n",
    "def objective(trial, X, Y):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 20, 2000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.5),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 50),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0)\n",
    "    }\n",
    "\n",
    "    scores = []\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        x_train, y_train=X.iloc[train_index], Y.iloc[train_index]\n",
    "        x_test, y_test=X.iloc[test_index], Y.iloc[test_index]\n",
    "\n",
    "        lgbm_model=LGBMRegressor(**params, boost_from_average=False,\n",
    "                                  random_state=42)\n",
    "        lgbm_model.fit(x_train, y_train)\n",
    "\n",
    "        y_pred=lgbm_model.predict(x_test)\n",
    "        score = mean_squared_error(y_test,y_pred)\n",
    "\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-29 05:03:49,821] A new study created in memory with name: no-name-514283bf-6e2d-436c-8346-b6e78a060e9e\n",
      "[I 2023-10-29 05:03:50,344] Trial 0 finished with value: 148.43722035289395 and parameters: {'n_estimators': 507, 'max_depth': 12, 'learning_rate': 0.26460784246774094, 'num_leaves': 4, 'min_child_samples': 51, 'reg_alpha': 0.7194743145630147, 'reg_lambda': 0.1237259911569168}. Best is trial 0 with value: 148.43722035289395.\n",
      "[I 2023-10-29 05:03:53,375] Trial 1 finished with value: 153.69982329290892 and parameters: {'n_estimators': 814, 'max_depth': 19, 'learning_rate': 0.17544280922235256, 'num_leaves': 24, 'min_child_samples': 21, 'reg_alpha': 0.10426322521806884, 'reg_lambda': 0.22319615011462182}. Best is trial 0 with value: 148.43722035289395.\n",
      "[I 2023-10-29 05:03:54,417] Trial 2 finished with value: 170.61716050713446 and parameters: {'n_estimators': 564, 'max_depth': 7, 'learning_rate': 0.3624080959248563, 'num_leaves': 28, 'min_child_samples': 68, 'reg_alpha': 0.6817480900749763, 'reg_lambda': 0.6377553374067182}. Best is trial 0 with value: 148.43722035289395.\n",
      "[I 2023-10-29 05:03:54,987] Trial 3 finished with value: 123.15901383569816 and parameters: {'n_estimators': 905, 'max_depth': 6, 'learning_rate': 0.0767276258477127, 'num_leaves': 3, 'min_child_samples': 32, 'reg_alpha': 0.8572586744129931, 'reg_lambda': 0.8728981369330745}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:03:57,186] Trial 4 finished with value: 159.7043458284119 and parameters: {'n_estimators': 990, 'max_depth': 12, 'learning_rate': 0.2706429365557994, 'num_leaves': 15, 'min_child_samples': 35, 'reg_alpha': 0.651191397789754, 'reg_lambda': 0.33622560727027956}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:03,553] Trial 5 finished with value: 144.5706524116871 and parameters: {'n_estimators': 1865, 'max_depth': 15, 'learning_rate': 0.03826287877164174, 'num_leaves': 23, 'min_child_samples': 23, 'reg_alpha': 0.29084860347508035, 'reg_lambda': 0.6844178698422108}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:04,744] Trial 6 finished with value: 162.94924348535315 and parameters: {'n_estimators': 549, 'max_depth': 14, 'learning_rate': 0.4184749886882078, 'num_leaves': 25, 'min_child_samples': 68, 'reg_alpha': 0.8923501077720358, 'reg_lambda': 0.76006345096108}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:07,771] Trial 7 finished with value: 150.78091907215713 and parameters: {'n_estimators': 1526, 'max_depth': 8, 'learning_rate': 0.08461499723406918, 'num_leaves': 12, 'min_child_samples': 18, 'reg_alpha': 0.7583757211179983, 'reg_lambda': 0.9371342213974065}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:08,850] Trial 8 finished with value: 151.59410591330706 and parameters: {'n_estimators': 843, 'max_depth': 5, 'learning_rate': 0.1829937299243773, 'num_leaves': 31, 'min_child_samples': 79, 'reg_alpha': 0.29483439325453614, 'reg_lambda': 0.9785356674889912}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:09,939] Trial 9 finished with value: 149.95904781729095 and parameters: {'n_estimators': 825, 'max_depth': 9, 'learning_rate': 0.14469646112092716, 'num_leaves': 7, 'min_child_samples': 7, 'reg_alpha': 0.7169410948558818, 'reg_lambda': 0.7335445671104273}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:10,174] Trial 10 finished with value: 362.996377894872 and parameters: {'n_estimators': 198, 'max_depth': 3, 'learning_rate': 0.0032208719231441307, 'num_leaves': 50, 'min_child_samples': 95, 'reg_alpha': 0.9702264443297473, 'reg_lambda': 0.46643416527537085}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:16,276] Trial 11 finished with value: 132.79373776270506 and parameters: {'n_estimators': 1862, 'max_depth': 16, 'learning_rate': 0.0014856344733003846, 'num_leaves': 42, 'min_child_samples': 39, 'reg_alpha': 0.4511302732907643, 'reg_lambda': 0.5808317441182119}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:20,787] Trial 12 finished with value: 124.78755253225921 and parameters: {'n_estimators': 1359, 'max_depth': 18, 'learning_rate': 0.008652904559486148, 'num_leaves': 46, 'min_child_samples': 43, 'reg_alpha': 0.4729610875957842, 'reg_lambda': 0.507288842938018}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:25,303] Trial 13 finished with value: 150.41897109548052 and parameters: {'n_estimators': 1344, 'max_depth': 20, 'learning_rate': 0.08049299883475042, 'num_leaves': 38, 'min_child_samples': 47, 'reg_alpha': 0.5863097229120381, 'reg_lambda': 0.4619495211823289}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:30,776] Trial 14 finished with value: 134.09344749391283 and parameters: {'n_estimators': 1376, 'max_depth': 18, 'learning_rate': 0.09680157367598052, 'num_leaves': 44, 'min_child_samples': 34, 'reg_alpha': 0.49018175707410594, 'reg_lambda': 0.8155563658668046}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:33,538] Trial 15 finished with value: 141.3116818307353 and parameters: {'n_estimators': 1230, 'max_depth': 10, 'learning_rate': 0.05542498184436409, 'num_leaves': 17, 'min_child_samples': 64, 'reg_alpha': 0.8596112390924867, 'reg_lambda': 0.009099126326007567}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:36,705] Trial 16 finished with value: 138.53300696740448 and parameters: {'n_estimators': 1638, 'max_depth': 6, 'learning_rate': 0.1208726863589556, 'num_leaves': 34, 'min_child_samples': 7, 'reg_alpha': 0.9998251236159328, 'reg_lambda': 0.8666962546482655}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:39,611] Trial 17 finished with value: 182.69147256005775 and parameters: {'n_estimators': 1209, 'max_depth': 17, 'learning_rate': 0.0011945761786022625, 'num_leaves': 50, 'min_child_samples': 59, 'reg_alpha': 0.5515953868538301, 'reg_lambda': 0.5950360458640944}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:39,726] Trial 18 finished with value: 169.96641470241948 and parameters: {'n_estimators': 127, 'max_depth': 13, 'learning_rate': 0.20532449642888406, 'num_leaves': 2, 'min_child_samples': 43, 'reg_alpha': 0.8432622169425101, 'reg_lambda': 0.9869546423359713}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:41,580] Trial 19 finished with value: 149.06803480462128 and parameters: {'n_estimators': 1638, 'max_depth': 3, 'learning_rate': 0.060006928095803005, 'num_leaves': 19, 'min_child_samples': 29, 'reg_alpha': 0.41594575209042695, 'reg_lambda': 0.8448524999940389}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:43,130] Trial 20 finished with value: 144.22494951244127 and parameters: {'n_estimators': 1071, 'max_depth': 10, 'learning_rate': 0.11196162750137872, 'num_leaves': 9, 'min_child_samples': 80, 'reg_alpha': 0.6132379727677391, 'reg_lambda': 0.535600521915585}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:50,171] Trial 21 finished with value: 127.5227620723953 and parameters: {'n_estimators': 1938, 'max_depth': 16, 'learning_rate': 0.0076888849523724925, 'num_leaves': 41, 'min_child_samples': 40, 'reg_alpha': 0.4479097297511037, 'reg_lambda': 0.5821838487988473}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:04:55,691] Trial 22 finished with value: 139.31666086311955 and parameters: {'n_estimators': 1856, 'max_depth': 17, 'learning_rate': 0.03937615138536937, 'num_leaves': 45, 'min_child_samples': 55, 'reg_alpha': 0.5274464443306534, 'reg_lambda': 0.698182587454519}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:05:03,089] Trial 23 finished with value: 140.62571531131692 and parameters: {'n_estimators': 1535, 'max_depth': 20, 'learning_rate': 0.04581598729028057, 'num_leaves': 38, 'min_child_samples': 27, 'reg_alpha': 0.3961018686682731, 'reg_lambda': 0.3991568102790025}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:05:06,585] Trial 24 finished with value: 149.1385733291632 and parameters: {'n_estimators': 1060, 'max_depth': 14, 'learning_rate': 0.13117980432345921, 'num_leaves': 38, 'min_child_samples': 43, 'reg_alpha': 0.6009302639141718, 'reg_lambda': 0.5442412931515123}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:05:12,381] Trial 25 finished with value: 144.78663248190279 and parameters: {'n_estimators': 1997, 'max_depth': 16, 'learning_rate': 0.08509639940844144, 'num_leaves': 46, 'min_child_samples': 15, 'reg_alpha': 0.7697963638824016, 'reg_lambda': 0.6108504883864868}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:05:13,798] Trial 26 finished with value: 124.55080285263861 and parameters: {'n_estimators': 359, 'max_depth': 18, 'learning_rate': 0.02681522443046664, 'num_leaves': 34, 'min_child_samples': 32, 'reg_alpha': 0.5113841789959404, 'reg_lambda': 0.7537993993439941}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:05:14,558] Trial 27 finished with value: 125.77771068710877 and parameters: {'n_estimators': 379, 'max_depth': 5, 'learning_rate': 0.037570176859742524, 'num_leaves': 33, 'min_child_samples': 33, 'reg_alpha': 0.6317819853742428, 'reg_lambda': 0.8918714886942062}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:05:16,449] Trial 28 finished with value: 146.38327442133973 and parameters: {'n_estimators': 393, 'max_depth': 18, 'learning_rate': 0.07160196339767294, 'num_leaves': 30, 'min_child_samples': 15, 'reg_alpha': 0.7894262020317612, 'reg_lambda': 0.7867967906900382}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:05:18,198] Trial 29 finished with value: 146.43187342546815 and parameters: {'n_estimators': 638, 'max_depth': 11, 'learning_rate': 0.1493697894344643, 'num_leaves': 20, 'min_child_samples': 50, 'reg_alpha': 0.6925902034417204, 'reg_lambda': 0.7760268488833342}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:05:18,349] Trial 30 finished with value: 128.69766917383325 and parameters: {'n_estimators': 34, 'max_depth': 13, 'learning_rate': 0.10570979109227682, 'num_leaves': 34, 'min_child_samples': 51, 'reg_alpha': 0.5477670741332045, 'reg_lambda': 0.6826282447607561}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:05:19,081] Trial 31 finished with value: 126.13637556624504 and parameters: {'n_estimators': 362, 'max_depth': 5, 'learning_rate': 0.03434863869248511, 'num_leaves': 32, 'min_child_samples': 32, 'reg_alpha': 0.648891213743581, 'reg_lambda': 0.8899209362842486}. Best is trial 3 with value: 123.15901383569816.\n",
      "[I 2023-10-29 05:05:19,739] Trial 32 finished with value: 122.51289825128575 and parameters: {'n_estimators': 313, 'max_depth': 5, 'learning_rate': 0.042864721205895635, 'num_leaves': 34, 'min_child_samples': 26, 'reg_alpha': 0.4937110732091576, 'reg_lambda': 0.9016729969779855}. Best is trial 32 with value: 122.51289825128575.\n",
      "[I 2023-10-29 05:05:20,719] Trial 33 finished with value: 151.09225377446242 and parameters: {'n_estimators': 644, 'max_depth': 4, 'learning_rate': 0.06720380997535849, 'num_leaves': 28, 'min_child_samples': 27, 'reg_alpha': 0.36405303418244717, 'reg_lambda': 0.8201540087415747}. Best is trial 32 with value: 122.51289825128575.\n",
      "[I 2023-10-29 05:05:21,728] Trial 34 finished with value: 119.46264877137648 and parameters: {'n_estimators': 269, 'max_depth': 7, 'learning_rate': 0.025722670017035727, 'num_leaves': 37, 'min_child_samples': 23, 'reg_alpha': 0.49046047524490544, 'reg_lambda': 0.9319043769847576}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:22,561] Trial 35 finished with value: 128.1220211813487 and parameters: {'n_estimators': 274, 'max_depth': 7, 'learning_rate': 0.10211922094347542, 'num_leaves': 36, 'min_child_samples': 22, 'reg_alpha': 0.14893339140402484, 'reg_lambda': 0.937089739986008}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:23,979] Trial 36 finished with value: 158.32666846972754 and parameters: {'n_estimators': 462, 'max_depth': 7, 'learning_rate': 0.2506389937015503, 'num_leaves': 27, 'min_child_samples': 14, 'reg_alpha': 0.5352892022737321, 'reg_lambda': 0.9202636862747112}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:24,630] Trial 37 finished with value: 122.33851224405508 and parameters: {'n_estimators': 197, 'max_depth': 8, 'learning_rate': 0.034651636042813974, 'num_leaves': 24, 'min_child_samples': 24, 'reg_alpha': 0.7049935419292336, 'reg_lambda': 0.9899073566784626}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:24,735] Trial 38 finished with value: 201.10014438161284 and parameters: {'n_estimators': 20, 'max_depth': 7, 'learning_rate': 0.06371413228767017, 'num_leaves': 13, 'min_child_samples': 22, 'reg_alpha': 0.6902254616772732, 'reg_lambda': 0.9546867931481906}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:26,419] Trial 39 finished with value: 157.04857421463177 and parameters: {'n_estimators': 712, 'max_depth': 8, 'learning_rate': 0.30288038342629403, 'num_leaves': 25, 'min_child_samples': 12, 'reg_alpha': 0.7657094301136128, 'reg_lambda': 0.9739932739845524}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:26,692] Trial 40 finished with value: 135.47639081067078 and parameters: {'n_estimators': 224, 'max_depth': 6, 'learning_rate': 0.161850044551592, 'num_leaves': 5, 'min_child_samples': 19, 'reg_alpha': 0.6823199989976956, 'reg_lambda': 0.8879918126893663}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:27,684] Trial 41 finished with value: 124.33136572455268 and parameters: {'n_estimators': 296, 'max_depth': 9, 'learning_rate': 0.02751463108967124, 'num_leaves': 29, 'min_child_samples': 26, 'reg_alpha': 0.5665110345820856, 'reg_lambda': 0.991453170848852}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:28,213] Trial 42 finished with value: 127.82334414112536 and parameters: {'n_estimators': 153, 'max_depth': 9, 'learning_rate': 0.02813894142006516, 'num_leaves': 22, 'min_child_samples': 24, 'reg_alpha': 0.5872376230793777, 'reg_lambda': 0.9763646012104525}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:30,258] Trial 43 finished with value: 138.27608470879161 and parameters: {'n_estimators': 481, 'max_depth': 8, 'learning_rate': 0.08165548298301886, 'num_leaves': 30, 'min_child_samples': 9, 'reg_alpha': 0.6324625747669276, 'reg_lambda': 0.9995828526984261}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:31,124] Trial 44 finished with value: 120.86836416199826 and parameters: {'n_estimators': 274, 'max_depth': 6, 'learning_rate': 0.026019226265173584, 'num_leaves': 29, 'min_child_samples': 27, 'reg_alpha': 0.7293000856024888, 'reg_lambda': 0.9137724413839575}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:31,418] Trial 45 finished with value: 124.93462369579271 and parameters: {'n_estimators': 128, 'max_depth': 4, 'learning_rate': 0.05253676866193854, 'num_leaves': 9, 'min_child_samples': 38, 'reg_alpha': 0.7285233530215109, 'reg_lambda': 0.9223329962032303}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:33,153] Trial 46 finished with value: 138.1109998296926 and parameters: {'n_estimators': 576, 'max_depth': 6, 'learning_rate': 0.09197692186169674, 'num_leaves': 40, 'min_child_samples': 17, 'reg_alpha': 0.9406135297056383, 'reg_lambda': 0.8422508705428351}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:34,416] Trial 47 finished with value: 160.22729184401098 and parameters: {'n_estimators': 879, 'max_depth': 4, 'learning_rate': 0.1253625032223218, 'num_leaves': 22, 'min_child_samples': 30, 'reg_alpha': 0.8166822487882135, 'reg_lambda': 0.9184798997716506}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:35,767] Trial 48 finished with value: 122.61620425419254 and parameters: {'n_estimators': 481, 'max_depth': 6, 'learning_rate': 0.020960019619310547, 'num_leaves': 26, 'min_child_samples': 19, 'reg_alpha': 0.8901596147067539, 'reg_lambda': 0.8586439537714678}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:38,573] Trial 49 finished with value: 132.41203012508888 and parameters: {'n_estimators': 740, 'max_depth': 6, 'learning_rate': 0.0229907785266824, 'num_leaves': 26, 'min_child_samples': 5, 'reg_alpha': 0.9076300610197348, 'reg_lambda': 0.8254810228345792}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:39,399] Trial 50 finished with value: 135.0683848223888 and parameters: {'n_estimators': 258, 'max_depth': 8, 'learning_rate': 0.05312818778350695, 'num_leaves': 24, 'min_child_samples': 10, 'reg_alpha': 0.740887833814218, 'reg_lambda': 0.8767804706632865}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:40,380] Trial 51 finished with value: 124.45149175785956 and parameters: {'n_estimators': 529, 'max_depth': 5, 'learning_rate': 0.014906349959753026, 'num_leaves': 36, 'min_child_samples': 20, 'reg_alpha': 0.8951147128934076, 'reg_lambda': 0.9483129810386909}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:41,427] Trial 52 finished with value: 404.02110850886186 and parameters: {'n_estimators': 427, 'max_depth': 7, 'learning_rate': 0.0006771730663525308, 'num_leaves': 16, 'min_child_samples': 37, 'reg_alpha': 0.8190744896041245, 'reg_lambda': 0.8601197282235011}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:42,576] Trial 53 finished with value: 149.66794205116787 and parameters: {'n_estimators': 955, 'max_depth': 4, 'learning_rate': 0.07047894494642087, 'num_leaves': 36, 'min_child_samples': 25, 'reg_alpha': 0.8739375855935767, 'reg_lambda': 0.7801667594560057}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:42,913] Trial 54 finished with value: 124.87626614832499 and parameters: {'n_estimators': 99, 'max_depth': 6, 'learning_rate': 0.04965937235140791, 'num_leaves': 19, 'min_child_samples': 21, 'reg_alpha': 0.9509170607283669, 'reg_lambda': 0.9071865749519351}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:43,205] Trial 55 finished with value: 162.1709618273886 and parameters: {'n_estimators': 194, 'max_depth': 3, 'learning_rate': 0.019926200227624175, 'num_leaves': 31, 'min_child_samples': 17, 'reg_alpha': 0.8213322288215, 'reg_lambda': 0.9451087760171136}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:43,836] Trial 56 finished with value: 135.88332659180665 and parameters: {'n_estimators': 311, 'max_depth': 5, 'learning_rate': 0.08868427437392905, 'num_leaves': 27, 'min_child_samples': 30, 'reg_alpha': 0.7145380036387445, 'reg_lambda': 0.8419178367155019}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:44,204] Trial 57 finished with value: 121.09452721634457 and parameters: {'n_estimators': 221, 'max_depth': 9, 'learning_rate': 0.040046865755876085, 'num_leaves': 12, 'min_child_samples': 95, 'reg_alpha': 0.9124465521110503, 'reg_lambda': 0.7953328138918883}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:44,373] Trial 58 finished with value: 137.22436077047266 and parameters: {'n_estimators': 80, 'max_depth': 10, 'learning_rate': 0.03843263610249935, 'num_leaves': 13, 'min_child_samples': 98, 'reg_alpha': 0.9131033907065884, 'reg_lambda': 0.8638961995755485}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:44,673] Trial 59 finished with value: 544.4462013063683 and parameters: {'n_estimators': 187, 'max_depth': 11, 'learning_rate': 0.0002302775424143158, 'num_leaves': 21, 'min_child_samples': 89, 'reg_alpha': 0.9949756853223626, 'reg_lambda': 0.8025228543917952}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:45,134] Trial 60 finished with value: 124.1690381331296 and parameters: {'n_estimators': 322, 'max_depth': 7, 'learning_rate': 0.015765306458188202, 'num_leaves': 40, 'min_child_samples': 76, 'reg_alpha': 0.5040660580234251, 'reg_lambda': 0.7147466433648958}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:45,551] Trial 61 finished with value: 125.23314570968074 and parameters: {'n_estimators': 234, 'max_depth': 8, 'learning_rate': 0.058285116529084274, 'num_leaves': 8, 'min_child_samples': 35, 'reg_alpha': 0.8641627554376914, 'reg_lambda': 0.7478696452269015}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:45,815] Trial 62 finished with value: 195.64585080862574 and parameters: {'n_estimators': 440, 'max_depth': 9, 'learning_rate': 0.0435754214858483, 'num_leaves': 2, 'min_child_samples': 71, 'reg_alpha': 0.8814099956745883, 'reg_lambda': 0.9062467636770879}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:46,345] Trial 63 finished with value: 124.68664254316597 and parameters: {'n_estimators': 541, 'max_depth': 6, 'learning_rate': 0.0775037139780776, 'num_leaves': 5, 'min_child_samples': 91, 'reg_alpha': 0.9198267816446464, 'reg_lambda': 0.8126594890242528}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:46,670] Trial 64 finished with value: 143.62575920634006 and parameters: {'n_estimators': 161, 'max_depth': 7, 'learning_rate': 0.018445558879195162, 'num_leaves': 11, 'min_child_samples': 58, 'reg_alpha': 0.791696573028818, 'reg_lambda': 0.9493128000328181}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:48,695] Trial 65 finished with value: 163.09568254100373 and parameters: {'n_estimators': 1206, 'max_depth': 5, 'learning_rate': 0.11130380399943512, 'num_leaves': 18, 'min_child_samples': 46, 'reg_alpha': 0.8383009030972477, 'reg_lambda': 0.8650058174661122}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:49,844] Trial 66 finished with value: 125.56601680343859 and parameters: {'n_estimators': 342, 'max_depth': 9, 'learning_rate': 0.03989476940310405, 'num_leaves': 32, 'min_child_samples': 28, 'reg_alpha': 0.9637815489629381, 'reg_lambda': 0.9991938249105313}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:51,663] Trial 67 finished with value: 133.99643808412677 and parameters: {'n_estimators': 640, 'max_depth': 8, 'learning_rate': 0.06027405223688767, 'num_leaves': 43, 'min_child_samples': 24, 'reg_alpha': 0.841809764691494, 'reg_lambda': 0.8882430815390767}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:51,961] Trial 68 finished with value: 123.91942316307039 and parameters: {'n_estimators': 62, 'max_depth': 10, 'learning_rate': 0.09331507299193553, 'num_leaves': 24, 'min_child_samples': 13, 'reg_alpha': 0.4691742657845108, 'reg_lambda': 0.9612913762899167}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:52,579] Trial 69 finished with value: 128.49593425958216 and parameters: {'n_estimators': 396, 'max_depth': 6, 'learning_rate': 0.011541115181190597, 'num_leaves': 11, 'min_child_samples': 85, 'reg_alpha': 0.6652031384628802, 'reg_lambda': 0.7923720277965972}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:53,023] Trial 70 finished with value: 137.4064374641785 and parameters: {'n_estimators': 281, 'max_depth': 4, 'learning_rate': 0.07329746714063634, 'num_leaves': 15, 'min_child_samples': 41, 'reg_alpha': 0.7785321945375796, 'reg_lambda': 0.8357101443018294}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:53,426] Trial 71 finished with value: 132.21475118230336 and parameters: {'n_estimators': 91, 'max_depth': 11, 'learning_rate': 0.09409094166544468, 'num_leaves': 24, 'min_child_samples': 14, 'reg_alpha': 0.447953834354476, 'reg_lambda': 0.9667067425073548}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:53,777] Trial 72 finished with value: 143.94659323307351 and parameters: {'n_estimators': 67, 'max_depth': 10, 'learning_rate': 0.03236694728119091, 'num_leaves': 28, 'min_child_samples': 12, 'reg_alpha': 0.4764109001882977, 'reg_lambda': 0.9320361986239998}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:54,665] Trial 73 finished with value: 133.11248288397425 and parameters: {'n_estimators': 228, 'max_depth': 10, 'learning_rate': 0.04243222627275822, 'num_leaves': 26, 'min_child_samples': 18, 'reg_alpha': 0.505492781579876, 'reg_lambda': 0.9046030701194469}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:55,395] Trial 74 finished with value: 131.55360971981438 and parameters: {'n_estimators': 185, 'max_depth': 12, 'learning_rate': 0.0624108531507534, 'num_leaves': 23, 'min_child_samples': 23, 'reg_alpha': 0.5953762694577984, 'reg_lambda': 0.9647682836328005}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:55,561] Trial 75 finished with value: 129.11246865507766 and parameters: {'n_estimators': 30, 'max_depth': 8, 'learning_rate': 0.08072509705300931, 'num_leaves': 35, 'min_child_samples': 33, 'reg_alpha': 0.5700919980138878, 'reg_lambda': 0.87232206916741}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:56,382] Trial 76 finished with value: 119.49583606086505 and parameters: {'n_estimators': 270, 'max_depth': 7, 'learning_rate': 0.025353894271543026, 'num_leaves': 30, 'min_child_samples': 30, 'reg_alpha': 0.7517465683892295, 'reg_lambda': 0.9344010521232442}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:57,391] Trial 77 finished with value: 126.20401337749608 and parameters: {'n_estimators': 423, 'max_depth': 5, 'learning_rate': 0.025273355290158225, 'num_leaves': 38, 'min_child_samples': 30, 'reg_alpha': 0.7538269420095873, 'reg_lambda': 0.9327592787021819}. Best is trial 34 with value: 119.46264877137648.\n",
      "[I 2023-10-29 05:05:59,413] Trial 78 finished with value: 119.26285354435943 and parameters: {'n_estimators': 489, 'max_depth': 7, 'learning_rate': 0.010423394277420409, 'num_leaves': 32, 'min_child_samples': 26, 'reg_alpha': 0.8010291100113381, 'reg_lambda': 0.7676527848812871}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:00,760] Trial 79 finished with value: 120.62197381731781 and parameters: {'n_estimators': 485, 'max_depth': 7, 'learning_rate': 0.014403418896985244, 'num_leaves': 30, 'min_child_samples': 35, 'reg_alpha': 0.7978276565216822, 'reg_lambda': 0.7561135152872526}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:01,851] Trial 80 finished with value: 469.47747648906 and parameters: {'n_estimators': 348, 'max_depth': 7, 'learning_rate': 0.0004470808219178661, 'num_leaves': 33, 'min_child_samples': 37, 'reg_alpha': 0.7031820832689104, 'reg_lambda': 0.6641423301407695}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:03,466] Trial 81 finished with value: 120.52786427153376 and parameters: {'n_estimators': 482, 'max_depth': 7, 'learning_rate': 0.01221279733278692, 'num_leaves': 31, 'min_child_samples': 27, 'reg_alpha': 0.8053625014901448, 'reg_lambda': 0.746841499663505}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:05,353] Trial 82 finished with value: 119.4376523412248 and parameters: {'n_estimators': 598, 'max_depth': 7, 'learning_rate': 0.010941688592121928, 'num_leaves': 31, 'min_child_samples': 27, 'reg_alpha': 0.7354159086325027, 'reg_lambda': 0.7210776775627753}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:07,166] Trial 83 finished with value: 122.7443971591312 and parameters: {'n_estimators': 596, 'max_depth': 7, 'learning_rate': 0.01160942091877269, 'num_leaves': 30, 'min_child_samples': 28, 'reg_alpha': 0.7283576705719882, 'reg_lambda': 0.7238932733103925}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:09,218] Trial 84 finished with value: 127.4954486452935 and parameters: {'n_estimators': 751, 'max_depth': 8, 'learning_rate': 0.029136155183691646, 'num_leaves': 32, 'min_child_samples': 35, 'reg_alpha': 0.7496000332287622, 'reg_lambda': 0.7514397461518882}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:10,838] Trial 85 finished with value: 120.91810117403335 and parameters: {'n_estimators': 493, 'max_depth': 9, 'learning_rate': 0.013510627735513718, 'num_leaves': 29, 'min_child_samples': 31, 'reg_alpha': 0.7878443276381033, 'reg_lambda': 0.777501969579533}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:12,032] Trial 86 finished with value: 124.92613513903554 and parameters: {'n_estimators': 502, 'max_depth': 7, 'learning_rate': 0.010747858442494755, 'num_leaves': 29, 'min_child_samples': 46, 'reg_alpha': 0.8022586940139402, 'reg_lambda': 0.7742530421081918}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:13,962] Trial 87 finished with value: 122.50162968455592 and parameters: {'n_estimators': 592, 'max_depth': 9, 'learning_rate': 0.01099028856455458, 'num_leaves': 31, 'min_child_samples': 31, 'reg_alpha': 0.7870899893585582, 'reg_lambda': 0.7340863443161576}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:15,042] Trial 88 finished with value: 129.1482144350035 and parameters: {'n_estimators': 402, 'max_depth': 8, 'learning_rate': 0.051473624274611246, 'num_leaves': 29, 'min_child_samples': 35, 'reg_alpha': 0.7543716715062776, 'reg_lambda': 0.808210258164175}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:17,136] Trial 89 finished with value: 129.24900256290778 and parameters: {'n_estimators': 791, 'max_depth': 7, 'learning_rate': 0.029706907280840546, 'num_leaves': 37, 'min_child_samples': 28, 'reg_alpha': 0.8554542514674949, 'reg_lambda': 0.6889347675899083}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:19,331] Trial 90 finished with value: 140.2882829230584 and parameters: {'n_estimators': 684, 'max_depth': 9, 'learning_rate': 0.04632384827387877, 'num_leaves': 31, 'min_child_samples': 26, 'reg_alpha': 0.7701709962674523, 'reg_lambda': 0.6477823850872746}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:20,541] Trial 91 finished with value: 127.51716818845293 and parameters: {'n_estimators': 475, 'max_depth': 6, 'learning_rate': 0.0332339688144192, 'num_leaves': 33, 'min_child_samples': 22, 'reg_alpha': 0.7144543001768673, 'reg_lambda': 0.7592825358173922}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:21,141] Trial 92 finished with value: 122.04800035653315 and parameters: {'n_estimators': 272, 'max_depth': 8, 'learning_rate': 0.020229747532746034, 'num_leaves': 28, 'min_child_samples': 64, 'reg_alpha': 0.6741205110528232, 'reg_lambda': 0.7073383849958748}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:21,661] Trial 93 finished with value: 439.12040466280126 and parameters: {'n_estimators': 266, 'max_depth': 9, 'learning_rate': 0.0008977274483566201, 'num_leaves': 27, 'min_child_samples': 70, 'reg_alpha': 0.666234232629848, 'reg_lambda': 0.7127462193574908}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:22,780] Trial 94 finished with value: 127.54229239200419 and parameters: {'n_estimators': 531, 'max_depth': 7, 'learning_rate': 0.021649471121184684, 'num_leaves': 29, 'min_child_samples': 55, 'reg_alpha': 0.8016373380922983, 'reg_lambda': 0.7846030227016123}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:23,610] Trial 95 finished with value: 122.92532273690735 and parameters: {'n_estimators': 381, 'max_depth': 8, 'learning_rate': 0.012236519675257405, 'num_leaves': 35, 'min_child_samples': 63, 'reg_alpha': 0.8240918026926854, 'reg_lambda': 0.733579363521262}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:24,522] Trial 96 finished with value: 135.61721339895064 and parameters: {'n_estimators': 431, 'max_depth': 6, 'learning_rate': 0.05146449877518813, 'num_leaves': 28, 'min_child_samples': 41, 'reg_alpha': 0.7275788882434657, 'reg_lambda': 0.6900427678863745}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:25,456] Trial 97 finished with value: 127.98016879895869 and parameters: {'n_estimators': 346, 'max_depth': 8, 'learning_rate': 0.0656602997680339, 'num_leaves': 30, 'min_child_samples': 33, 'reg_alpha': 0.7695703413902576, 'reg_lambda': 0.8223836168295238}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:26,490] Trial 98 finished with value: 128.71374425218346 and parameters: {'n_estimators': 606, 'max_depth': 7, 'learning_rate': 0.024818344403801512, 'num_leaves': 32, 'min_child_samples': 82, 'reg_alpha': 0.740764097240042, 'reg_lambda': 0.7756904251140286}. Best is trial 78 with value: 119.26285354435943.\n",
      "[I 2023-10-29 05:06:28,899] Trial 99 finished with value: 131.24205105040835 and parameters: {'n_estimators': 678, 'max_depth': 9, 'learning_rate': 0.03711679117862132, 'num_leaves': 35, 'min_child_samples': 20, 'reg_alpha': 0.804751782729701, 'reg_lambda': 0.7027312098401878}. Best is trial 78 with value: 119.26285354435943.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(lambda trial: objective(trial, x_df12_3, y_df12_3), n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-29 13:36:13,601] A new study created in memory with name: no-name-b1d4f209-b223-4143-86f2-d344e36e1a45\n",
      "[I 2023-10-29 13:36:15,653] Trial 0 finished with value: 92.83057193501789 and parameters: {'n_estimators': 612, 'max_depth': 14, 'learning_rate': 0.12515245113275905, 'num_leaves': 42, 'min_child_samples': 39, 'reg_alpha': 0.7859672447500442, 'reg_lambda': 0.6233576286391087}. Best is trial 0 with value: 92.83057193501789.\n",
      "[I 2023-10-29 13:36:17,004] Trial 1 finished with value: 100.12959008406303 and parameters: {'n_estimators': 307, 'max_depth': 13, 'learning_rate': 0.08224706272709245, 'num_leaves': 24, 'min_child_samples': 12, 'reg_alpha': 0.53174442336726, 'reg_lambda': 0.9959888013043962}. Best is trial 0 with value: 92.83057193501789.\n",
      "[I 2023-10-29 13:36:17,449] Trial 2 finished with value: 92.22211724550773 and parameters: {'n_estimators': 207, 'max_depth': 15, 'learning_rate': 0.27197694857496957, 'num_leaves': 30, 'min_child_samples': 62, 'reg_alpha': 0.922032714587479, 'reg_lambda': 0.09587773968640015}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:19,094] Trial 3 finished with value: 116.59619485903683 and parameters: {'n_estimators': 758, 'max_depth': 6, 'learning_rate': 0.4599950690403356, 'num_leaves': 22, 'min_child_samples': 24, 'reg_alpha': 0.1166756096806368, 'reg_lambda': 0.7807555183291418}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:21,497] Trial 4 finished with value: 134.084966459696 and parameters: {'n_estimators': 1299, 'max_depth': 15, 'learning_rate': 0.2925068919738223, 'num_leaves': 32, 'min_child_samples': 10, 'reg_alpha': 0.042846701916982984, 'reg_lambda': 0.8759292016756587}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:23,245] Trial 5 finished with value: 99.29707522883832 and parameters: {'n_estimators': 1033, 'max_depth': 9, 'learning_rate': 0.09012931188698801, 'num_leaves': 24, 'min_child_samples': 80, 'reg_alpha': 0.19966046862075437, 'reg_lambda': 0.6543163716728158}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:34,891] Trial 6 finished with value: 101.95703518962266 and parameters: {'n_estimators': 1970, 'max_depth': 16, 'learning_rate': 0.019664983899913804, 'num_leaves': 47, 'min_child_samples': 12, 'reg_alpha': 0.4755865573550849, 'reg_lambda': 0.5952113749310052}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:35,366] Trial 7 finished with value: 104.73929799583605 and parameters: {'n_estimators': 372, 'max_depth': 3, 'learning_rate': 0.16233601242514337, 'num_leaves': 17, 'min_child_samples': 7, 'reg_alpha': 0.9645385331637161, 'reg_lambda': 0.98734891876687}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:37,871] Trial 8 finished with value: 96.41827947350933 and parameters: {'n_estimators': 1502, 'max_depth': 11, 'learning_rate': 0.023460247374851452, 'num_leaves': 21, 'min_child_samples': 80, 'reg_alpha': 0.8269457732586342, 'reg_lambda': 0.8093119642666273}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:40,082] Trial 9 finished with value: 95.40405764775902 and parameters: {'n_estimators': 767, 'max_depth': 12, 'learning_rate': 0.08384526745687247, 'num_leaves': 20, 'min_child_samples': 27, 'reg_alpha': 0.21997856345615946, 'reg_lambda': 0.5227616683742742}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:40,207] Trial 10 finished with value: 98.30830085853333 and parameters: {'n_estimators': 76, 'max_depth': 20, 'learning_rate': 0.2778942414791799, 'num_leaves': 5, 'min_child_samples': 62, 'reg_alpha': 0.9911484293843822, 'reg_lambda': 0.06147866170619625}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:41,395] Trial 11 finished with value: 101.22326830708268 and parameters: {'n_estimators': 475, 'max_depth': 18, 'learning_rate': 0.20710191510542672, 'num_leaves': 39, 'min_child_samples': 52, 'reg_alpha': 0.7839090420912121, 'reg_lambda': 0.3145986488755985}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:41,716] Trial 12 finished with value: 117.46494125233039 and parameters: {'n_estimators': 115, 'max_depth': 15, 'learning_rate': 0.33934866832834176, 'num_leaves': 38, 'min_child_samples': 49, 'reg_alpha': 0.733581018937483, 'reg_lambda': 0.3398991405143449}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:42,653] Trial 13 finished with value: 105.83474257019634 and parameters: {'n_estimators': 665, 'max_depth': 9, 'learning_rate': 0.1983265487245622, 'num_leaves': 50, 'min_child_samples': 97, 'reg_alpha': 0.6649622573306391, 'reg_lambda': 0.052669258056798895}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:44,343] Trial 14 finished with value: 105.01855031411215 and parameters: {'n_estimators': 1021, 'max_depth': 17, 'learning_rate': 0.36477973742551595, 'num_leaves': 35, 'min_child_samples': 35, 'reg_alpha': 0.8858168794022458, 'reg_lambda': 0.38554779133485306}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:45,273] Trial 15 finished with value: 102.83358845969953 and parameters: {'n_estimators': 476, 'max_depth': 13, 'learning_rate': 0.15563149703477525, 'num_leaves': 44, 'min_child_samples': 69, 'reg_alpha': 0.8729059372569741, 'reg_lambda': 0.14390707224162663}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:45,965] Trial 16 finished with value: 105.06551191899094 and parameters: {'n_estimators': 216, 'max_depth': 19, 'learning_rate': 0.2400300265475379, 'num_leaves': 31, 'min_child_samples': 39, 'reg_alpha': 0.6451150446499809, 'reg_lambda': 0.20022558305366367}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:47,025] Trial 17 finished with value: 94.95823549272869 and parameters: {'n_estimators': 639, 'max_depth': 10, 'learning_rate': 0.16396596094446525, 'num_leaves': 11, 'min_child_samples': 61, 'reg_alpha': 0.9882839005905031, 'reg_lambda': 0.45219621742527644}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:49,785] Trial 18 finished with value: 101.27110451875022 and parameters: {'n_estimators': 1260, 'max_depth': 14, 'learning_rate': 0.24649383423142665, 'num_leaves': 30, 'min_child_samples': 44, 'reg_alpha': 0.7553812720911509, 'reg_lambda': 0.22136313733923274}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:49,909] Trial 19 finished with value: 105.3502511444531 and parameters: {'n_estimators': 47, 'max_depth': 6, 'learning_rate': 0.3272268769231969, 'num_leaves': 42, 'min_child_samples': 76, 'reg_alpha': 0.8883768476175578, 'reg_lambda': 0.4997759216517758}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:51,186] Trial 20 finished with value: 117.89468482388673 and parameters: {'n_estimators': 889, 'max_depth': 17, 'learning_rate': 0.38312100936153054, 'num_leaves': 28, 'min_child_samples': 95, 'reg_alpha': 0.665102444416172, 'reg_lambda': 0.015675827325800903}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:52,362] Trial 21 finished with value: 96.88303313857783 and parameters: {'n_estimators': 636, 'max_depth': 10, 'learning_rate': 0.1429370485149575, 'num_leaves': 12, 'min_child_samples': 59, 'reg_alpha': 0.9827524992186141, 'reg_lambda': 0.4304670674926466}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:52,908] Trial 22 finished with value: 103.38646511680773 and parameters: {'n_estimators': 522, 'max_depth': 7, 'learning_rate': 0.19997857911042927, 'num_leaves': 5, 'min_child_samples': 60, 'reg_alpha': 0.9994026824658465, 'reg_lambda': 0.2864006792771169}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:53,518] Trial 23 finished with value: 92.63865068730053 and parameters: {'n_estimators': 349, 'max_depth': 12, 'learning_rate': 0.12717474369180773, 'num_leaves': 12, 'min_child_samples': 71, 'reg_alpha': 0.8908842819276193, 'reg_lambda': 0.4364185140143836}. Best is trial 2 with value: 92.22211724550773.\n",
      "[I 2023-10-29 13:36:54,007] Trial 24 finished with value: 90.66078557034284 and parameters: {'n_estimators': 285, 'max_depth': 14, 'learning_rate': 0.11665511290789252, 'num_leaves': 16, 'min_child_samples': 87, 'reg_alpha': 0.8522141573098163, 'reg_lambda': 0.6309510479756439}. Best is trial 24 with value: 90.66078557034284.\n",
      "[I 2023-10-29 13:36:54,451] Trial 25 finished with value: 92.26565849822768 and parameters: {'n_estimators': 259, 'max_depth': 12, 'learning_rate': 0.10611602141593397, 'num_leaves': 13, 'min_child_samples': 85, 'reg_alpha': 0.8957790123660108, 'reg_lambda': 0.12721019259140245}. Best is trial 24 with value: 90.66078557034284.\n",
      "[I 2023-10-29 13:36:54,836] Trial 26 finished with value: 90.47523695747707 and parameters: {'n_estimators': 234, 'max_depth': 16, 'learning_rate': 0.058813659383822395, 'num_leaves': 17, 'min_child_samples': 90, 'reg_alpha': 0.8457731391789797, 'reg_lambda': 0.18253544754665646}. Best is trial 26 with value: 90.47523695747707.\n",
      "[I 2023-10-29 13:36:54,964] Trial 27 finished with value: 258.0145198334638 and parameters: {'n_estimators': 161, 'max_depth': 16, 'learning_rate': 0.04192853199438029, 'num_leaves': 2, 'min_child_samples': 89, 'reg_alpha': 0.8349192222660329, 'reg_lambda': 0.24855444224872916}. Best is trial 26 with value: 90.47523695747707.\n",
      "[I 2023-10-29 13:36:55,061] Trial 28 finished with value: 145.47168811122614 and parameters: {'n_estimators': 34, 'max_depth': 18, 'learning_rate': 0.05406278078195734, 'num_leaves': 18, 'min_child_samples': 100, 'reg_alpha': 0.7207834492854727, 'reg_lambda': 0.002817326670832815}. Best is trial 26 with value: 90.47523695747707.\n",
      "[I 2023-10-29 13:36:57,711] Trial 29 finished with value: 112.0690486413306 and parameters: {'n_estimators': 1800, 'max_depth': 14, 'learning_rate': 0.0017419878793995258, 'num_leaves': 16, 'min_child_samples': 93, 'reg_alpha': 0.9256331155714064, 'reg_lambda': 0.13927025621019162}. Best is trial 26 with value: 90.47523695747707.\n",
      "[I 2023-10-29 13:36:58,397] Trial 30 finished with value: 97.04650655337208 and parameters: {'n_estimators': 416, 'max_depth': 15, 'learning_rate': 0.11379681683089732, 'num_leaves': 26, 'min_child_samples': 87, 'reg_alpha': 0.8532669120181249, 'reg_lambda': 0.3645030135705759}. Best is trial 26 with value: 90.47523695747707.\n",
      "[I 2023-10-29 13:36:58,807] Trial 31 finished with value: 87.66759081414612 and parameters: {'n_estimators': 227, 'max_depth': 13, 'learning_rate': 0.06627141783471381, 'num_leaves': 14, 'min_child_samples': 85, 'reg_alpha': 0.798779123724671, 'reg_lambda': 0.13062424619256963}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:36:59,176] Trial 32 finished with value: 89.31078686130144 and parameters: {'n_estimators': 227, 'max_depth': 13, 'learning_rate': 0.05049420802057342, 'num_leaves': 9, 'min_child_samples': 71, 'reg_alpha': 0.7999179675751833, 'reg_lambda': 0.18757255523519767}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:36:59,588] Trial 33 finished with value: 90.62386305788675 and parameters: {'n_estimators': 285, 'max_depth': 13, 'learning_rate': 0.05501537786376724, 'num_leaves': 8, 'min_child_samples': 74, 'reg_alpha': 0.7925957384359075, 'reg_lambda': 0.18479673669327595}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:36:59,881] Trial 34 finished with value: 89.67954003108444 and parameters: {'n_estimators': 185, 'max_depth': 13, 'learning_rate': 0.06059814751150161, 'num_leaves': 8, 'min_child_samples': 69, 'reg_alpha': 0.7995906380097798, 'reg_lambda': 0.1788516252527319}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:00,183] Trial 35 finished with value: 88.47615598367105 and parameters: {'n_estimators': 190, 'max_depth': 11, 'learning_rate': 0.07348107642093003, 'num_leaves': 8, 'min_child_samples': 69, 'reg_alpha': 0.5918466940977082, 'reg_lambda': 0.262574058761845}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:00,908] Trial 36 finished with value: 91.05017591394177 and parameters: {'n_estimators': 515, 'max_depth': 11, 'learning_rate': 0.07792740538480396, 'num_leaves': 8, 'min_child_samples': 65, 'reg_alpha': 0.5578668888713249, 'reg_lambda': 0.2597975013408809}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:01,188] Trial 37 finished with value: 268.7386097973946 and parameters: {'n_estimators': 165, 'max_depth': 8, 'learning_rate': 0.0023200586978902454, 'num_leaves': 9, 'min_child_samples': 79, 'reg_alpha': 0.6116796335511496, 'reg_lambda': 0.2816559103858995}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:01,806] Trial 38 finished with value: 103.54228064374931 and parameters: {'n_estimators': 800, 'max_depth': 10, 'learning_rate': 0.09166720044098267, 'num_leaves': 3, 'min_child_samples': 56, 'reg_alpha': 0.7119852727742843, 'reg_lambda': 0.08367672644382473}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:02,260] Trial 39 finished with value: 99.39252943322151 and parameters: {'n_estimators': 371, 'max_depth': 13, 'learning_rate': 0.022836206194820935, 'num_leaves': 6, 'min_child_samples': 68, 'reg_alpha': 0.7944307807275806, 'reg_lambda': 0.11935321982619362}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:03,179] Trial 40 finished with value: 92.70160983470744 and parameters: {'n_estimators': 567, 'max_depth': 11, 'learning_rate': 0.07145002152025524, 'num_leaves': 14, 'min_child_samples': 83, 'reg_alpha': 0.4969320739291775, 'reg_lambda': 0.22179650185096766}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:03,517] Trial 41 finished with value: 88.72888843812053 and parameters: {'n_estimators': 202, 'max_depth': 16, 'learning_rate': 0.04486179288658186, 'num_leaves': 10, 'min_child_samples': 74, 'reg_alpha': 0.8109237923552596, 'reg_lambda': 0.19161603026268728}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:03,735] Trial 42 finished with value: 99.6367231570646 and parameters: {'n_estimators': 125, 'max_depth': 13, 'learning_rate': 0.03930135493653209, 'num_leaves': 9, 'min_child_samples': 75, 'reg_alpha': 0.7522118093226069, 'reg_lambda': 0.15334570093682806}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:03,816] Trial 43 finished with value: 162.6318480951288 and parameters: {'n_estimators': 25, 'max_depth': 14, 'learning_rate': 0.09008213817980479, 'num_leaves': 6, 'min_child_samples': 66, 'reg_alpha': 0.934679050784211, 'reg_lambda': 0.23264387486298066}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:04,439] Trial 44 finished with value: 89.23403489948231 and parameters: {'n_estimators': 380, 'max_depth': 12, 'learning_rate': 0.0327068504916647, 'num_leaves': 10, 'min_child_samples': 72, 'reg_alpha': 0.8017612166624623, 'reg_lambda': 0.07877348963430517}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:05,152] Trial 45 finished with value: 89.67468255618907 and parameters: {'n_estimators': 426, 'max_depth': 9, 'learning_rate': 0.030675710267908822, 'num_leaves': 14, 'min_child_samples': 81, 'reg_alpha': 0.680076702111032, 'reg_lambda': 0.08902507344644384}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:05,588] Trial 46 finished with value: 114.05464496184726 and parameters: {'n_estimators': 313, 'max_depth': 4, 'learning_rate': 0.018501723021786903, 'num_leaves': 23, 'min_child_samples': 74, 'reg_alpha': 0.6036383169129447, 'reg_lambda': 0.04950189659188339}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:07,495] Trial 47 finished with value: 97.35930272766824 and parameters: {'n_estimators': 1237, 'max_depth': 12, 'learning_rate': 0.07452249421538557, 'num_leaves': 10, 'min_child_samples': 79, 'reg_alpha': 0.707764504150296, 'reg_lambda': 0.3003417850839588}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:07,811] Trial 48 finished with value: 124.88269017992936 and parameters: {'n_estimators': 393, 'max_depth': 11, 'learning_rate': 0.042887732547944354, 'num_leaves': 3, 'min_child_samples': 53, 'reg_alpha': 0.8068996385679761, 'reg_lambda': 0.10466718545187043}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:08,937] Trial 49 finished with value: 98.4450322679836 and parameters: {'n_estimators': 1125, 'max_depth': 15, 'learning_rate': 0.09420941746960758, 'num_leaves': 5, 'min_child_samples': 64, 'reg_alpha': 0.7657186811460956, 'reg_lambda': 0.32853351120374485}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:10,111] Trial 50 finished with value: 92.9369238438783 and parameters: {'n_estimators': 716, 'max_depth': 16, 'learning_rate': 0.007971898731832426, 'num_leaves': 19, 'min_child_samples': 77, 'reg_alpha': 0.4546986835784309, 'reg_lambda': 0.16176873512421525}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:10,794] Trial 51 finished with value: 89.42776343108504 and parameters: {'n_estimators': 426, 'max_depth': 8, 'learning_rate': 0.03705114473085197, 'num_leaves': 13, 'min_child_samples': 81, 'reg_alpha': 0.688236687910516, 'reg_lambda': 0.0841831362453902}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:11,056] Trial 52 finished with value: 98.8023826399586 and parameters: {'n_estimators': 132, 'max_depth': 8, 'learning_rate': 0.031577016326546886, 'num_leaves': 15, 'min_child_samples': 71, 'reg_alpha': 0.7493201085306098, 'reg_lambda': 0.0554397196474551}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:11,422] Trial 53 finished with value: 89.33468845615425 and parameters: {'n_estimators': 242, 'max_depth': 5, 'learning_rate': 0.06606631824836803, 'num_leaves': 11, 'min_child_samples': 84, 'reg_alpha': 0.8180567657719294, 'reg_lambda': 0.1087130094301683}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:11,709] Trial 54 finished with value: 102.64956793244214 and parameters: {'n_estimators': 248, 'max_depth': 3, 'learning_rate': 0.064073483472044, 'num_leaves': 10, 'min_child_samples': 91, 'reg_alpha': 0.9401869267700004, 'reg_lambda': 0.21864273858389247}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:11,904] Trial 55 finished with value: 90.41318246808783 and parameters: {'n_estimators': 118, 'max_depth': 6, 'learning_rate': 0.09722552318001106, 'num_leaves': 7, 'min_child_samples': 85, 'reg_alpha': 0.8099175922229653, 'reg_lambda': 0.13116370338780564}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:14,603] Trial 56 finished with value: 100.29896330518389 and parameters: {'n_estimators': 1570, 'max_depth': 19, 'learning_rate': 0.1262742257537374, 'num_leaves': 11, 'min_child_samples': 18, 'reg_alpha': 0.7681423115192708, 'reg_lambda': 0.183973781111298}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:15,126] Trial 57 finished with value: 90.90212861186599 and parameters: {'n_estimators': 335, 'max_depth': 5, 'learning_rate': 0.019123281786673205, 'num_leaves': 20, 'min_child_samples': 73, 'reg_alpha': 0.860540889071596, 'reg_lambda': 0.032095265431401565}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:15,408] Trial 58 finished with value: 98.3803730089106 and parameters: {'n_estimators': 210, 'max_depth': 4, 'learning_rate': 0.08599412539059481, 'num_leaves': 11, 'min_child_samples': 97, 'reg_alpha': 0.8259052508753011, 'reg_lambda': 0.2570494338277487}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:15,526] Trial 59 finished with value: 201.0220174889956 and parameters: {'n_estimators': 87, 'max_depth': 17, 'learning_rate': 0.05310227583143182, 'num_leaves': 3, 'min_child_samples': 48, 'reg_alpha': 0.9078294352293925, 'reg_lambda': 0.09586270863046245}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:16,082] Trial 60 finished with value: 96.57954279904011 and parameters: {'n_estimators': 476, 'max_depth': 10, 'learning_rate': 0.06903253767823334, 'num_leaves': 6, 'min_child_samples': 83, 'reg_alpha': 0.724168616825132, 'reg_lambda': 0.032380854633185185}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:16,791] Trial 61 finished with value: 90.93498017064003 and parameters: {'n_estimators': 439, 'max_depth': 8, 'learning_rate': 0.03861176894836263, 'num_leaves': 13, 'min_child_samples': 78, 'reg_alpha': 0.6904131298045894, 'reg_lambda': 0.06863110442540177}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:17,642] Trial 62 finished with value: 89.34167812218968 and parameters: {'n_estimators': 558, 'max_depth': 7, 'learning_rate': 0.015939200576444722, 'num_leaves': 13, 'min_child_samples': 83, 'reg_alpha': 0.7731765972655531, 'reg_lambda': 0.11275485739202877}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:18,486] Trial 63 finished with value: 88.91359300662108 and parameters: {'n_estimators': 562, 'max_depth': 5, 'learning_rate': 0.012354617424246199, 'num_leaves': 16, 'min_child_samples': 71, 'reg_alpha': 0.8711925891289208, 'reg_lambda': 0.12818416319036036}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:18,972] Trial 64 finished with value: 310.9511545120451 and parameters: {'n_estimators': 312, 'max_depth': 5, 'learning_rate': 0.0005728384731783984, 'num_leaves': 16, 'min_child_samples': 71, 'reg_alpha': 0.9591293726663881, 'reg_lambda': 0.19751810040579232}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:20,432] Trial 65 finished with value: 98.09257792288311 and parameters: {'n_estimators': 893, 'max_depth': 12, 'learning_rate': 0.10741999044932898, 'num_leaves': 10, 'min_child_samples': 58, 'reg_alpha': 0.8725016096128058, 'reg_lambda': 0.14655418210799803}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:20,840] Trial 66 finished with value: 91.06567936537365 and parameters: {'n_estimators': 221, 'max_depth': 5, 'learning_rate': 0.04794026009976462, 'num_leaves': 21, 'min_child_samples': 68, 'reg_alpha': 0.8349179676567109, 'reg_lambda': 0.16532660886987796}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:21,374] Trial 67 finished with value: 93.02552140891618 and parameters: {'n_estimators': 274, 'max_depth': 7, 'learning_rate': 0.0762277930218077, 'num_leaves': 15, 'min_child_samples': 63, 'reg_alpha': 0.8873527374088688, 'reg_lambda': 0.20797213544202647}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:21,834] Trial 68 finished with value: 103.2828837746375 and parameters: {'n_estimators': 352, 'max_depth': 4, 'learning_rate': 0.02563306238763771, 'num_leaves': 18, 'min_child_samples': 87, 'reg_alpha': 0.8218647527665549, 'reg_lambda': 0.11973886529082091}. Best is trial 31 with value: 87.66759081414612.\n",
      "[I 2023-10-29 13:37:22,157] Trial 69 finished with value: 86.80335195420442 and parameters: {'n_estimators': 178, 'max_depth': 20, 'learning_rate': 0.05620086250558467, 'num_leaves': 12, 'min_child_samples': 76, 'reg_alpha': 0.91446074553082, 'reg_lambda': 0.006155406301766017}. Best is trial 69 with value: 86.80335195420442.\n",
      "[I 2023-10-29 13:37:22,325] Trial 70 finished with value: 116.52368163990243 and parameters: {'n_estimators': 72, 'max_depth': 19, 'learning_rate': 0.04780140749179822, 'num_leaves': 8, 'min_child_samples': 71, 'reg_alpha': 0.9584661842122937, 'reg_lambda': 0.03585253985064611}. Best is trial 69 with value: 86.80335195420442.\n",
      "[I 2023-10-29 13:37:22,633] Trial 71 finished with value: 85.79560737045892 and parameters: {'n_estimators': 158, 'max_depth': 18, 'learning_rate': 0.06371970568039746, 'num_leaves': 12, 'min_child_samples': 75, 'reg_alpha': 0.863282252201884, 'reg_lambda': 8.132417850026386e-06}. Best is trial 71 with value: 85.79560737045892.\n",
      "[I 2023-10-29 13:37:22,946] Trial 72 finished with value: 109.65267489568794 and parameters: {'n_estimators': 165, 'max_depth': 20, 'learning_rate': 0.01866563330679124, 'num_leaves': 15, 'min_child_samples': 77, 'reg_alpha': 0.9123193722311279, 'reg_lambda': 0.0019410331648765213}. Best is trial 71 with value: 85.79560737045892.\n",
      "[I 2023-10-29 13:37:23,307] Trial 73 finished with value: 90.64979639124809 and parameters: {'n_estimators': 187, 'max_depth': 18, 'learning_rate': 0.05782350405577549, 'num_leaves': 12, 'min_child_samples': 66, 'reg_alpha': 0.8865853965514264, 'reg_lambda': 0.07126843600119757}. Best is trial 71 with value: 85.79560737045892.\n",
      "[I 2023-10-29 13:37:23,508] Trial 74 finished with value: 93.82300377667524 and parameters: {'n_estimators': 100, 'max_depth': 19, 'learning_rate': 0.08206431635360979, 'num_leaves': 9, 'min_child_samples': 73, 'reg_alpha': 0.8621534639218424, 'reg_lambda': 0.01738417984738935}. Best is trial 71 with value: 85.79560737045892.\n",
      "[I 2023-10-29 13:37:23,932] Trial 75 finished with value: 93.805981189538 and parameters: {'n_estimators': 299, 'max_depth': 18, 'learning_rate': 0.13721785480310972, 'num_leaves': 7, 'min_child_samples': 61, 'reg_alpha': 0.9214053560589313, 'reg_lambda': 0.058645048863214644}. Best is trial 71 with value: 85.79560737045892.\n",
      "[I 2023-10-29 13:37:24,024] Trial 76 finished with value: 193.75185113219484 and parameters: {'n_estimators': 32, 'max_depth': 20, 'learning_rate': 0.10373162972220132, 'num_leaves': 4, 'min_child_samples': 76, 'reg_alpha': 0.7788163454662844, 'reg_lambda': 0.14982554555193867}. Best is trial 71 with value: 85.79560737045892.\n",
      "[I 2023-10-29 13:37:24,719] Trial 77 finished with value: 91.76191015071373 and parameters: {'n_estimators': 361, 'max_depth': 17, 'learning_rate': 0.02841871603132487, 'num_leaves': 17, 'min_child_samples': 69, 'reg_alpha': 0.8467937141642028, 'reg_lambda': 0.0065925673284148445}. Best is trial 71 with value: 85.79560737045892.\n",
      "[I 2023-10-29 13:37:25,034] Trial 78 finished with value: 139.58624368631445 and parameters: {'n_estimators': 172, 'max_depth': 14, 'learning_rate': 0.010509209520500183, 'num_leaves': 14, 'min_child_samples': 80, 'reg_alpha': 0.9042950483026166, 'reg_lambda': 0.07690962947075827}. Best is trial 71 with value: 85.79560737045892.\n",
      "[I 2023-10-29 13:37:25,991] Trial 79 finished with value: 92.12675300625997 and parameters: {'n_estimators': 590, 'max_depth': 17, 'learning_rate': 0.04796828147081056, 'num_leaves': 12, 'min_child_samples': 73, 'reg_alpha': 0.7426818676284089, 'reg_lambda': 0.04023879974931203}. Best is trial 71 with value: 85.79560737045892.\n",
      "[I 2023-10-29 13:37:27,208] Trial 80 finished with value: 94.41920936577526 and parameters: {'n_estimators': 530, 'max_depth': 20, 'learning_rate': 0.11413865063279993, 'num_leaves': 36, 'min_child_samples': 58, 'reg_alpha': 0.9693607082015285, 'reg_lambda': 0.17083376997510813}. Best is trial 71 with value: 85.79560737045892.\n",
      "[I 2023-10-29 13:37:27,590] Trial 81 finished with value: 87.42010615044468 and parameters: {'n_estimators': 238, 'max_depth': 16, 'learning_rate': 0.061350248078068124, 'num_leaves': 10, 'min_child_samples': 85, 'reg_alpha': 0.8101390305114426, 'reg_lambda': 0.10009905408365732}. Best is trial 71 with value: 85.79560737045892.\n",
      "[I 2023-10-29 13:37:27,973] Trial 82 finished with value: 84.23491331615064 and parameters: {'n_estimators': 256, 'max_depth': 15, 'learning_rate': 0.06565765832030765, 'num_leaves': 9, 'min_child_samples': 92, 'reg_alpha': 0.8676499876674159, 'reg_lambda': 0.12931172139386432}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:28,328] Trial 83 finished with value: 90.21417438108361 and parameters: {'n_estimators': 271, 'max_depth': 16, 'learning_rate': 0.07582257999056956, 'num_leaves': 7, 'min_child_samples': 94, 'reg_alpha': 0.860963845139161, 'reg_lambda': 0.09636833514727865}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:29,027] Trial 84 finished with value: 89.89210471916591 and parameters: {'n_estimators': 476, 'max_depth': 16, 'learning_rate': 0.06345787041539153, 'num_leaves': 10, 'min_child_samples': 89, 'reg_alpha': 0.9445408330586469, 'reg_lambda': 0.12588388349696467}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:29,218] Trial 85 finished with value: 146.9949512915926 and parameters: {'n_estimators': 136, 'max_depth': 15, 'learning_rate': 0.03215216441348304, 'num_leaves': 5, 'min_child_samples': 92, 'reg_alpha': 0.837694779577957, 'reg_lambda': 0.06298925002619668}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:29,382] Trial 86 finished with value: 91.85064056681865 and parameters: {'n_estimators': 70, 'max_depth': 14, 'learning_rate': 0.08830636896466887, 'num_leaves': 12, 'min_child_samples': 97, 'reg_alpha': 0.7965896240531558, 'reg_lambda': 0.14095550579280774}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:29,934] Trial 87 finished with value: 89.98249838857969 and parameters: {'n_estimators': 383, 'max_depth': 18, 'learning_rate': 0.10091921351233835, 'num_leaves': 9, 'min_child_samples': 87, 'reg_alpha': 0.8723463189525459, 'reg_lambda': 0.022805707516786917}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:30,341] Trial 88 finished with value: 88.28011541650238 and parameters: {'n_estimators': 220, 'max_depth': 15, 'learning_rate': 0.03698130509116329, 'num_leaves': 14, 'min_child_samples': 82, 'reg_alpha': 0.9942626217273897, 'reg_lambda': 0.08588010667881651}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:30,626] Trial 89 finished with value: 86.65525979713354 and parameters: {'n_estimators': 174, 'max_depth': 15, 'learning_rate': 0.06410380542163671, 'num_leaves': 25, 'min_child_samples': 100, 'reg_alpha': 0.9831868417958635, 'reg_lambda': 0.10684122272261493}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:30,930] Trial 90 finished with value: 88.7278773784396 and parameters: {'n_estimators': 186, 'max_depth': 15, 'learning_rate': 0.06650232211422114, 'num_leaves': 27, 'min_child_samples': 98, 'reg_alpha': 0.9858624579249436, 'reg_lambda': 0.23460876958118615}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:31,246] Trial 91 finished with value: 87.72686353429539 and parameters: {'n_estimators': 203, 'max_depth': 15, 'learning_rate': 0.06656449723088423, 'num_leaves': 27, 'min_child_samples': 99, 'reg_alpha': 0.9880457978841383, 'reg_lambda': 0.20050455485465568}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:31,501] Trial 92 finished with value: 88.05569037680662 and parameters: {'n_estimators': 145, 'max_depth': 15, 'learning_rate': 0.06279683507917688, 'num_leaves': 29, 'min_child_samples': 99, 'reg_alpha': 0.9921845953540293, 'reg_lambda': 0.23819522204737026}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:31,748] Trial 93 finished with value: 86.1936391214557 and parameters: {'n_estimators': 138, 'max_depth': 15, 'learning_rate': 0.08257255361493829, 'num_leaves': 29, 'min_child_samples': 100, 'reg_alpha': 0.9892283776425118, 'reg_lambda': 0.10142549651163718}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:31,837] Trial 94 finished with value: 144.8161738056463 and parameters: {'n_estimators': 21, 'max_depth': 15, 'learning_rate': 0.08190537068164476, 'num_leaves': 29, 'min_child_samples': 100, 'reg_alpha': 0.9758079099500261, 'reg_lambda': 0.09831296936303606}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:32,065] Trial 95 finished with value: 89.2538304333472 and parameters: {'n_estimators': 121, 'max_depth': 14, 'learning_rate': 0.05861154037823697, 'num_leaves': 32, 'min_child_samples': 95, 'reg_alpha': 0.998976852125014, 'reg_lambda': 0.040394622823626526}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:32,452] Trial 96 finished with value: 88.2990186969717 and parameters: {'n_estimators': 256, 'max_depth': 15, 'learning_rate': 0.09824053676272287, 'num_leaves': 25, 'min_child_samples': 100, 'reg_alpha': 0.942220949841067, 'reg_lambda': 0.05432750600572039}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:32,631] Trial 97 finished with value: 88.938825055262 and parameters: {'n_estimators': 83, 'max_depth': 16, 'learning_rate': 0.12214065157063364, 'num_leaves': 33, 'min_child_samples': 95, 'reg_alpha': 0.9973702007435696, 'reg_lambda': 0.15727948483586385}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:33,158] Trial 98 finished with value: 87.59136013208372 and parameters: {'n_estimators': 314, 'max_depth': 17, 'learning_rate': 0.05561776332972245, 'num_leaves': 29, 'min_child_samples': 90, 'reg_alpha': 0.9645071587836886, 'reg_lambda': 0.002692765233534622}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:33,663] Trial 99 finished with value: 92.24867341707912 and parameters: {'n_estimators': 316, 'max_depth': 17, 'learning_rate': 0.08555889413075035, 'num_leaves': 27, 'min_child_samples': 91, 'reg_alpha': 0.9270129740023761, 'reg_lambda': 0.011207948930853057}. Best is trial 82 with value: 84.23491331615064.\n",
      "[I 2023-10-29 13:37:33,740] A new study created in memory with name: no-name-7ee13b4b-df7c-49a8-9e7d-70bfb7ab461c\n",
      "[I 2023-10-29 13:37:35,324] Trial 0 finished with value: 350.6191026770985 and parameters: {'n_estimators': 1539, 'max_depth': 8, 'learning_rate': 0.444211753755641, 'num_leaves': 23, 'min_child_samples': 19, 'reg_alpha': 0.64876413961144, 'reg_lambda': 0.7910429507231056}. Best is trial 0 with value: 350.6191026770985.\n",
      "[I 2023-10-29 13:37:35,972] Trial 1 finished with value: 338.2215461729672 and parameters: {'n_estimators': 231, 'max_depth': 8, 'learning_rate': 0.14373344347006262, 'num_leaves': 22, 'min_child_samples': 22, 'reg_alpha': 0.330454249458822, 'reg_lambda': 0.5837474435150829}. Best is trial 1 with value: 338.2215461729672.\n",
      "[I 2023-10-29 13:37:38,135] Trial 2 finished with value: 358.0491596335296 and parameters: {'n_estimators': 634, 'max_depth': 19, 'learning_rate': 0.22834323130804446, 'num_leaves': 28, 'min_child_samples': 23, 'reg_alpha': 0.6427265556976974, 'reg_lambda': 0.16965678761537342}. Best is trial 1 with value: 338.2215461729672.\n",
      "[I 2023-10-29 13:37:39,204] Trial 3 finished with value: 362.9622856270478 and parameters: {'n_estimators': 834, 'max_depth': 15, 'learning_rate': 0.3302052801952576, 'num_leaves': 17, 'min_child_samples': 94, 'reg_alpha': 0.4060329939574393, 'reg_lambda': 0.4889156065388678}. Best is trial 1 with value: 338.2215461729672.\n",
      "[I 2023-10-29 13:37:39,548] Trial 4 finished with value: 317.9556403258344 and parameters: {'n_estimators': 228, 'max_depth': 14, 'learning_rate': 0.2364077645701531, 'num_leaves': 38, 'min_child_samples': 82, 'reg_alpha': 0.02959695461596698, 'reg_lambda': 0.3112726984876758}. Best is trial 4 with value: 317.9556403258344.\n",
      "[I 2023-10-29 13:37:41,574] Trial 5 finished with value: 332.21124338011566 and parameters: {'n_estimators': 1919, 'max_depth': 17, 'learning_rate': 0.266819231165619, 'num_leaves': 5, 'min_child_samples': 31, 'reg_alpha': 0.9461360196474635, 'reg_lambda': 0.780221095730011}. Best is trial 4 with value: 317.9556403258344.\n",
      "[I 2023-10-29 13:37:43,228] Trial 6 finished with value: 359.13697584500557 and parameters: {'n_estimators': 1030, 'max_depth': 12, 'learning_rate': 0.4586678972482012, 'num_leaves': 49, 'min_child_samples': 75, 'reg_alpha': 0.3812920026792352, 'reg_lambda': 0.248976342458574}. Best is trial 4 with value: 317.9556403258344.\n",
      "[I 2023-10-29 13:37:47,159] Trial 7 finished with value: 323.6544207879567 and parameters: {'n_estimators': 1304, 'max_depth': 12, 'learning_rate': 0.045474573893250846, 'num_leaves': 30, 'min_child_samples': 33, 'reg_alpha': 0.4095564700732223, 'reg_lambda': 0.6334465194047019}. Best is trial 4 with value: 317.9556403258344.\n",
      "[I 2023-10-29 13:37:47,474] Trial 8 finished with value: 289.5259286265763 and parameters: {'n_estimators': 98, 'max_depth': 11, 'learning_rate': 0.03545256125598629, 'num_leaves': 37, 'min_child_samples': 35, 'reg_alpha': 0.5411333135899625, 'reg_lambda': 0.41756870817889685}. Best is trial 8 with value: 289.5259286265763.\n",
      "[I 2023-10-29 13:37:48,598] Trial 9 finished with value: 316.3278189132489 and parameters: {'n_estimators': 376, 'max_depth': 18, 'learning_rate': 0.07160797183728598, 'num_leaves': 23, 'min_child_samples': 35, 'reg_alpha': 0.19907433548040276, 'reg_lambda': 0.929545625597744}. Best is trial 8 with value: 289.5259286265763.\n",
      "[I 2023-10-29 13:37:48,701] Trial 10 finished with value: 537.9976981071013 and parameters: {'n_estimators': 44, 'max_depth': 3, 'learning_rate': 0.00519504149329756, 'num_leaves': 47, 'min_child_samples': 55, 'reg_alpha': 0.6199861123028986, 'reg_lambda': 0.06318064102756205}. Best is trial 8 with value: 289.5259286265763.\n",
      "[I 2023-10-29 13:37:49,929] Trial 11 finished with value: 309.21578714286477 and parameters: {'n_estimators': 468, 'max_depth': 20, 'learning_rate': 0.0849431609534102, 'num_leaves': 38, 'min_child_samples': 46, 'reg_alpha': 0.14514861384184896, 'reg_lambda': 0.9653068017546456}. Best is trial 8 with value: 289.5259286265763.\n",
      "[I 2023-10-29 13:37:50,918] Trial 12 finished with value: 326.5057436431101 and parameters: {'n_estimators': 535, 'max_depth': 7, 'learning_rate': 0.10486522162783266, 'num_leaves': 38, 'min_child_samples': 51, 'reg_alpha': 0.035260710390989825, 'reg_lambda': 0.9864362964653555}. Best is trial 8 with value: 289.5259286265763.\n",
      "[I 2023-10-29 13:37:51,459] Trial 13 finished with value: 372.6934665834277 and parameters: {'n_estimators': 79, 'max_depth': 20, 'learning_rate': 0.009972839808235778, 'num_leaves': 39, 'min_child_samples': 5, 'reg_alpha': 0.19751884707647985, 'reg_lambda': 0.417350667985651}. Best is trial 8 with value: 289.5259286265763.\n",
      "[I 2023-10-29 13:37:53,173] Trial 14 finished with value: 293.6959048703597 and parameters: {'n_estimators': 710, 'max_depth': 15, 'learning_rate': 0.13458747245706681, 'num_leaves': 43, 'min_child_samples': 51, 'reg_alpha': 0.5406979542555265, 'reg_lambda': 0.39858496493958945}. Best is trial 8 with value: 289.5259286265763.\n",
      "[I 2023-10-29 13:37:54,844] Trial 15 finished with value: 312.8411245280354 and parameters: {'n_estimators': 902, 'max_depth': 10, 'learning_rate': 0.12737894211146927, 'num_leaves': 47, 'min_child_samples': 67, 'reg_alpha': 0.550419196294162, 'reg_lambda': 0.3746936858355974}. Best is trial 8 with value: 289.5259286265763.\n",
      "[I 2023-10-29 13:37:57,305] Trial 16 finished with value: 310.39659776948383 and parameters: {'n_estimators': 1219, 'max_depth': 15, 'learning_rate': 0.16714311232782322, 'num_leaves': 33, 'min_child_samples': 63, 'reg_alpha': 0.7683403421674732, 'reg_lambda': 0.4670620116751977}. Best is trial 8 with value: 289.5259286265763.\n",
      "[I 2023-10-29 13:37:58,457] Trial 17 finished with value: 300.43622074457653 and parameters: {'n_estimators': 732, 'max_depth': 5, 'learning_rate': 0.0507308547679399, 'num_leaves': 44, 'min_child_samples': 43, 'reg_alpha': 0.5146629496422306, 'reg_lambda': 0.2939794028562192}. Best is trial 8 with value: 289.5259286265763.\n",
      "[I 2023-10-29 13:38:00,845] Trial 18 finished with value: 315.83265521670086 and parameters: {'n_estimators': 1588, 'max_depth': 10, 'learning_rate': 0.14998197313012654, 'num_leaves': 33, 'min_child_samples': 5, 'reg_alpha': 0.7928277585356878, 'reg_lambda': 0.5463352210199441}. Best is trial 8 with value: 289.5259286265763.\n",
      "[I 2023-10-29 13:38:03,154] Trial 19 finished with value: 315.6924147638311 and parameters: {'n_estimators': 1094, 'max_depth': 14, 'learning_rate': 0.0834219565359107, 'num_leaves': 43, 'min_child_samples': 60, 'reg_alpha': 0.4838575923282777, 'reg_lambda': 0.40152236875238567}. Best is trial 8 with value: 289.5259286265763.\n",
      "[I 2023-10-29 13:38:03,869] Trial 20 finished with value: 324.27972222020594 and parameters: {'n_estimators': 342, 'max_depth': 16, 'learning_rate': 0.1788794639374952, 'num_leaves': 14, 'min_child_samples': 43, 'reg_alpha': 0.29980023145833523, 'reg_lambda': 0.15174907348273736}. Best is trial 8 with value: 289.5259286265763.\n",
      "[I 2023-10-29 13:38:04,625] Trial 21 finished with value: 289.9204164771758 and parameters: {'n_estimators': 726, 'max_depth': 3, 'learning_rate': 0.04928497942778428, 'num_leaves': 44, 'min_child_samples': 44, 'reg_alpha': 0.5044481199688264, 'reg_lambda': 0.29588415127123685}. Best is trial 8 with value: 289.5259286265763.\n",
      "[I 2023-10-29 13:38:05,491] Trial 22 finished with value: 266.8876389647 and parameters: {'n_estimators': 749, 'max_depth': 3, 'learning_rate': 0.0035468191757556944, 'num_leaves': 42, 'min_child_samples': 38, 'reg_alpha': 0.4899896858201212, 'reg_lambda': 0.3423442844925272}. Best is trial 22 with value: 266.8876389647.\n",
      "[I 2023-10-29 13:38:06,149] Trial 23 finished with value: 253.0991206039589 and parameters: {'n_estimators': 588, 'max_depth': 3, 'learning_rate': 0.008496027305196236, 'num_leaves': 35, 'min_child_samples': 38, 'reg_alpha': 0.4614328423973235, 'reg_lambda': 0.24109173104670856}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:07,810] Trial 24 finished with value: 293.6747839923835 and parameters: {'n_estimators': 557, 'max_depth': 5, 'learning_rate': 0.003749941296263868, 'num_leaves': 34, 'min_child_samples': 15, 'reg_alpha': 0.44158985103461484, 'reg_lambda': 0.1844843049258068}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:08,293] Trial 25 finished with value: 276.60270742540047 and parameters: {'n_estimators': 242, 'max_depth': 5, 'learning_rate': 0.032108433966872664, 'num_leaves': 35, 'min_child_samples': 35, 'reg_alpha': 0.56375841425861, 'reg_lambda': 0.022248607979598378}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:08,852] Trial 26 finished with value: 311.04204506733754 and parameters: {'n_estimators': 373, 'max_depth': 4, 'learning_rate': 0.09680511023568625, 'num_leaves': 29, 'min_child_samples': 29, 'reg_alpha': 0.3206091973563663, 'reg_lambda': 0.00021995137002720974}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:12,613] Trial 27 finished with value: 328.16733035504706 and parameters: {'n_estimators': 865, 'max_depth': 6, 'learning_rate': 0.001344284222315125, 'num_leaves': 41, 'min_child_samples': 13, 'reg_alpha': 0.440071241380973, 'reg_lambda': 0.11229875439155754}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:12,988] Trial 28 finished with value: 270.747221393303 and parameters: {'n_estimators': 236, 'max_depth': 4, 'learning_rate': 0.045421562040387144, 'num_leaves': 34, 'min_child_samples': 39, 'reg_alpha': 0.5927550362054943, 'reg_lambda': 0.20621847302053736}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:14,124] Trial 29 finished with value: 312.81017449798543 and parameters: {'n_estimators': 452, 'max_depth': 7, 'learning_rate': 0.053530571616809526, 'num_leaves': 23, 'min_child_samples': 28, 'reg_alpha': 0.6679787933070038, 'reg_lambda': 0.21978653621965527}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:14,773] Trial 30 finished with value: 320.7065337453296 and parameters: {'n_estimators': 613, 'max_depth': 3, 'learning_rate': 0.11142053994621096, 'num_leaves': 30, 'min_child_samples': 42, 'reg_alpha': 0.697847240431924, 'reg_lambda': 0.09703924574560857}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:15,245] Trial 31 finished with value: 270.27606193939533 and parameters: {'n_estimators': 238, 'max_depth': 5, 'learning_rate': 0.031649525288092985, 'num_leaves': 34, 'min_child_samples': 38, 'reg_alpha': 0.574598581252882, 'reg_lambda': 0.02605035978170095}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:15,700] Trial 32 finished with value: 264.98156847377766 and parameters: {'n_estimators': 278, 'max_depth': 4, 'learning_rate': 0.03293564194155181, 'num_leaves': 32, 'min_child_samples': 24, 'reg_alpha': 0.6074221718578153, 'reg_lambda': 0.22349792685240139}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:16,152] Trial 33 finished with value: 303.17380921461927 and parameters: {'n_estimators': 158, 'max_depth': 7, 'learning_rate': 0.08141391260788854, 'num_leaves': 28, 'min_child_samples': 25, 'reg_alpha': 0.6044869763193889, 'reg_lambda': 0.13360255072951172}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:16,711] Trial 34 finished with value: 299.8760843016337 and parameters: {'n_estimators': 347, 'max_depth': 4, 'learning_rate': 0.07573875373717814, 'num_leaves': 27, 'min_child_samples': 19, 'reg_alpha': 0.45979584034052856, 'reg_lambda': 0.06650838972154191}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:18,139] Trial 35 finished with value: 284.7779208467415 and parameters: {'n_estimators': 472, 'max_depth': 8, 'learning_rate': 0.00803160049443225, 'num_leaves': 19, 'min_child_samples': 19, 'reg_alpha': 0.4790397914584085, 'reg_lambda': 0.23454185795471055}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:19,328] Trial 36 finished with value: 302.4234582734132 and parameters: {'n_estimators': 676, 'max_depth': 6, 'learning_rate': 0.024118448790511227, 'num_leaves': 41, 'min_child_samples': 50, 'reg_alpha': 0.37375911689925106, 'reg_lambda': 0.15695625787514755}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:20,124] Trial 37 finished with value: 311.4318421502185 and parameters: {'n_estimators': 929, 'max_depth': 3, 'learning_rate': 0.06440667072351432, 'num_leaves': 31, 'min_child_samples': 97, 'reg_alpha': 0.6382326827210908, 'reg_lambda': 0.33313093528290955}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:21,148] Trial 38 finished with value: 291.53889374195563 and parameters: {'n_estimators': 261, 'max_depth': 9, 'learning_rate': 0.032949560937650654, 'num_leaves': 26, 'min_child_samples': 13, 'reg_alpha': 0.5874304493085364, 'reg_lambda': 0.25020443550489724}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:21,475] Trial 39 finished with value: 266.5333957193962 and parameters: {'n_estimators': 575, 'max_depth': 6, 'learning_rate': 0.11295437780503322, 'num_leaves': 2, 'min_child_samples': 24, 'reg_alpha': 0.5078946258621573, 'reg_lambda': 0.34263016280866054}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:22,042] Trial 40 finished with value: 287.7339632015324 and parameters: {'n_estimators': 767, 'max_depth': 6, 'learning_rate': 0.11254917580114475, 'num_leaves': 3, 'min_child_samples': 23, 'reg_alpha': 0.3732594162612391, 'reg_lambda': 0.28111812812468184}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:22,857] Trial 41 finished with value: 271.3180764142988 and parameters: {'n_estimators': 585, 'max_depth': 4, 'learning_rate': 0.02866862149611532, 'num_leaves': 10, 'min_child_samples': 37, 'reg_alpha': 0.5017361642301467, 'reg_lambda': 0.3209470658577358}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:23,131] Trial 42 finished with value: 255.6525848034307 and parameters: {'n_estimators': 163, 'max_depth': 5, 'learning_rate': 0.06401643867532963, 'num_leaves': 8, 'min_child_samples': 30, 'reg_alpha': 0.4154192284671793, 'reg_lambda': 0.36530750740916407}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:23,972] Trial 43 finished with value: 305.00543106673206 and parameters: {'n_estimators': 799, 'max_depth': 3, 'learning_rate': 0.06502686720084831, 'num_leaves': 7, 'min_child_samples': 28, 'reg_alpha': 0.4482966720774814, 'reg_lambda': 0.34889565764538594}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:25,199] Trial 44 finished with value: 310.4947010381995 and parameters: {'n_estimators': 1000, 'max_depth': 4, 'learning_rate': 0.06395081639233607, 'num_leaves': 7, 'min_child_samples': 23, 'reg_alpha': 0.4137226061304453, 'reg_lambda': 0.35038852025433626}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:25,530] Trial 45 finished with value: 291.08121498293804 and parameters: {'n_estimators': 161, 'max_depth': 6, 'learning_rate': 0.09519901160745044, 'num_leaves': 11, 'min_child_samples': 32, 'reg_alpha': 0.5263351651223962, 'reg_lambda': 0.26367470221607736}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:26,507] Trial 46 finished with value: 304.10476922527226 and parameters: {'n_estimators': 1972, 'max_depth': 8, 'learning_rate': 0.20249598209727568, 'num_leaves': 2, 'min_child_samples': 18, 'reg_alpha': 0.3438078480493315, 'reg_lambda': 0.46741945198783097}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:27,069] Trial 47 finished with value: 300.65009091688006 and parameters: {'n_estimators': 628, 'max_depth': 5, 'learning_rate': 0.12162466196169242, 'num_leaves': 4, 'min_child_samples': 30, 'reg_alpha': 0.4148853703033246, 'reg_lambda': 0.2056965062563122}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:27,164] Trial 48 finished with value: 408.355863376841 and parameters: {'n_estimators': 28, 'max_depth': 3, 'learning_rate': 0.021244030715277737, 'num_leaves': 25, 'min_child_samples': 10, 'reg_alpha': 0.2817913942579252, 'reg_lambda': 0.4364864730063054}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:28,099] Trial 49 finished with value: 322.15265162216525 and parameters: {'n_estimators': 489, 'max_depth': 4, 'learning_rate': 0.0022904531496935696, 'num_leaves': 17, 'min_child_samples': 26, 'reg_alpha': 0.5363528090712821, 'reg_lambda': 0.3865797645094484}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:28,438] Trial 50 finished with value: 300.7536984017147 and parameters: {'n_estimators': 150, 'max_depth': 7, 'learning_rate': 0.09113463631716895, 'num_leaves': 50, 'min_child_samples': 55, 'reg_alpha': 0.47825979063444374, 'reg_lambda': 0.3205517143840942}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:29,047] Trial 51 finished with value: 274.7340313567243 and parameters: {'n_estimators': 330, 'max_depth': 5, 'learning_rate': 0.029900095727051886, 'num_leaves': 36, 'min_child_samples': 39, 'reg_alpha': 0.5785169961369323, 'reg_lambda': 0.2559181577731807}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:29,719] Trial 52 finished with value: 291.6095764009087 and parameters: {'n_estimators': 413, 'max_depth': 5, 'learning_rate': 0.04693792642675104, 'num_leaves': 32, 'min_child_samples': 47, 'reg_alpha': 0.5435165237499313, 'reg_lambda': 0.3657795089831244}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:30,159] Trial 53 finished with value: 281.92214816358415 and parameters: {'n_estimators': 305, 'max_depth': 6, 'learning_rate': 0.021837135498162033, 'num_leaves': 40, 'min_child_samples': 85, 'reg_alpha': 0.4996315391650473, 'reg_lambda': 0.1818022453557193}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:30,401] Trial 54 finished with value: 266.051347406327 and parameters: {'n_estimators': 115, 'max_depth': 4, 'learning_rate': 0.06084901536152042, 'num_leaves': 37, 'min_child_samples': 33, 'reg_alpha': 0.6203873605549437, 'reg_lambda': 0.28814199074104924}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:30,573] Trial 55 finished with value: 254.24491771887585 and parameters: {'n_estimators': 107, 'max_depth': 3, 'learning_rate': 0.0680409927054294, 'num_leaves': 37, 'min_child_samples': 34, 'reg_alpha': 0.6407723786573941, 'reg_lambda': 0.29714727241899563}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:30,737] Trial 56 finished with value: 275.0369745100725 and parameters: {'n_estimators': 74, 'max_depth': 4, 'learning_rate': 0.14048304409478, 'num_leaves': 37, 'min_child_samples': 32, 'reg_alpha': 0.6288538597199677, 'reg_lambda': 0.2812171734528326}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:30,939] Trial 57 finished with value: 262.737776280892 and parameters: {'n_estimators': 138, 'max_depth': 3, 'learning_rate': 0.07314054111690418, 'num_leaves': 21, 'min_child_samples': 33, 'reg_alpha': 0.6819853026528071, 'reg_lambda': 0.31472276371274893}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:31,169] Trial 58 finished with value: 260.3480585553761 and parameters: {'n_estimators': 168, 'max_depth': 3, 'learning_rate': 0.07294532350965369, 'num_leaves': 21, 'min_child_samples': 34, 'reg_alpha': 0.7009427170622431, 'reg_lambda': 0.2980052167045022}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:31,405] Trial 59 finished with value: 263.56646644456936 and parameters: {'n_estimators': 181, 'max_depth': 3, 'learning_rate': 0.08050579774543218, 'num_leaves': 20, 'min_child_samples': 41, 'reg_alpha': 0.7014341285626607, 'reg_lambda': 0.21557069997867173}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:31,873] Trial 60 finished with value: 304.0923038644021 and parameters: {'n_estimators': 176, 'max_depth': 13, 'learning_rate': 0.08097789446686754, 'num_leaves': 20, 'min_child_samples': 48, 'reg_alpha': 0.7346271519244397, 'reg_lambda': 0.4145224640084299}. Best is trial 23 with value: 253.0991206039589.\n",
      "[I 2023-10-29 13:38:32,041] Trial 61 finished with value: 247.82956465073204 and parameters: {'n_estimators': 79, 'max_depth': 3, 'learning_rate': 0.07366832038117353, 'num_leaves': 16, 'min_child_samples': 41, 'reg_alpha': 0.6770121559307064, 'reg_lambda': 0.22403926089858545}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:32,138] Trial 62 finished with value: 262.4972195773265 and parameters: {'n_estimators': 35, 'max_depth': 3, 'learning_rate': 0.10351059585853026, 'num_leaves': 16, 'min_child_samples': 43, 'reg_alpha': 0.676758543097527, 'reg_lambda': 0.3072613556215614}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:32,240] Trial 63 finished with value: 259.04098610426314 and parameters: {'n_estimators': 37, 'max_depth': 3, 'learning_rate': 0.0981800412146269, 'num_leaves': 16, 'min_child_samples': 35, 'reg_alpha': 0.7855966070043215, 'reg_lambda': 0.30587572930809304}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:32,348] Trial 64 finished with value: 259.67097525340785 and parameters: {'n_estimators': 46, 'max_depth': 3, 'learning_rate': 0.1298056123268255, 'num_leaves': 15, 'min_child_samples': 45, 'reg_alpha': 0.7686142083997789, 'reg_lambda': 0.3813330098136357}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:32,439] Trial 65 finished with value: 257.5460562602097 and parameters: {'n_estimators': 24, 'max_depth': 3, 'learning_rate': 0.12629311469657886, 'num_leaves': 13, 'min_child_samples': 56, 'reg_alpha': 0.8099280471716606, 'reg_lambda': 0.3675494773895498}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:32,586] Trial 66 finished with value: 282.4697038173094 and parameters: {'n_estimators': 73, 'max_depth': 4, 'learning_rate': 0.16166632855606156, 'num_leaves': 14, 'min_child_samples': 60, 'reg_alpha': 0.8202980108915802, 'reg_lambda': 0.39292413888221955}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:32,685] Trial 67 finished with value: 268.8225779347994 and parameters: {'n_estimators': 22, 'max_depth': 5, 'learning_rate': 0.13396362442668297, 'num_leaves': 11, 'min_child_samples': 69, 'reg_alpha': 0.8085198458322346, 'reg_lambda': 0.4389412073051472}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:34,369] Trial 68 finished with value: 334.8048092167726 and parameters: {'n_estimators': 1619, 'max_depth': 3, 'learning_rate': 0.12925482840007416, 'num_leaves': 9, 'min_child_samples': 54, 'reg_alpha': 0.835900437987224, 'reg_lambda': 0.37471591634887225}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:34,546] Trial 69 finished with value: 277.7458041376854 and parameters: {'n_estimators': 94, 'max_depth': 4, 'learning_rate': 0.09579671944471325, 'num_leaves': 13, 'min_child_samples': 57, 'reg_alpha': 0.762324545412235, 'reg_lambda': 0.5329195750489211}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:35,032] Trial 70 finished with value: 294.3541182518565 and parameters: {'n_estimators': 220, 'max_depth': 11, 'learning_rate': 0.051041631456235886, 'num_leaves': 15, 'min_child_samples': 50, 'reg_alpha': 0.8583897659274997, 'reg_lambda': 0.26606024496969455}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:35,312] Trial 71 finished with value: 277.10801048143264 and parameters: {'n_estimators': 201, 'max_depth': 3, 'learning_rate': 0.10573365561135037, 'num_leaves': 18, 'min_child_samples': 36, 'reg_alpha': 0.7234710432161776, 'reg_lambda': 0.24505325642532963}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:35,478] Trial 72 finished with value: 284.6259324365953 and parameters: {'n_estimators': 103, 'max_depth': 3, 'learning_rate': 0.15341229750415603, 'num_leaves': 13, 'min_child_samples': 45, 'reg_alpha': 0.6590439158668516, 'reg_lambda': 0.29953608866229176}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:35,917] Trial 73 finished with value: 292.66987676277694 and parameters: {'n_estimators': 291, 'max_depth': 4, 'learning_rate': 0.09017583960844877, 'num_leaves': 22, 'min_child_samples': 40, 'reg_alpha': 0.7601854122580275, 'reg_lambda': 0.37744735506952226}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:36,376] Trial 74 finished with value: 312.03537670911686 and parameters: {'n_estimators': 395, 'max_depth': 3, 'learning_rate': 0.12329391540974766, 'num_leaves': 17, 'min_child_samples': 35, 'reg_alpha': 0.7868073377155966, 'reg_lambda': 0.17987390054384184}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:36,625] Trial 75 finished with value: 267.30985255856706 and parameters: {'n_estimators': 110, 'max_depth': 5, 'learning_rate': 0.05958074239002491, 'num_leaves': 24, 'min_child_samples': 47, 'reg_alpha': 0.8848837993155636, 'reg_lambda': 0.35783409957655327}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:36,719] Trial 76 finished with value: 327.80007304145505 and parameters: {'n_estimators': 21, 'max_depth': 19, 'learning_rate': 0.04595733028864708, 'num_leaves': 8, 'min_child_samples': 28, 'reg_alpha': 0.74654695913418, 'reg_lambda': 0.3181073546130521}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:38,061] Trial 77 finished with value: 323.43604985966715 and parameters: {'n_estimators': 1297, 'max_depth': 3, 'learning_rate': 0.07408249466263027, 'num_leaves': 13, 'min_child_samples': 35, 'reg_alpha': 0.7070090975163174, 'reg_lambda': 0.2734407055574747}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:38,404] Trial 78 finished with value: 312.3207858384432 and parameters: {'n_estimators': 232, 'max_depth': 4, 'learning_rate': 0.13941441551287273, 'num_leaves': 11, 'min_child_samples': 52, 'reg_alpha': 0.7807646838819252, 'reg_lambda': 0.24466196689860087}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:38,570] Trial 79 finished with value: 278.00043770712324 and parameters: {'n_estimators': 69, 'max_depth': 5, 'learning_rate': 0.11609279105932431, 'num_leaves': 15, 'min_child_samples': 64, 'reg_alpha': 0.7227058805021898, 'reg_lambda': 0.3364710451623915}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:38,999] Trial 80 finished with value: 302.00088608365877 and parameters: {'n_estimators': 290, 'max_depth': 4, 'learning_rate': 0.10164757130101473, 'num_leaves': 18, 'min_child_samples': 44, 'reg_alpha': 0.6728771844478102, 'reg_lambda': 0.40605781608332625}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:39,197] Trial 81 finished with value: 270.2627213762266 and parameters: {'n_estimators': 133, 'max_depth': 3, 'learning_rate': 0.10299806648070538, 'num_leaves': 16, 'min_child_samples': 42, 'reg_alpha': 0.7361164890532399, 'reg_lambda': 0.30183504069647304}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:39,284] Trial 82 finished with value: 278.207522610691 and parameters: {'n_estimators': 21, 'max_depth': 3, 'learning_rate': 0.08826972064704064, 'num_leaves': 12, 'min_child_samples': 38, 'reg_alpha': 0.6876911532706684, 'reg_lambda': 0.2902497246694416}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:39,604] Trial 83 finished with value: 272.90976209675404 and parameters: {'n_estimators': 189, 'max_depth': 4, 'learning_rate': 0.06902880423876269, 'num_leaves': 16, 'min_child_samples': 30, 'reg_alpha': 0.6468278564509421, 'reg_lambda': 0.22533163046708315}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:39,739] Trial 84 finished with value: 262.32872957291727 and parameters: {'n_estimators': 66, 'max_depth': 3, 'learning_rate': 0.040667857805282016, 'num_leaves': 46, 'min_child_samples': 41, 'reg_alpha': 0.6486019982670121, 'reg_lambda': 0.35447861618402354}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:39,936] Trial 85 finished with value: 304.78967938831124 and parameters: {'n_estimators': 81, 'max_depth': 4, 'learning_rate': 0.015754016605786292, 'num_leaves': 46, 'min_child_samples': 40, 'reg_alpha': 0.6515171642505996, 'reg_lambda': 0.35054231292568566}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:40,144] Trial 86 finished with value: 249.54415272420448 and parameters: {'n_estimators': 145, 'max_depth': 3, 'learning_rate': 0.05807274858792298, 'num_leaves': 46, 'min_child_samples': 48, 'reg_alpha': 0.7588704888509489, 'reg_lambda': 0.3674208567977518}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:40,774] Trial 87 finished with value: 294.2435651199357 and parameters: {'n_estimators': 522, 'max_depth': 5, 'learning_rate': 0.05949400042985692, 'num_leaves': 6, 'min_child_samples': 49, 'reg_alpha': 0.7587445474449708, 'reg_lambda': 0.43470477590049644}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:41,201] Trial 88 finished with value: 267.69738466853426 and parameters: {'n_estimators': 352, 'max_depth': 3, 'learning_rate': 0.04041225325269327, 'num_leaves': 9, 'min_child_samples': 58, 'reg_alpha': 0.7935827779519802, 'reg_lambda': 0.37776306523004194}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:41,623] Trial 89 finished with value: 257.276426603064 and parameters: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.019450172866503704, 'num_leaves': 19, 'min_child_samples': 45, 'reg_alpha': 0.7124779471928993, 'reg_lambda': 0.20211432019195447}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:45,647] Trial 90 finished with value: 300.46341369955763 and parameters: {'n_estimators': 1790, 'max_depth': 17, 'learning_rate': 0.01899228697723777, 'num_leaves': 19, 'min_child_samples': 53, 'reg_alpha': 0.7507332885234609, 'reg_lambda': 0.192847527271314}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:45,910] Trial 91 finished with value: 257.8572148016349 and parameters: {'n_estimators': 141, 'max_depth': 4, 'learning_rate': 0.03649676923106769, 'num_leaves': 22, 'min_child_samples': 45, 'reg_alpha': 0.701550297088381, 'reg_lambda': 0.2348266724068017}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:46,362] Trial 92 finished with value: 259.36930343401895 and parameters: {'n_estimators': 269, 'max_depth': 4, 'learning_rate': 0.014152790492504876, 'num_leaves': 24, 'min_child_samples': 45, 'reg_alpha': 0.7177516180635157, 'reg_lambda': 0.23363794595297005}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:46,891] Trial 93 finished with value: 267.4295338292246 and parameters: {'n_estimators': 254, 'max_depth': 5, 'learning_rate': 0.012634911557488176, 'num_leaves': 23, 'min_child_samples': 46, 'reg_alpha': 0.7262610821799284, 'reg_lambda': 0.1577291545660744}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:47,260] Trial 94 finished with value: 257.2781072212171 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.03725834094786061, 'num_leaves': 25, 'min_child_samples': 37, 'reg_alpha': 0.5964458921634315, 'reg_lambda': 0.23288798410089265}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:48,143] Trial 95 finished with value: 301.5613442118781 and parameters: {'n_estimators': 421, 'max_depth': 6, 'learning_rate': 0.036423785014313426, 'num_leaves': 26, 'min_child_samples': 37, 'reg_alpha': 0.6888664557579208, 'reg_lambda': 0.26279850946636985}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:48,382] Trial 96 finished with value: 266.1733905690677 and parameters: {'n_estimators': 133, 'max_depth': 4, 'learning_rate': 0.05609387251202651, 'num_leaves': 48, 'min_child_samples': 52, 'reg_alpha': 0.6030584602330381, 'reg_lambda': 0.19254860902780802}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:48,850] Trial 97 finished with value: 270.73986725572934 and parameters: {'n_estimators': 199, 'max_depth': 5, 'learning_rate': 0.0276012317121566, 'num_leaves': 39, 'min_child_samples': 26, 'reg_alpha': 0.5604771352642756, 'reg_lambda': 0.20940231478147167}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:49,092] Trial 98 finished with value: 265.5631556243441 and parameters: {'n_estimators': 129, 'max_depth': 4, 'learning_rate': 0.04838654833073578, 'num_leaves': 28, 'min_child_samples': 48, 'reg_alpha': 0.5852346841455159, 'reg_lambda': 0.23684442748865672}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:49,690] Trial 99 finished with value: 518.6273107697349 and parameters: {'n_estimators': 301, 'max_depth': 5, 'learning_rate': 0.0008765613708527353, 'num_leaves': 30, 'min_child_samples': 56, 'reg_alpha': 0.6232538482576334, 'reg_lambda': 0.12635868164592903}. Best is trial 61 with value: 247.82956465073204.\n",
      "[I 2023-10-29 13:38:49,716] A new study created in memory with name: no-name-4cc56f5b-9d5a-4e27-844a-8b711163a0e4\n",
      "[I 2023-10-29 13:38:52,522] Trial 0 finished with value: 178.9055153967133 and parameters: {'n_estimators': 1529, 'max_depth': 10, 'learning_rate': 0.37548949614258814, 'num_leaves': 34, 'min_child_samples': 53, 'reg_alpha': 0.7460696304757592, 'reg_lambda': 0.1969150560166992}. Best is trial 0 with value: 178.9055153967133.\n",
      "[I 2023-10-29 13:38:56,448] Trial 1 finished with value: 150.4681673174895 and parameters: {'n_estimators': 1547, 'max_depth': 9, 'learning_rate': 0.15762955762903877, 'num_leaves': 36, 'min_child_samples': 34, 'reg_alpha': 0.9546251785821322, 'reg_lambda': 0.14860331860429923}. Best is trial 1 with value: 150.4681673174895.\n",
      "[I 2023-10-29 13:38:58,801] Trial 2 finished with value: 166.45882787251782 and parameters: {'n_estimators': 920, 'max_depth': 19, 'learning_rate': 0.466211743252812, 'num_leaves': 18, 'min_child_samples': 51, 'reg_alpha': 0.056112009304634766, 'reg_lambda': 0.7446710236819536}. Best is trial 1 with value: 150.4681673174895.\n",
      "[I 2023-10-29 13:38:59,405] Trial 3 finished with value: 141.740144647747 and parameters: {'n_estimators': 527, 'max_depth': 3, 'learning_rate': 0.01176497377470283, 'num_leaves': 9, 'min_child_samples': 34, 'reg_alpha': 0.18446518898683695, 'reg_lambda': 0.9319681230465003}. Best is trial 3 with value: 141.740144647747.\n",
      "[I 2023-10-29 13:39:01,874] Trial 4 finished with value: 150.77754721323058 and parameters: {'n_estimators': 1387, 'max_depth': 15, 'learning_rate': 0.20744188655764592, 'num_leaves': 41, 'min_child_samples': 89, 'reg_alpha': 0.5235380824469652, 'reg_lambda': 0.5488780618249759}. Best is trial 3 with value: 141.740144647747.\n",
      "[I 2023-10-29 13:39:04,550] Trial 5 finished with value: 167.569836994938 and parameters: {'n_estimators': 959, 'max_depth': 15, 'learning_rate': 0.32626645187688946, 'num_leaves': 26, 'min_child_samples': 54, 'reg_alpha': 0.13252450047952052, 'reg_lambda': 0.36599051129907395}. Best is trial 3 with value: 141.740144647747.\n",
      "[I 2023-10-29 13:39:09,183] Trial 6 finished with value: 130.5580394224107 and parameters: {'n_estimators': 1488, 'max_depth': 7, 'learning_rate': 0.009407927317383574, 'num_leaves': 23, 'min_child_samples': 14, 'reg_alpha': 0.8907399263411794, 'reg_lambda': 0.8253344125796368}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:10,324] Trial 7 finished with value: 135.3476745358931 and parameters: {'n_estimators': 1184, 'max_depth': 8, 'learning_rate': 0.1280348291999428, 'num_leaves': 6, 'min_child_samples': 99, 'reg_alpha': 0.4810688795024387, 'reg_lambda': 0.20973735430429893}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:12,032] Trial 8 finished with value: 145.60182243674893 and parameters: {'n_estimators': 1739, 'max_depth': 3, 'learning_rate': 0.2066876421983631, 'num_leaves': 26, 'min_child_samples': 74, 'reg_alpha': 0.14107323308858988, 'reg_lambda': 0.7179036427510648}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:14,357] Trial 9 finished with value: 147.69869003013557 and parameters: {'n_estimators': 687, 'max_depth': 19, 'learning_rate': 0.22528556829955926, 'num_leaves': 35, 'min_child_samples': 44, 'reg_alpha': 0.3859203173451856, 'reg_lambda': 0.4467185915502837}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:15,023] Trial 10 finished with value: 162.53665275115392 and parameters: {'n_estimators': 217, 'max_depth': 7, 'learning_rate': 0.008750513784420322, 'num_leaves': 15, 'min_child_samples': 9, 'reg_alpha': 0.9536000085508142, 'reg_lambda': 0.9762540377582307}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:18,383] Trial 11 finished with value: 132.38932595614799 and parameters: {'n_estimators': 1204, 'max_depth': 7, 'learning_rate': 0.10490202794798156, 'num_leaves': 50, 'min_child_samples': 6, 'reg_alpha': 0.6317009255445019, 'reg_lambda': 0.06174678540514006}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:22,638] Trial 12 finished with value: 139.1476151289467 and parameters: {'n_estimators': 1972, 'max_depth': 6, 'learning_rate': 0.08624171520847321, 'num_leaves': 49, 'min_child_samples': 7, 'reg_alpha': 0.7627279386876172, 'reg_lambda': 0.036891264007961666}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:26,352] Trial 13 finished with value: 144.88827804944668 and parameters: {'n_estimators': 1238, 'max_depth': 13, 'learning_rate': 0.07664044718565552, 'num_leaves': 19, 'min_child_samples': 19, 'reg_alpha': 0.7596487494983758, 'reg_lambda': 0.3263594339283339}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:29,922] Trial 14 finished with value: 151.14821513404686 and parameters: {'n_estimators': 1730, 'max_depth': 5, 'learning_rate': 0.06923888540062365, 'num_leaves': 50, 'min_child_samples': 21, 'reg_alpha': 0.645528559046643, 'reg_lambda': 0.5943491098861431}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:34,137] Trial 15 finished with value: 133.37496409060276 and parameters: {'n_estimators': 1118, 'max_depth': 11, 'learning_rate': 0.018181243944251687, 'num_leaves': 29, 'min_child_samples': 21, 'reg_alpha': 0.9988017250781718, 'reg_lambda': 0.7869612841180729}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:36,115] Trial 16 finished with value: 144.65315094664848 and parameters: {'n_estimators': 754, 'max_depth': 5, 'learning_rate': 0.1510453848465394, 'num_leaves': 43, 'min_child_samples': 5, 'reg_alpha': 0.8300729460774205, 'reg_lambda': 0.012318021558043069}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:36,252] Trial 17 finished with value: 133.0917087941626 and parameters: {'n_estimators': 43, 'max_depth': 13, 'learning_rate': 0.10154877070612382, 'num_leaves': 12, 'min_child_samples': 33, 'reg_alpha': 0.6074018648847475, 'reg_lambda': 0.6003160399624565}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:38,882] Trial 18 finished with value: 136.81917116882775 and parameters: {'n_estimators': 1353, 'max_depth': 8, 'learning_rate': 0.056903185121526256, 'num_leaves': 22, 'min_child_samples': 70, 'reg_alpha': 0.8665363411235953, 'reg_lambda': 0.4704766684511826}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:39,844] Trial 19 finished with value: 156.0102852494381 and parameters: {'n_estimators': 1909, 'max_depth': 5, 'learning_rate': 0.1571199431811636, 'num_leaves': 2, 'min_child_samples': 15, 'reg_alpha': 0.6730702995935125, 'reg_lambda': 0.8628906805448948}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:45,866] Trial 20 finished with value: 136.38506730451857 and parameters: {'n_estimators': 1609, 'max_depth': 12, 'learning_rate': 0.043824337817782166, 'num_leaves': 31, 'min_child_samples': 25, 'reg_alpha': 0.8645118275351484, 'reg_lambda': 0.6333706047618983}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:45,968] Trial 21 finished with value: 145.50713041532512 and parameters: {'n_estimators': 20, 'max_depth': 15, 'learning_rate': 0.11102507135535984, 'num_leaves': 14, 'min_child_samples': 32, 'reg_alpha': 0.6056447133062024, 'reg_lambda': 0.668026568540298}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:46,811] Trial 22 finished with value: 139.04053394862524 and parameters: {'n_estimators': 421, 'max_depth': 13, 'learning_rate': 0.11121105788678544, 'num_leaves': 11, 'min_child_samples': 29, 'reg_alpha': 0.6008727182048482, 'reg_lambda': 0.8215519703596776}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:52,085] Trial 23 finished with value: 463.3041063700444 and parameters: {'n_estimators': 1395, 'max_depth': 17, 'learning_rate': 0.00011669813738143797, 'num_leaves': 23, 'min_child_samples': 13, 'reg_alpha': 0.44510255933254095, 'reg_lambda': 0.5334644111828535}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:54,275] Trial 24 finished with value: 145.13176551486626 and parameters: {'n_estimators': 796, 'max_depth': 10, 'learning_rate': 0.0527919245527663, 'num_leaves': 41, 'min_child_samples': 44, 'reg_alpha': 0.6992092500162818, 'reg_lambda': 0.6811378046496792}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:55,019] Trial 25 finished with value: 134.29965995368946 and parameters: {'n_estimators': 1054, 'max_depth': 7, 'learning_rate': 0.10423881495323266, 'num_leaves': 3, 'min_child_samples': 40, 'reg_alpha': 0.5766673024040276, 'reg_lambda': 0.6022729994941594}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:55,919] Trial 26 finished with value: 130.78258561196566 and parameters: {'n_estimators': 432, 'max_depth': 12, 'learning_rate': 0.04573820666095184, 'num_leaves': 11, 'min_child_samples': 12, 'reg_alpha': 0.35777645348576026, 'reg_lambda': 0.8715756776445562}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:39:57,357] Trial 27 finished with value: 132.83200706640673 and parameters: {'n_estimators': 486, 'max_depth': 10, 'learning_rate': 0.03831672035721314, 'num_leaves': 18, 'min_child_samples': 15, 'reg_alpha': 0.3213372333327786, 'reg_lambda': 0.9013553771683442}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:40:02,977] Trial 28 finished with value: 140.4979841182248 and parameters: {'n_estimators': 1311, 'max_depth': 8, 'learning_rate': 0.04267757127109176, 'num_leaves': 46, 'min_child_samples': 11, 'reg_alpha': 0.3328092715359831, 'reg_lambda': 0.8060925927118046}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:40:04,609] Trial 29 finished with value: 133.51107769847036 and parameters: {'n_estimators': 330, 'max_depth': 11, 'learning_rate': 0.07411004542291119, 'num_leaves': 31, 'min_child_samples': 5, 'reg_alpha': 0.519384346785249, 'reg_lambda': 0.9704576601497726}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:40:05,915] Trial 30 finished with value: 150.74542170925542 and parameters: {'n_estimators': 599, 'max_depth': 9, 'learning_rate': 0.26962787907584845, 'num_leaves': 38, 'min_child_samples': 62, 'reg_alpha': 0.7131105612825579, 'reg_lambda': 0.8783097353390363}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:40:06,878] Trial 31 finished with value: 131.795371822008 and parameters: {'n_estimators': 315, 'max_depth': 10, 'learning_rate': 0.038227899794043285, 'num_leaves': 17, 'min_child_samples': 16, 'reg_alpha': 0.3122934923462613, 'reg_lambda': 0.995424041533111}. Best is trial 6 with value: 130.5580394224107.\n",
      "[I 2023-10-29 13:40:07,625] Trial 32 finished with value: 130.1125769951208 and parameters: {'n_estimators': 232, 'max_depth': 9, 'learning_rate': 0.03868494740735391, 'num_leaves': 22, 'min_child_samples': 26, 'reg_alpha': 0.27214586209281233, 'reg_lambda': 0.9428661734868815}. Best is trial 32 with value: 130.1125769951208.\n",
      "[I 2023-10-29 13:40:08,314] Trial 33 finished with value: 310.1031496101043 and parameters: {'n_estimators': 204, 'max_depth': 9, 'learning_rate': 0.002636262838371782, 'num_leaves': 22, 'min_child_samples': 23, 'reg_alpha': 0.26643425709858765, 'reg_lambda': 0.982716416069854}. Best is trial 32 with value: 130.1125769951208.\n",
      "[I 2023-10-29 13:40:08,825] Trial 34 finished with value: 127.27089165680833 and parameters: {'n_estimators': 300, 'max_depth': 12, 'learning_rate': 0.024788396530703662, 'num_leaves': 8, 'min_child_samples': 26, 'reg_alpha': 0.41169326086915337, 'reg_lambda': 0.9170777006844473}. Best is trial 34 with value: 127.27089165680833.\n",
      "[I 2023-10-29 13:40:09,172] Trial 35 finished with value: 135.62185162064924 and parameters: {'n_estimators': 191, 'max_depth': 12, 'learning_rate': 0.027799281657909666, 'num_leaves': 8, 'min_child_samples': 27, 'reg_alpha': 0.43019282381604745, 'reg_lambda': 0.9151502741704354}. Best is trial 34 with value: 127.27089165680833.\n",
      "[I 2023-10-29 13:40:10,176] Trial 36 finished with value: 134.4032470218604 and parameters: {'n_estimators': 606, 'max_depth': 14, 'learning_rate': 0.05132445594521412, 'num_leaves': 9, 'min_child_samples': 38, 'reg_alpha': 0.23564184880688444, 'reg_lambda': 0.8502112654004977}. Best is trial 34 with value: 127.27089165680833.\n",
      "[I 2023-10-29 13:40:11,109] Trial 37 finished with value: 145.30881773525766 and parameters: {'n_estimators': 883, 'max_depth': 16, 'learning_rate': 0.13580212557036403, 'num_leaves': 5, 'min_child_samples': 28, 'reg_alpha': 0.38484081121761876, 'reg_lambda': 0.7473936616794058}. Best is trial 34 with value: 127.27089165680833.\n",
      "[I 2023-10-29 13:40:12,029] Trial 38 finished with value: 134.2263554993598 and parameters: {'n_estimators': 403, 'max_depth': 11, 'learning_rate': 0.07890125826924568, 'num_leaves': 13, 'min_child_samples': 18, 'reg_alpha': 0.009502911075612464, 'reg_lambda': 0.9173984033832806}. Best is trial 34 with value: 127.27089165680833.\n",
      "[I 2023-10-29 13:40:12,353] Trial 39 finished with value: 136.0577334804068 and parameters: {'n_estimators': 121, 'max_depth': 9, 'learning_rate': 0.17557710713453262, 'num_leaves': 24, 'min_child_samples': 54, 'reg_alpha': 0.4741487905831296, 'reg_lambda': 0.7705156365553388}. Best is trial 34 with value: 127.27089165680833.\n",
      "[I 2023-10-29 13:40:12,916] Trial 40 finished with value: 121.40559767255007 and parameters: {'n_estimators': 328, 'max_depth': 4, 'learning_rate': 0.023644912058503557, 'num_leaves': 20, 'min_child_samples': 39, 'reg_alpha': 0.5337980498867384, 'reg_lambda': 0.8357118491423353}. Best is trial 40 with value: 121.40559767255007.\n",
      "[I 2023-10-29 13:40:13,358] Trial 41 finished with value: 131.1118715081276 and parameters: {'n_estimators': 327, 'max_depth': 3, 'learning_rate': 0.027176494412460832, 'num_leaves': 19, 'min_child_samples': 36, 'reg_alpha': 0.5543000569130017, 'reg_lambda': 0.8183308464172983}. Best is trial 40 with value: 121.40559767255007.\n",
      "[I 2023-10-29 13:40:14,219] Trial 42 finished with value: 158.09582333261733 and parameters: {'n_estimators': 495, 'max_depth': 4, 'learning_rate': 0.005164869823284641, 'num_leaves': 16, 'min_child_samples': 47, 'reg_alpha': 0.3946574347983349, 'reg_lambda': 0.9335427245824773}. Best is trial 40 with value: 121.40559767255007.\n",
      "[I 2023-10-29 13:40:14,952] Trial 43 finished with value: 128.41397233401295 and parameters: {'n_estimators': 273, 'max_depth': 6, 'learning_rate': 0.024918163686355136, 'num_leaves': 28, 'min_child_samples': 32, 'reg_alpha': 0.5146823788933759, 'reg_lambda': 0.7441843783654525}. Best is trial 40 with value: 121.40559767255007.\n",
      "[I 2023-10-29 13:40:15,346] Trial 44 finished with value: 134.68625640746376 and parameters: {'n_estimators': 138, 'max_depth': 6, 'learning_rate': 0.0221007000090911, 'num_leaves': 28, 'min_child_samples': 40, 'reg_alpha': 0.46664932524780184, 'reg_lambda': 0.737445307902292}. Best is trial 40 with value: 121.40559767255007.\n",
      "[I 2023-10-29 13:40:15,777] Trial 45 finished with value: 128.91226656256364 and parameters: {'n_estimators': 277, 'max_depth': 4, 'learning_rate': 0.06885945685648126, 'num_leaves': 33, 'min_child_samples': 48, 'reg_alpha': 0.527916173343408, 'reg_lambda': 0.8345923533145639}. Best is trial 40 with value: 121.40559767255007.\n",
      "[I 2023-10-29 13:40:16,188] Trial 46 finished with value: 126.95595647707377 and parameters: {'n_estimators': 277, 'max_depth': 4, 'learning_rate': 0.06622261361012016, 'num_leaves': 33, 'min_child_samples': 60, 'reg_alpha': 0.5281225896257858, 'reg_lambda': 0.9448275721255244}. Best is trial 40 with value: 121.40559767255007.\n",
      "[I 2023-10-29 13:40:17,020] Trial 47 finished with value: 132.93195203677934 and parameters: {'n_estimators': 607, 'max_depth': 4, 'learning_rate': 0.06496315420259577, 'num_leaves': 34, 'min_child_samples': 62, 'reg_alpha': 0.5195618000777663, 'reg_lambda': 0.843013565378689}. Best is trial 40 with value: 121.40559767255007.\n",
      "[I 2023-10-29 13:40:17,483] Trial 48 finished with value: 136.17475948441725 and parameters: {'n_estimators': 308, 'max_depth': 4, 'learning_rate': 0.08548224775515507, 'num_leaves': 33, 'min_child_samples': 59, 'reg_alpha': 0.5016768615096743, 'reg_lambda': 0.7844884770951197}. Best is trial 40 with value: 121.40559767255007.\n",
      "[I 2023-10-29 13:40:17,669] Trial 49 finished with value: 131.95858181353384 and parameters: {'n_estimators': 119, 'max_depth': 3, 'learning_rate': 0.07087582532530852, 'num_leaves': 37, 'min_child_samples': 50, 'reg_alpha': 0.5401889741266402, 'reg_lambda': 0.8872040643677435}. Best is trial 40 with value: 121.40559767255007.\n",
      "[I 2023-10-29 13:40:18,123] Trial 50 finished with value: 137.15398532758633 and parameters: {'n_estimators': 253, 'max_depth': 6, 'learning_rate': 0.13672378158630055, 'num_leaves': 28, 'min_child_samples': 84, 'reg_alpha': 0.5616575166870371, 'reg_lambda': 0.7296610790781877}. Best is trial 40 with value: 121.40559767255007.\n",
      "[I 2023-10-29 13:40:18,585] Trial 51 finished with value: 120.87395891191115 and parameters: {'n_estimators': 257, 'max_depth': 4, 'learning_rate': 0.02569601965933067, 'num_leaves': 25, 'min_child_samples': 43, 'reg_alpha': 0.42542267522589877, 'reg_lambda': 0.9341879697428686}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:19,192] Trial 52 finished with value: 120.97886924719172 and parameters: {'n_estimators': 383, 'max_depth': 4, 'learning_rate': 0.021250144464682978, 'num_leaves': 26, 'min_child_samples': 43, 'reg_alpha': 0.43579682136910347, 'reg_lambda': 0.9448208483613346}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:19,954] Trial 53 finished with value: 122.73712573246137 and parameters: {'n_estimators': 371, 'max_depth': 5, 'learning_rate': 0.018240059721483664, 'num_leaves': 25, 'min_child_samples': 43, 'reg_alpha': 0.4273328189129458, 'reg_lambda': 0.9548731651282445}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:20,852] Trial 54 finished with value: 124.60061514544238 and parameters: {'n_estimators': 530, 'max_depth': 5, 'learning_rate': 0.0176190481114671, 'num_leaves': 26, 'min_child_samples': 57, 'reg_alpha': 0.4197605060937886, 'reg_lambda': 0.9996979024468682}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:21,715] Trial 55 finished with value: 152.41591188181275 and parameters: {'n_estimators': 533, 'max_depth': 5, 'learning_rate': 0.004876213299154428, 'num_leaves': 25, 'min_child_samples': 69, 'reg_alpha': 0.4432083543158502, 'reg_lambda': 0.9978262310875857}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:22,182] Trial 56 finished with value: 141.75875092318833 and parameters: {'n_estimators': 388, 'max_depth': 3, 'learning_rate': 0.015273841205530683, 'num_leaves': 20, 'min_child_samples': 56, 'reg_alpha': 0.4762081000499628, 'reg_lambda': 0.947951102557808}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:23,378] Trial 57 finished with value: 136.67941649297887 and parameters: {'n_estimators': 685, 'max_depth': 5, 'learning_rate': 0.05595711378174287, 'num_leaves': 30, 'min_child_samples': 43, 'reg_alpha': 0.4284586518949557, 'reg_lambda': 0.9628591774757573}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:24,061] Trial 58 finished with value: 140.23275527926305 and parameters: {'n_estimators': 525, 'max_depth': 4, 'learning_rate': 0.09706694562265417, 'num_leaves': 26, 'min_child_samples': 66, 'reg_alpha': 0.38037282222415825, 'reg_lambda': 0.9416126541174892}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:24,922] Trial 59 finished with value: 411.3335039778964 and parameters: {'n_estimators': 382, 'max_depth': 6, 'learning_rate': 0.000804434720686524, 'num_leaves': 20, 'min_child_samples': 52, 'reg_alpha': 0.4716064864246368, 'reg_lambda': 0.8882890854007357}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:25,061] Trial 60 finished with value: 137.38112785539687 and parameters: {'n_estimators': 72, 'max_depth': 3, 'learning_rate': 0.08585971487989992, 'num_leaves': 24, 'min_child_samples': 46, 'reg_alpha': 0.41382955488623574, 'reg_lambda': 0.9936183823319462}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:25,354] Trial 61 finished with value: 127.72245004088018 and parameters: {'n_estimators': 171, 'max_depth': 5, 'learning_rate': 0.027100076289500525, 'num_leaves': 26, 'min_child_samples': 78, 'reg_alpha': 0.40978811005817073, 'reg_lambda': 0.918655941327732}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:26,277] Trial 62 finished with value: 144.01743190250266 and parameters: {'n_estimators': 464, 'max_depth': 7, 'learning_rate': 0.058449105935879236, 'num_leaves': 27, 'min_child_samples': 57, 'reg_alpha': 0.45307579090355243, 'reg_lambda': 0.9019941773602461}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:26,889] Trial 63 finished with value: 122.36851920321614 and parameters: {'n_estimators': 376, 'max_depth': 4, 'learning_rate': 0.019484568792826944, 'num_leaves': 32, 'min_child_samples': 41, 'reg_alpha': 0.3691941543436664, 'reg_lambda': 0.9607298272907195}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:27,927] Trial 64 finished with value: 125.1656786129825 and parameters: {'n_estimators': 701, 'max_depth': 4, 'learning_rate': 0.016123455448961113, 'num_leaves': 32, 'min_child_samples': 50, 'reg_alpha': 0.3552229861472809, 'reg_lambda': 0.952323863187696}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:29,192] Trial 65 finished with value: 126.9262928747096 and parameters: {'n_estimators': 655, 'max_depth': 5, 'learning_rate': 0.017893606710490766, 'num_leaves': 30, 'min_child_samples': 42, 'reg_alpha': 0.34562346485165285, 'reg_lambda': 0.9702710072510503}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:29,824] Trial 66 finished with value: 125.58210594783442 and parameters: {'n_estimators': 545, 'max_depth': 3, 'learning_rate': 0.04500224293489617, 'num_leaves': 39, 'min_child_samples': 35, 'reg_alpha': 0.3736126929469618, 'reg_lambda': 0.8651011916368491}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:30,905] Trial 67 finished with value: 126.01317464423049 and parameters: {'n_estimators': 751, 'max_depth': 4, 'learning_rate': 0.016345962211088014, 'num_leaves': 31, 'min_child_samples': 50, 'reg_alpha': 0.4873027396375826, 'reg_lambda': 0.9650997658862376}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:32,534] Trial 68 finished with value: 137.53763963233592 and parameters: {'n_estimators': 869, 'max_depth': 5, 'learning_rate': 0.0358265451050471, 'num_leaves': 35, 'min_child_samples': 39, 'reg_alpha': 0.3061155745070399, 'reg_lambda': 0.9984997696139333}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:33,305] Trial 69 finished with value: 133.5910891948207 and parameters: {'n_estimators': 360, 'max_depth': 6, 'learning_rate': 0.04809790106477779, 'num_leaves': 24, 'min_child_samples': 47, 'reg_alpha': 0.36493995480546015, 'reg_lambda': 0.8625335836542796}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:34,291] Trial 70 finished with value: 145.94636046528694 and parameters: {'n_estimators': 453, 'max_depth': 7, 'learning_rate': 0.11883174285408821, 'num_leaves': 21, 'min_child_samples': 44, 'reg_alpha': 0.3455289555987061, 'reg_lambda': 0.8938132859563542}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:36,615] Trial 71 finished with value: 136.52586249651878 and parameters: {'n_estimators': 572, 'max_depth': 20, 'learning_rate': 0.042380983415655216, 'num_leaves': 39, 'min_child_samples': 34, 'reg_alpha': 0.3678399740513295, 'reg_lambda': 0.8578073846681811}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:37,524] Trial 72 finished with value: 126.65017121696083 and parameters: {'n_estimators': 768, 'max_depth': 3, 'learning_rate': 0.01584870582679049, 'num_leaves': 42, 'min_child_samples': 37, 'reg_alpha': 0.4034131460102555, 'reg_lambda': 0.959382947033887}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:38,168] Trial 73 finished with value: 124.78665507900133 and parameters: {'n_estimators': 548, 'max_depth': 3, 'learning_rate': 0.03863708812947177, 'num_leaves': 40, 'min_child_samples': 30, 'reg_alpha': 0.44125416837223014, 'reg_lambda': 0.9283644507760002}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:40,078] Trial 74 finished with value: 362.338521450721 and parameters: {'n_estimators': 985, 'max_depth': 4, 'learning_rate': 0.0005269002436949083, 'num_leaves': 25, 'min_child_samples': 30, 'reg_alpha': 0.42621897565556277, 'reg_lambda': 0.913992804524082}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:41,313] Trial 75 finished with value: 133.2437453429625 and parameters: {'n_estimators': 666, 'max_depth': 5, 'learning_rate': 0.03285389194740444, 'num_leaves': 29, 'min_child_samples': 41, 'reg_alpha': 0.4511038704940464, 'reg_lambda': 0.9310318566040533}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:41,811] Trial 76 finished with value: 128.7618109581693 and parameters: {'n_estimators': 430, 'max_depth': 3, 'learning_rate': 0.09154294801855029, 'num_leaves': 23, 'min_child_samples': 53, 'reg_alpha': 0.3913668976863298, 'reg_lambda': 0.9691929936062057}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:42,361] Trial 77 finished with value: 126.64544112784114 and parameters: {'n_estimators': 360, 'max_depth': 4, 'learning_rate': 0.054121260796452415, 'num_leaves': 45, 'min_child_samples': 45, 'reg_alpha': 0.4961817481970846, 'reg_lambda': 0.8172824834725303}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:42,868] Trial 78 finished with value: 133.6111777061218 and parameters: {'n_estimators': 224, 'max_depth': 5, 'learning_rate': 0.015416204313547965, 'num_leaves': 27, 'min_child_samples': 49, 'reg_alpha': 0.4547798983518566, 'reg_lambda': 0.8832862080555456}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:43,418] Trial 79 finished with value: 131.84554064280334 and parameters: {'n_estimators': 487, 'max_depth': 3, 'learning_rate': 0.08036733443226261, 'num_leaves': 36, 'min_child_samples': 38, 'reg_alpha': 0.3353035023068883, 'reg_lambda': 0.9998636230476271}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:44,934] Trial 80 finished with value: 141.6738359389961 and parameters: {'n_estimators': 646, 'max_depth': 6, 'learning_rate': 0.03391136371912972, 'num_leaves': 32, 'min_child_samples': 30, 'reg_alpha': 0.5032897850551018, 'reg_lambda': 0.925548569625917}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:45,793] Trial 81 finished with value: 129.76536127956123 and parameters: {'n_estimators': 569, 'max_depth': 4, 'learning_rate': 0.050226006857888875, 'num_leaves': 39, 'min_child_samples': 33, 'reg_alpha': 0.368410404937265, 'reg_lambda': 0.8673494702769385}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:46,433] Trial 82 finished with value: 128.76689891624412 and parameters: {'n_estimators': 544, 'max_depth': 3, 'learning_rate': 0.030403418419056284, 'num_leaves': 45, 'min_child_samples': 35, 'reg_alpha': 0.4289638957278483, 'reg_lambda': 0.941091701850335}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:46,860] Trial 83 finished with value: 126.51022610441765 and parameters: {'n_estimators': 435, 'max_depth': 3, 'learning_rate': 0.04385951330900399, 'num_leaves': 40, 'min_child_samples': 99, 'reg_alpha': 0.3881219164556767, 'reg_lambda': 0.8467820054804114}. Best is trial 51 with value: 120.87395891191115.\n",
      "[I 2023-10-29 13:40:47,959] Trial 84 finished with value: 120.82269836639388 and parameters: {'n_estimators': 719, 'max_depth': 4, 'learning_rate': 0.012499124853140363, 'num_leaves': 43, 'min_child_samples': 42, 'reg_alpha': 0.3071367281645422, 'reg_lambda': 0.8990868453260012}. Best is trial 84 with value: 120.82269836639388.\n",
      "[I 2023-10-29 13:40:48,983] Trial 85 finished with value: 123.1792884987247 and parameters: {'n_estimators': 719, 'max_depth': 4, 'learning_rate': 0.01536741810302501, 'num_leaves': 44, 'min_child_samples': 55, 'reg_alpha': 0.30614707762030474, 'reg_lambda': 0.972450773955901}. Best is trial 84 with value: 120.82269836639388.\n",
      "[I 2023-10-29 13:40:50,346] Trial 86 finished with value: 144.02829041948863 and parameters: {'n_estimators': 829, 'max_depth': 5, 'learning_rate': 0.061188090379677776, 'num_leaves': 48, 'min_child_samples': 55, 'reg_alpha': 0.2977464674351809, 'reg_lambda': 0.9722415243525868}. Best is trial 84 with value: 120.82269836639388.\n",
      "[I 2023-10-29 13:40:52,419] Trial 87 finished with value: 129.72152282456972 and parameters: {'n_estimators': 744, 'max_depth': 8, 'learning_rate': 0.010979083245610075, 'num_leaves': 44, 'min_child_samples': 40, 'reg_alpha': 0.32193688691488265, 'reg_lambda': 0.9031713561390974}. Best is trial 84 with value: 120.82269836639388.\n",
      "[I 2023-10-29 13:40:54,065] Trial 88 finished with value: 132.11542214409502 and parameters: {'n_estimators': 922, 'max_depth': 5, 'learning_rate': 0.027456383062933618, 'num_leaves': 47, 'min_child_samples': 42, 'reg_alpha': 0.2775514725453023, 'reg_lambda': 0.978205353390518}. Best is trial 84 with value: 120.82269836639388.\n",
      "[I 2023-10-29 13:40:55,463] Trial 89 finished with value: 136.44015767032462 and parameters: {'n_estimators': 1058, 'max_depth': 4, 'learning_rate': 0.07276419059814586, 'num_leaves': 41, 'min_child_samples': 58, 'reg_alpha': 0.4078427534271731, 'reg_lambda': 0.9030105227538651}. Best is trial 84 with value: 120.82269836639388.\n",
      "[I 2023-10-29 13:40:56,176] Trial 90 finished with value: 407.28243503178936 and parameters: {'n_estimators': 351, 'max_depth': 6, 'learning_rate': 0.0009446540434472767, 'num_leaves': 43, 'min_child_samples': 63, 'reg_alpha': 0.19605652851721667, 'reg_lambda': 0.8028981240666855}. Best is trial 84 with value: 120.82269836639388.\n",
      "[I 2023-10-29 13:40:57,253] Trial 91 finished with value: 122.84746344372994 and parameters: {'n_estimators': 715, 'max_depth': 4, 'learning_rate': 0.015606277239968897, 'num_leaves': 43, 'min_child_samples': 45, 'reg_alpha': 0.34665853303936933, 'reg_lambda': 0.9463091707840525}. Best is trial 84 with value: 120.82269836639388.\n",
      "[I 2023-10-29 13:40:58,168] Trial 92 finished with value: 123.18756598244083 and parameters: {'n_estimators': 608, 'max_depth': 4, 'learning_rate': 0.02358581631294924, 'num_leaves': 46, 'min_child_samples': 45, 'reg_alpha': 0.3289981526380359, 'reg_lambda': 0.9363172297933862}. Best is trial 84 with value: 120.82269836639388.\n",
      "[I 2023-10-29 13:40:59,378] Trial 93 finished with value: 121.97626387218152 and parameters: {'n_estimators': 819, 'max_depth': 4, 'learning_rate': 0.012042972652072798, 'num_leaves': 46, 'min_child_samples': 45, 'reg_alpha': 0.3356287587786911, 'reg_lambda': 0.962265547172365}. Best is trial 84 with value: 120.82269836639388.\n",
      "[I 2023-10-29 13:41:00,656] Trial 94 finished with value: 120.77769747083448 and parameters: {'n_estimators': 802, 'max_depth': 4, 'learning_rate': 0.010116756399064425, 'num_leaves': 50, 'min_child_samples': 46, 'reg_alpha': 0.2924801436621804, 'reg_lambda': 0.95489473411295}. Best is trial 94 with value: 120.77769747083448.\n",
      "[I 2023-10-29 13:41:01,949] Trial 95 finished with value: 120.22828411838069 and parameters: {'n_estimators': 839, 'max_depth': 4, 'learning_rate': 0.00977336856357859, 'num_leaves': 43, 'min_child_samples': 43, 'reg_alpha': 0.2885856463873081, 'reg_lambda': 0.8843805928842879}. Best is trial 95 with value: 120.22828411838069.\n",
      "[I 2023-10-29 13:41:03,266] Trial 96 finished with value: 121.24193692209049 and parameters: {'n_estimators': 846, 'max_depth': 4, 'learning_rate': 0.0088382363219774, 'num_leaves': 43, 'min_child_samples': 47, 'reg_alpha': 0.24133675149249995, 'reg_lambda': 0.8892089772560158}. Best is trial 95 with value: 120.22828411838069.\n",
      "[I 2023-10-29 13:41:05,204] Trial 97 finished with value: 125.9650636898538 and parameters: {'n_estimators': 836, 'max_depth': 6, 'learning_rate': 0.005559573846475957, 'num_leaves': 50, 'min_child_samples': 48, 'reg_alpha': 0.23396619663425566, 'reg_lambda': 0.8302926242499904}. Best is trial 95 with value: 120.22828411838069.\n",
      "[I 2023-10-29 13:41:06,834] Trial 98 finished with value: 133.81552377853075 and parameters: {'n_estimators': 915, 'max_depth': 5, 'learning_rate': 0.03304108452189049, 'num_leaves': 49, 'min_child_samples': 52, 'reg_alpha': 0.2561895687151637, 'reg_lambda': 0.8849844292113879}. Best is trial 95 with value: 120.22828411838069.\n",
      "[I 2023-10-29 13:41:07,937] Trial 99 finished with value: 138.3018673960106 and parameters: {'n_estimators': 1048, 'max_depth': 3, 'learning_rate': 0.05908484978458963, 'num_leaves': 49, 'min_child_samples': 43, 'reg_alpha': 0.2884002602363121, 'reg_lambda': 0.8372572790589252}. Best is trial 95 with value: 120.22828411838069.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../model/MLP/round2/mlp2lgbm_r2_T3.pkl']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(lambda trial: objective(trial, x_df12_1, y_df12_1), n_trials=100)\n",
    "# 모델 학습 후 저장 !!! 필 히  이름 바 꿀 것!!!!!!11\n",
    "model=LGBMRegressor(**study.best_params,random_state=42)\n",
    "model.fit(x_df12_1,y_df12_1)\n",
    "joblib.dump(model,\"../model/MLP2LGBM_V2/round1/mlp2lgbm_r1_T1.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(lambda trial: objective(trial, x_df12_2, y_df12_2), n_trials=100)\n",
    "# 모델 학습 후 저장 !!! 필 히  이름 바 꿀 것!!!!!!11\n",
    "model=LGBMRegressor(**study.best_params,random_state=42)\n",
    "model.fit(x_df12_2,y_df12_2)\n",
    "joblib.dump(model,\"../model/MLP2LGBM_V2/round1/mlp2lgbm_r1_T2.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(lambda trial: objective(trial, x_df12_3, y_df12_3), n_trials=100)\n",
    "# 모델 학습 후 저장 !!! 필 히  이름 바 꿀 것!!!!!!11\n",
    "model=LGBMRegressor(**study.best_params,random_state=42)\n",
    "model.fit(x_df12_3,y_df12_3)\n",
    "joblib.dump(model,\"../model/MLP2LGBM_V2/round1/mlp2lgbm_r1_T3.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(lambda trial: objective(trial, x_df22_1, y_df22_1), n_trials=100)\n",
    "# 모델 학습 후 저장 !!! 필 히  이름 바 꿀 것!!!!!!11\n",
    "model=LGBMRegressor(**study.best_params,random_state=42)\n",
    "model.fit(x_df22_1,y_df22_1)\n",
    "joblib.dump(model,\"../model/MLP2LGBM_V2/round2/mlp2lgbm_r2_T1.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(lambda trial: objective(trial, x_df22_2, y_df22_2), n_trials=100)\n",
    "# 모델 학습 후 저장 !!! 필 히  이름 바 꿀 것!!!!!!11\n",
    "model=LGBMRegressor(**study.best_params,random_state=42)\n",
    "model.fit(x_df22_2,y_df22_2)\n",
    "joblib.dump(model,\"../model/MLP2LGBM_V2/round2/mlp2lgbm_r2_T2.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(lambda trial: objective(trial, x_df22_3, y_df22_3), n_trials=100)\n",
    "# 모델 학습 후 저장 !!! 필 히  이름 바 꿀 것!!!!!!11\n",
    "model=LGBMRegressor(**study.best_params,random_state=42)\n",
    "model.fit(x_df22_3,y_df22_3)\n",
    "joblib.dump(model,\"../model/MLP2LGBM_V2/round2/mlp2lgbm_r2_T3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229.26909909077193\n",
      "{'n_estimators': 41, 'max_depth': 13, 'learning_rate': 0.15729921567662142, 'num_leaves': 9, 'min_child_samples': 24, 'reg_alpha': 0.9124606950026853, 'reg_lambda': 0.05559201100626268}\n"
     ]
    }
   ],
   "source": [
    "print(study.best_value)\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/MLP/mlp2lgbm_r1_T3.pkl']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습 후 저장 !!! 필 히  이름 바 꿀 것!!!!!!11\n",
    "model=LGBMRegressor(**study.best_params,random_state=42)\n",
    "model.fit(x_df12_3,y_df12_3)\n",
    "joblib.dump(model,\"../model/MLP2LGBM_V2/round1/mlp2lgbm_r1_T3.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측하려는 파일을 이제 시간대별 round별로 나눠야함 \n",
    "## 해당 기능을 수행하는 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test data set 만드는 함수\n",
    "def make_test_xy(wf,gf):\n",
    "    gf.columns = [\"time\",\"m0\",\"m1\",\"m2\",\"m3\",\"m4\"]\n",
    "    train = pd.merge(gf,wf,on=\"time\")\n",
    "    train[\"time\"] = pd.to_datetime(train[\"time\"])\n",
    "    train[\"month\"]=train[\"time\"].dt.month\n",
    "    train[\"day\"]=train[\"time\"].dt.day\n",
    "    train[\"hour\"]=train[\"time\"].dt.hour\n",
    "    \n",
    "    train_1 = train[(train[\"hour\"]<=10) & (train[\"hour\"]>=7)]\n",
    "    train_2 = train[(train[\"hour\"]<=14) & (train[\"hour\"]>=11)]\n",
    "    train_3 = train[(train[\"hour\"]<=19) & (train[\"hour\"]>=15)]\n",
    "    \n",
    "    train11 = train_1[[\"m0\",\"m1\",\"m2\",\"m3\",\"uv_idx\",\"elevation\"]]\n",
    "    train12 = train_2[[\"m0\",\"m1\",\"m2\",\"m3\",\"uv_idx\",\"elevation\"]]\n",
    "    train13 = train_3[[\"m0\",\"m1\",\"m2\",\"m3\",\"uv_idx\",\"elevation\"]]\n",
    "    train21 = train_1.drop([\"m0\",\"m1\",\"m2\",\"m3\",\"m4\",\"uv_idx\",\"elevation\",\"time\",\"hour\"],axis=1)\n",
    "    train22 = train_2.drop([\"m0\",\"m1\",\"m2\",\"m3\",\"m4\",\"uv_idx\",\"elevation\",\"time\",\"hour\"],axis=1)\n",
    "    train23 = train_3.drop([\"m0\",\"m1\",\"m2\",\"m3\",\"m4\",\"uv_idx\",\"elevation\",\"time\",\"hour\"],axis=1)\n",
    "    \n",
    "    return train11,train12,train13,train21,train22,train23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측에 사용할 데이터 셋 불러오기 \n",
    "x_pred_ori = pd.read_csv(\"../data/gen_forecasts/gen_fcst_10_2023-10-23.csv\")\n",
    "x_pred_wf = pd.read_csv(\"../data/weather_forecasts/wf10_2023-10-23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수에 넣어서 데이터셋 분리\n",
    "test11,test12,test13,test21,test22,test23 = make_test_xy(x_pred_wf,x_pred_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model11 = MLP_regressor(6)\n",
    "model12 = MLP_regressor(6)\n",
    "model13 = MLP_regressor(6)\n",
    "model11.load_state_dict(torch.load('../model/MLP2LGBM_V2/round1/m1_mlp11.pt', map_location=torch.device('cpu')))\n",
    "model12.load_state_dict(torch.load('../model/MLP2LGBM_V2/round1/m1_mlp12.pt', map_location=torch.device('cpu')))\n",
    "model13.load_state_dict(torch.load('../model/MLP2LGBM_V2/round1/m1_mlp13.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 진행 (1차 모델링 y_hat)\n",
    "y_pred11 = mlpPredict(model11, test11)\n",
    "y_pred12 = mlpPredict(model12, test12)\n",
    "y_pred13 = mlpPredict(model13, test13)\n",
    "y_pred11 = mms_Y_111.inverse_transform(y_pred11).reshape(-1,)\n",
    "y_pred12 = mms_Y_111.inverse_transform(y_pred12).reshape(-1,)\n",
    "y_pred13 = mms_Y_111.inverse_transform(y_pred13).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model21=joblib.load(\"../model/MLP/round1/mlp2lgbm_r1_T1.pkl\")\n",
    "model22=joblib.load(\"../model/MLP/round1/mlp2lgbm_r1_T2.pkl\")\n",
    "model23=joblib.load(\"../model/MLP/round1/mlp2lgbm_r1_T3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred21 = model21.predict(test21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred21 = model21.predict(test21)\n",
    "y_pred22 = model22.predict(test22)\n",
    "y_pred23 = model23.predict(test23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = y_pred11 + y_pred21\n",
    "result2 = y_pred12 + y_pred22\n",
    "result3 = y_pred13 + y_pred23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.25103003,  20.01773017,  41.59958489,  59.09831726,\n",
       "        57.08067546,  60.56545104,  64.27020125,  63.16248678,\n",
       "        57.82880004,  37.16326818,  12.97002658,  -6.04935423,\n",
       "       -10.26915885])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.concatenate((result1,result2,result3))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.915545, 16.915545, 16.91554 , 16.91554 ], dtype=float32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred11\n",
    "\n",
    "# y_pred13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
